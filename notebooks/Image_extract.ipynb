{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook extracts images from PDF files encodes them , stores them in a mongodb file. Along with each image, the textual content on the same page as the image is fed to the LLM to obtain relevant summaries of the extracted image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import fitz  \n",
    "import base64\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "import time\n",
    "import re\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import PIL.Image\n",
    "import google.ai.generativelanguage as glm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "MONGO_DB_URL = os.getenv('MONGO_DB_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted selected files to /Users/shreyasreedhar/Desktop/QA_BOT/extracted_papers\n"
     ]
    }
   ],
   "source": [
    "#Unzipping folder\n",
    "\n",
    "zip_file_path = \"/Users/shreyasreedhar/Desktop/QA_BOT/papers-20241017T051416Z-001.zip\"\n",
    "extraction_dir = \"/Users/shreyasreedhar/Desktop/QA_BOT/extracted_papers\"\n",
    "\n",
    "papers = [\n",
    "    '2306.11695.pdf', '1710.05941.pdf', '2110.15343.pdf', '2310.06825.pdf',\n",
    "    '2306.17806.pdf', '2306.12929.pdf', 'ICML03-094.pdf', '2301.00774.pdf',\n",
    "    '2005.14165.pdf', 'Adaptive-mixtures-of-local-experts.pdf', '1907.01470.pdf',\n",
    "    '2305.18290.pdf', '2310.20707.pdf', '2207.00112.pdf', '2211.05102.pdf',\n",
    "    '2203.02155.pdf', '2104.09864.pdf', '2307.13304.pdf', '2001.08361.pdf',\n",
    "    '2302.10866.pdf', '2305.07185.pdf', '2306.07629.pdf',\n",
    "    '10000000_662098952474184_2584067087619170692_n.pdf', '2109.06243.pdf',\n",
    "    '2301.13688.pdf'\n",
    "]\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    for file in zip_ref.namelist():\n",
    "        if os.path.basename(file) in papers:\n",
    "            zip_ref.extract(file, extraction_dir)\n",
    "\n",
    "print(f\"Extracted selected files to {extraction_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ICML03-094.pdf\n",
      "Total pages: 8\n",
      "Page 1 Text: Weighted Low-Rank Approximations\n",
      "Nathan Srebro\n",
      "nati@mit.edu\n",
      "Tommi Jaakkola\n",
      "tommi@ai.mit.edu\n",
      "Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA\n",
      "Abstract\n",
      "We study the common problem of approx-\n",
      "imating a target matrix with a matrix of\n",
      "lower rank. We provide a simple and eﬃcient\n",
      "(EM) algorithm for solving weighted low-rank\n",
      "approximation problems, which, unlike their\n",
      "unweighted version, do not admit a closed-\n",
      "form solution in general. We analyze...\n",
      "No images found on page 1\n",
      "Page 2 Text: proximations. Unlike for the unweighted case, such a\n",
      "greedy procedure is sub-optimal.\n",
      "We suggest optimization methods that are signiﬁ-\n",
      "cantly more eﬃcient and simpler to implement (Sec-\n",
      "tion 2). We also consider other measures of deviation,\n",
      "beyond weighted Frobenius norms.\n",
      "Such measures\n",
      "arise, for example, when the noise model associated\n",
      "with matrix elements is known but not is Gaussian.\n",
      "For example, for binary data, a logistic model with an\n",
      "underlying low-rank representation might be more ap-\n",
      "p...\n",
      "No images found on page 2\n",
      "Page 3 Text: In order to understand the behavior of the objective\n",
      "function, it is important to study the remaining critical\n",
      "points. For a critical point (U, V ) spanned by eigen-\n",
      "vectors corresponding to eigenvalues as above (assum-\n",
      "ing no repeated eigenvalues), the Hessian has exactly\n",
      "P\n",
      "q∈Q q −\n",
      "\u0000k\n",
      "2\n",
      "\u0001\n",
      "negative eigenvalues: we can replace any\n",
      "eigencomponent with eigenvalue σ with an alternate\n",
      "eigencomponent not already in (U, V ) with eigenvalue\n",
      "σ′ > σ, decreasing the objective function. The change\n",
      "can be do...\n",
      "No images found on page 3\n",
      "Page 4 Text: invertible scalings, V can be speciﬁed as an angle θ on\n",
      "a semi-circle. We plot the value of J∗([cos θ, sin θ]) for\n",
      "each θ, and for varying weight matrices of the form\n",
      "W =\n",
      "\u0000 1+α\n",
      "1\n",
      "1\n",
      "1+α\n",
      "\u0001\n",
      ". At the front of the plot, the weight\n",
      "matrix is uniform and indeed there is only a single lo-\n",
      "cal minimum, but at the back of the plot, where the\n",
      "weight matrix emphasizes the diagonal, a non-global\n",
      "local minimum emerges.\n",
      "0\n",
      "pi/2\n",
      "pi\n",
      "0\n",
      "0.5\n",
      "1\n",
      "2\n",
      "2.5\n",
      "3\n",
      "θ\n",
      "α\n",
      "J*(cos θ, sin θ) for W = 1 + α I\n",
      "Figure 1. Emergence of local...\n",
      "No images found on page 4\n",
      "Page 5 Text: to A, and to initialize X to zero.\n",
      "Initializing X to\n",
      "A works reasonably well if the weights are bounded\n",
      "away from zero, or if the target values in A have rela-\n",
      "tively small variance. However, when the weights are\n",
      "zero, or very close to zero, the target values become\n",
      "meaningless, and can throw oﬀthe search. Initializing\n",
      "X to zero avoids this problem, as target values with\n",
      "zero weights are completely ignored (as they should\n",
      "be), and works well as long as the weights are fairly\n",
      "dense. However, when...\n",
      "No images found on page 5\n",
      "Page 6 Text: (2003) and recently studied by Schein et al. (2003).\n",
      "Using a weighted low-rank approximation, we can ﬁt\n",
      "a low-rank matrix X minimizing a quadratic loss from\n",
      "the target. In order to ﬁt a non-quadratic loss such as\n",
      "a logistic loss, Loss(Xia; yia) = log g(yiaXia), we use a\n",
      "quadratic approximation to the loss.\n",
      "Consider\n",
      "the\n",
      "second-order\n",
      "Taylor\n",
      "expansion\n",
      "of\n",
      "log g(yx) about ˜x:\n",
      "log g(yx) ≈\n",
      "≈log g(y˜x) + yg(−y˜x)(x −˜x) −g(y˜x)g(−y˜x)\n",
      "2\n",
      "(x −˜x)2\n",
      "≈−g(y˜x)g(−y˜x)\n",
      "2\n",
      "\u0010\n",
      "x −\n",
      "\u0010\n",
      "˜x +\n",
      "y\n",
      "g(y˜x)\n",
      "\u0011\u00112\n",
      "+log g(y˜x)+ g...\n",
      "No images found on page 6\n",
      "Page 7 Text: et al. (2001) use a low-rank approximation of a fully-\n",
      "observed subset of columns of the matrix, thus avoid-\n",
      "ing the need to introduce weights. Billsus and Paz-\n",
      "zani (1998) use a singular value decomposition of a\n",
      "sparse binary observation matrix. Both Goldberg and\n",
      "Billsus use the low-rank approximation only as a pre-\n",
      "processing step, and then use clustering (Goldberg)\n",
      "and neural networks (Billsus) to learn the preferences.\n",
      "Azar et al. (2001) proved asymptotic consistency of\n",
      "a method in which uno...\n",
      "No images found on page 7\n",
      "Page 8 Text: also ensures scale and transformation invariability for\n",
      "a user’s ratings, and places more emphasis on users\n",
      "with a bimodal rating distribution than on users for\n",
      "which all ratings are clustered together. We proceeded\n",
      "to ﬁt a low-rank logistic model (q.v. Section 3) using\n",
      "the observed posterior probabilities as empirical prob-\n",
      "abilities. Since the resulting low-rank model no longer\n",
      "predicts the absolute rating of jokes, we measured suc-\n",
      "cess by analyzing the relative ranking of jokes by each\n",
      "user....\n",
      "No images found on page 8\n",
      "Finished processing ICML03-094.pdf\n",
      "Processing: 2301.00774.pdf\n",
      "Total pages: 14\n",
      "Page 1 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "Elias Frantar 1 Dan Alistarh 1 2\n",
      "Abstract\n",
      "We show for the ﬁrst time that large-scale genera-\n",
      "tive pretrained transformer (GPT) family mod-\n",
      "els can be pruned to at least 50% sparsity in\n",
      "one-shot, without any retraining, at minimal\n",
      "loss of accuracy.\n",
      "This is achieved via a new\n",
      "pruning method called SparseGPT, speciﬁcally\n",
      "designed to work efﬁciently and accurately on\n",
      "massive GPT-family models. We can execute\n",
      "SparseGPT on the lar...\n",
      "No images found on page 1\n",
      "Page 2 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Sparsity\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "Perplexity on raw-WikiText2\n",
      "Magnitude\n",
      "SparseOPT\n",
      "Dense\n",
      "Figure 1. Sparsity-vs-perplexity\n",
      "comparison\n",
      "of\n",
      "SparseGPT\n",
      "against magnitude pruning on OPT-175B, when pruning to different\n",
      "uniform per-layer sparsities.\n",
      "10\n",
      "1\n",
      "100\n",
      "101\n",
      "102\n",
      "#Params in Billions\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Perplexity on raw-WikiText2\n",
      "2:4\n",
      "4:8\n",
      "50% Unstructured\n",
      "Dense\n",
      "Figure 2. Perplexity vs. model and sparsity type ...\n",
      "No images found on page 2\n",
      "Page 3 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "select & invert\n",
      "reconstruct\n",
      "Hessian\n",
      "Figure 3. Illustration of the row-Hessian challenge: rows are spar-\n",
      "siﬁed independently, pruned weights are in white.\n",
      "with several minutes to a few hours of compute. However,\n",
      "our goal here is to sparsify models up to 1000× larger.\n",
      "Even AdaPrune, the method optimized for an ideal\n",
      "speed/accuracy trade-off, takes a few hours to sparsify mod-\n",
      "els with just 1.3 billion parameters (see also Sect...\n",
      "No images found on page 3\n",
      "Page 4 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "elimination\n",
      "update\n",
      "prune\n",
      "frozen\n",
      "not yet pruned\n",
      "p% sparse\n",
      "mask\n",
      "Figure 4. [Left] Visualization of the SparseGPT reconstruction algorithm. Given a ﬁxed pruning mask M, we incrementally prune\n",
      "weights in each column of the weight matrix W, using a sequence of Hessian inverses (HUj)−1, and updating the remainder of the\n",
      "weights in those rows, located to the “right” of the column being processed. Speciﬁcally, the weights to the “rig...\n",
      "No images found on page 4\n",
      "Page 5 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "value and ensuring that it is never “decompressed” again via\n",
      "some future update, i.e. that it is frozen. Hence, by deﬁning\n",
      "column-wise compression as:\n",
      "compress(wj)i = 0 if j ̸∈Mi and wj\n",
      "i otherwise,\n",
      "(6)\n",
      "i.e. zeroing weights not in the mask and ﬁxing the rest to\n",
      "their current value, our algorithm can be interpreted as an\n",
      "exact column-wise greedy scheme. This perspective will\n",
      "allow us to cleanly merge sparsiﬁcation and quantiz...\n",
      "No images found on page 5\n",
      "Page 6 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "Table 1. OPT perplexity results on raw-WikiText2.\n",
      "OPT - 50%\n",
      "125M\n",
      "350M\n",
      "1.3B\n",
      "Dense\n",
      "27.66\n",
      "22.00\n",
      "14.62\n",
      "Magnitude\n",
      "193.\n",
      "97.80\n",
      "1.7e4\n",
      "AdaPrune\n",
      "58.66\n",
      "48.46\n",
      "32.52\n",
      "SparseGPT\n",
      "36.85\n",
      "31.58\n",
      "17.46\n",
      "OPT\n",
      "Sparsity\n",
      "2.7B\n",
      "6.7B\n",
      "13B\n",
      "30B\n",
      "66B\n",
      "175B\n",
      "Dense\n",
      "0%\n",
      "12.47\n",
      "10.86\n",
      "10.13\n",
      "9.56\n",
      "9.34\n",
      "8.35\n",
      "Magnitude\n",
      "50%\n",
      "265.\n",
      "969.\n",
      "1.2e4\n",
      "168.\n",
      "4.2e3\n",
      "4.3e4\n",
      "SparseGPT\n",
      "50%\n",
      "13.48\n",
      "11.55\n",
      "11.17\n",
      "9.79\n",
      "9.32\n",
      "8.21\n",
      "SparseGPT\n",
      "4:8\n",
      "14.98\n",
      "12.56\n",
      "11.77\n",
      "10.30\n",
      "9.65\n",
      "8.45\n",
      "SparseGPT\n",
      "2:4\n",
      "17.18\n",
      "14....\n",
      "No images found on page 6\n",
      "Page 7 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Sparsity\n",
      "7.5\n",
      "10.0\n",
      "12.5\n",
      "15.0\n",
      "17.5\n",
      "20.0\n",
      "22.5\n",
      "25.0\n",
      "Perplexity on raw-WikiText2\n",
      "Magnitude\n",
      "SparseOPT\n",
      "Dense\n",
      "Figure 5. Uniform pruning BLOOM-176B.\n",
      "Table 2. ZeroShot results on several datasets for sparsiﬁed variants of OPT-175B.\n",
      "Method\n",
      "Spars.\n",
      "Lamb.\n",
      "PIQA\n",
      "ARC-e\n",
      "ARC-c\n",
      "Story.\n",
      "Avg.\n",
      "Dense\n",
      "0%\n",
      "75.59\n",
      "81.07\n",
      "71.04\n",
      "43.94\n",
      "79.82\n",
      "70.29\n",
      "Magnitude\n",
      "50%\n",
      "00.02\n",
      "54.73\n",
      "28.03\n",
      "25.60\n",
      "47.10\n",
      "31.10\n",
      "SparseGPT\n",
      "50%\n",
      "78.47\n",
      "80.63\n",
      "...\n",
      "No images found on page 7\n",
      "Page 8 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "sparse + 4-bit weights, store only the non-zero weights and\n",
      "use a bitmask to indicate their positions, then this has the\n",
      "same overall memory consumption as 3-bit quantization.\n",
      "Hence, in Figure 6 (right) we compare SparseGPT 50%\n",
      "+ 4-bit with state-of-the-art GPTQ (Frantar et al., 2022a)\n",
      "3-bit numbers. It can be seen that 50% + 4-bit models are\n",
      "more accurate than their respective 3-bit versions for 2.7B+\n",
      "parameter models, incl...\n",
      "No images found on page 8\n",
      "Page 9 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "zero-shot performance. Speciﬁcally, we have shown that\n",
      "the largest open-source GPT-family models (e.g. OPT-175B\n",
      "and BLOOM-176B) can reach 50-60% sparsity, dropping\n",
      "more than 100B weights, with low accuracy ﬂuctuations.\n",
      "Our work shows that the high degree of parametrization\n",
      "of massive GPT models allows pruning to directly identify\n",
      "sparse accurate models in the “close neighborhood” of the\n",
      "dense model, without gradient informat...\n",
      "No images found on page 9\n",
      "Page 10 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,\n",
      "D. Accurate post training quantization with small cal-\n",
      "ibration sets. In International Conference on Machine\n",
      "Learning (ICML), 2021b.\n",
      "HuggingFace.\n",
      "HuggingFace Perplexity Calculation,\n",
      "2022.\n",
      "URL https://huggingface.co/docs/\n",
      "transformers/perplexity.\n",
      "Kingdon, J.\n",
      "Hypothesising Neural Nets, pp. 81–106.\n",
      "Springer London, London, 1997. ISBN 978-1-4471-0949-\n",
      "5. doi: 10.1007/9...\n",
      "No images found on page 10\n",
      "Page 11 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow,\n",
      "D., Castagn´e, R., Luccioni, A. S., Yvon, F., Gall´e, M.,\n",
      "et al. Bloom: A 176b-parameter open-access multilingual\n",
      "language model. arXiv preprint arXiv:2211.05100, 2022.\n",
      "Singh, S. P. and Alistarh, D. WoodFisher: Efﬁcient second-\n",
      "order approximation for neural network compression. In\n",
      "Conference on Neural Information Processing Systems\n",
      "(NeurIPS), 2020.\n",
      "Tata, S. a...\n",
      "No images found on page 11\n",
      "Page 12 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "A. Ablation Studies\n",
      "In this section, we conduct ablations studies with respect to several of the main parameters of SparseGPT. For a fast\n",
      "iteration time and making it possible to also explore more compute and memory intensive settings, we focus on the OPT-2.7B\n",
      "model here. Unless stated otherwise, we always prune uniformly to the default 50% sparsity. For brevity we only show\n",
      "raw-WikiText2 results here, but would like to note...\n",
      "No images found on page 12\n",
      "Page 13 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Layer index\n",
      "1.0\n",
      "1.2\n",
      "1.4\n",
      "1.6\n",
      "1.8\n",
      "2.0\n",
      "SparseOPT relative to exact reconstruction\n",
      "k\n",
      "v\n",
      "q\n",
      "out\n",
      "fc1\n",
      "fc2\n",
      "Figure 11. Error of SparseGPT reconstruction relative to exact reconstruction for the ﬁrst half of OPT-2.7B at 50% sparsity.\n",
      "encode the result with the model’s matching tokenizer and then split it into non-overlapping segments of 2048 tokens (the\n",
      "maximum history of the models we study). Those are run through the mod...\n",
      "No images found on page 13\n",
      "Page 14 Text: SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n",
      "D. Partial 2:4 Results\n",
      "Tables 5 and 6 show the performance of a sequence of partially 2:4 sparse models on three different language modelling\n",
      "datasets. The ﬁrst fraction of layers is fully sparsiﬁed while the remainder is kept dense. In this way, speedup and accuracy\n",
      "can be traded off also from binary compression choices, such as n:m-pruning.\n",
      "Table 5. Pruning different fractions (as consecutive segments from the beginning) o...\n",
      "No images found on page 14\n",
      "Finished processing 2301.00774.pdf\n",
      "Processing: 2110.15343.pdf\n",
      "Total pages: 34\n",
      "Page 1 Text: Scatterbrain: Unifying Sparse and Low-rank Attention\n",
      "Approximation\n",
      "Beidi Chen∗†, Tri Dao∗†, Eric Winsor†, Zhao Song§, Atri Rudra‡, and Christopher Ré†\n",
      "†Department of Computer Science, Stanford University\n",
      "§Adobe Research\n",
      "‡Department of Computer Science and Engineering, University at Buﬀalo, SUNY\n",
      "{beidic,trid,winsor}@stanford.edu, zsong@adobe.com, atri@buffalo.edu,\n",
      "chrismre@cs.stanford.edu\n",
      "October 29, 2021\n",
      "Abstract\n",
      "Recent advances in eﬃcient Transformers have exploited either the sparsity or low-r...\n",
      "No images found on page 1\n",
      "Page 2 Text: 85%\n",
      "95%\n",
      "Kernel & Hash Construction\n",
      " \n",
      " \n",
      ",\n",
      ",\n",
      "CATEGORIZATION\n",
      "Input\n",
      "SPARSE\n",
      "LOWRANK\n",
      "SCATTERBRAIN\n",
      "Figure 1: Left: regimes that sparse+low-rank approximation is more accurate, based on the entropy of the\n",
      "attention matrices. Right: Scatterbrain Workﬂow. For the attention layer in Transformers, after computing\n",
      "Query Q, Key K, and Value V matrices, we approximate softmax(QK⊤)V with two components: (i) sparse\n",
      "SV (ii) low-rank φ(Q)(φ(K)⊤V ).\n",
      "We observe that sparse and low-rank approximations are complementa...\n",
      "Extracted 4 images from page 2\n",
      "Page 3 Text: achieves 2.1× lower error compared to other eﬃcient baselines on real benchmarks. This leads to a direct\n",
      "application of Scatterbrain as a drop-in replacement to pre-trained full attention, thus reducing up to 98%\n",
      "of the memory required for attention computations in pre-trained T2T-ViT and BigGAN while maintaining\n",
      "similar quality. Last we show that its superior accuracy and eﬃciency can improve the eﬃciency-accuracy\n",
      "trade-oﬀs of Transformer end-to-end training. On the WikiText-103 language modeli...\n",
      "No images found on page 3\n",
      "Page 4 Text: 3.1\n",
      "Motivating Observations:\n",
      "Low-rank and Sparse Structures of Attention\n",
      "Matrices\n",
      "We empirically characterize regimes where sparse and low-rank approximation are well-suited, based on the\n",
      "softmax temperature (for which we use the softmax distribution entropy is a proxy). Speciﬁcally, in Fig. 1\n",
      "(left), we present the approximation error of the original attention matrices and the approximation (sparse or\n",
      "low-rank) of matrices sampled from a 4-layer Transformer trained on IMDb reviews classiﬁcation...\n",
      "No images found on page 4\n",
      "Page 5 Text: low-rank matrix. In the middle regime of β, we need the sparse part to cover the intra-cluster attention and\n",
      "the low-rank part to approximate the inter-cluster attention.\n",
      "We formalize this intuition in Theorem 1 (in bounds below we think of ϵ as a constant). All the proofs\n",
      "are in Appendix D.\n",
      "Theorem 1. Let Mβ, be the attention matrix in Process 1. Fix ϵ ∈(0, 1). Let R ∈Rn×n be a matrix.\n",
      "Consider low-rank, sparse, and sparse + low-rank approximations to Mβ.\n",
      "1. High temperature: Assume β = o(log n...\n",
      "No images found on page 5\n",
      "Page 6 Text: 1. If the goal is accuracy, Robust PCA is the most studied algorithm to ﬁnd a sparse + low-rank approximation\n",
      "to a given matrix. It relaxes the NP-hard problem of ﬁnding the best sparse + low-rank approximation\n",
      "into a convex optimization problem, with the nuclear norm and ℓ1 constraints. Even though it can be\n",
      "solved in polynomial time, it is orders of magnitude too slow to be used in each iteration of a training\n",
      "loop. Moreover, it requires materializing the attention matrix, which defeats the ma...\n",
      "No images found on page 6\n",
      "Page 7 Text: −1.0\n",
      "−0.5\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "q⊤k\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "MSE\n",
      "Reformer (sparse)\n",
      "Performer (low-rank)\n",
      "Scatterbrain\n",
      "(sparse+low-rank)\n",
      "Figure 4: Per-entry MSE for diﬀer-\n",
      "ent approximations, across a range\n",
      "of magnitude of q⊤k.Scatterbrain has\n",
      "low MSE for both small and large en-\n",
      "tries, thus outperforming its sparse\n",
      "(Reformer) and low-rank (Performer)\n",
      "counterparts.\n",
      "The Scatterbrain method would work exactly the same way. As\n",
      "long as the low-rank component is unbiased (e.g., Performer), its\n",
      "combination with any sparse co...\n",
      "No images found on page 7\n",
      "Page 8 Text: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Entropy\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "Approx. Error\n",
      "Scatterbrain\n",
      "Robust-PCA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Entropy\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "Approx. Error\n",
      "Smyrf\n",
      "Performer\n",
      "Scatterbrain\n",
      "10−1\n",
      "100\n",
      "Memory\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Inception\n",
      "Smyrf\n",
      "Performer\n",
      "Scatterbrain\n",
      "Full\n",
      "10−1\n",
      "100\n",
      "Memory\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "FID\n",
      "Smyrf\n",
      "Performer\n",
      "Scatterbrain\n",
      "Full\n",
      "Figure 5: First: approximation comparison between Scatterbrain and its “lowerbound\" Robust PCA. Second:\n",
      "comparison of error vs. entropy among SMYRF, Performer and Scatterbrai...\n",
      "No images found on page 8\n",
      "Page 9 Text: Table 2: The performance of Scatterbrain, Reformer, Performer and Full-Attention on Long-Range-Arena\n",
      "benchmarks and 2 popular language modeling tasks. We ﬁx the same number of parameters (1/8 of the full)\n",
      "used for approximating the attention matrix for each method.\n",
      "Attention\n",
      "Copy (ppl)\n",
      "WikiText-103 (ppl)\n",
      "Full Attention\n",
      "1\n",
      "25.258\n",
      "Reformer\n",
      "6.8\n",
      "27.68\n",
      "Performer\n",
      "49\n",
      "66\n",
      "Scatterbrain\n",
      "2.58\n",
      "26.72\n",
      "Attention\n",
      "ListOps\n",
      "Text\n",
      "Retrieval\n",
      "Image\n",
      "Pathﬁnder\n",
      "Avg\n",
      "Full Attention\n",
      "38.2\n",
      "63.29\n",
      "80.85\n",
      "41.78\n",
      "73.98\n",
      "59.62\n",
      "Reformer...\n",
      "No images found on page 9\n",
      "Page 10 Text: 5.2.2\n",
      "Classiﬁcation Tasks\n",
      "On a suite of long-range benchmark tasks (Long Range Area), Scatterbrain outperforms Reformer (sparse\n",
      "baseline) and Performer (low-rank baseline) by up to 5 points on average.\n",
      "Settings: We compare the performance of Scatterbrain against Reformer and Performer on ListOps, two\n",
      "classiﬁcations: byte-level IMDb reviews text classiﬁcation, image classiﬁcation on sequences of pixels, a text\n",
      "retrieval, and pathﬁnder tasks. The datasets are obtained from the Long Range Arena (LR...\n",
      "No images found on page 10\n",
      "Page 11 Text: Potential negative societal impacts. Our work seeks to understand the role of matrix approximation\n",
      "(and potentially energy savings) in the attention layer, which may improve a wide range of applications, each\n",
      "with their own potential beneﬁts and harms. For example, making it language modeling more compute and\n",
      "memory eﬃcient might facilitate spreading misinformation, and better image and video processing may make\n",
      "automatic surveillance easier. To mitigate these risks, one needs to address applica...\n",
      "No images found on page 11\n",
      "Page 12 Text: [5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\n",
      "preprint arXiv:2004.05150, 2020.\n",
      "[6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. On the dangers\n",
      "of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference\n",
      "on Fairness, Accountability, and Transparency, New York, NY, USA, 2021. Association for Computing\n",
      "Machinery.\n",
      "[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Ja...\n",
      "No images found on page 12\n",
      "Page 13 Text: [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,\n",
      "and Christopher Ré. Kaleidoscope: An eﬃcient, learnable representation for all structured linear maps.\n",
      "In The International Conference on Learning Representations (ICLR), 2020.\n",
      "[22] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf: Eﬃcient attention\n",
      "using asymmetric clustering. arXiv preprint arXiv:2010.05315, 2020.\n",
      "[23] Christopher De Sa, Christopher Re, and Kunle O...\n",
      "No images found on page 13\n",
      "Page 14 Text: [39] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear\n",
      "memory: How to make performers slim. arXiv preprint arXiv:2012.11346, 2020.\n",
      "[40] Drew Linsley, Junkyung Kim, Vijay Veerabadran, and Thomas Serre. Learning long-range spatial\n",
      "dependencies with horizontal gated-recurrent units. arXiv preprint arXiv:1805.08315, 2018.\n",
      "[41] Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, and Anshumali Shrivastava. Climbing the\n",
      "wol: Training for chea...\n",
      "No images found on page 14\n",
      "Page 15 Text: [56] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span\n",
      "in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\n",
      "2019.\n",
      "[57] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\n",
      "Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers.\n",
      "arXiv preprint arXiv:2011.04006, 2020.\n",
      "[58] Yi Tay, Mostafa Dehghani, Dara Bahri, and Do...\n",
      "No images found on page 15\n",
      "Page 16 Text: Appendix\n",
      "Table of Contents\n",
      "A\n",
      "Extended Related Work\n",
      "17\n",
      "A.1 Robust PCA\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "A.2 Eﬃcient Transformers\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "A.3 Locality Sensitive Hashing for Eﬃcient Neural Network Training\n",
      ". . . . . . . . . . . . . .\n",
      "18\n",
      "A.4 Structured Matrices for Eﬃcient Machine Learning Models\n",
      ". . . . . . . . . . . . . . . . .\n",
      "18\n",
      "B\n",
      "Motivating Observations: Low-rank and Spa...\n",
      "No images found on page 16\n",
      "Page 17 Text: A\n",
      "Extended Related Work\n",
      "A.1\n",
      "Robust PCA\n",
      "Robust Principle Component Analysis (robust PCA) is the problem of ﬁnding a composition of a matrix M\n",
      "into a sum of sparse and low-rank components: M = S + L. It is a modiﬁcation of PCA to accommodate\n",
      "corrupted observations (aka, noise). The sparse part covers the noise, while the low-rank part recovers the\n",
      "principle components. The most popular method to solve the problem is convex relaxation [8], where one\n",
      "minimizes the error ∥M −S −L∥2\n",
      "F subject to ℓ1 co...\n",
      "No images found on page 17\n",
      "Page 18 Text: low-rank approximation due to the non-linearity between the two attention steps. It is not clear to us that\n",
      "it combines diﬀerent kinds of attention.\n",
      "• Long-short transformer[71] (concurrent work): a special case of Scatterbrain where the sparse component\n",
      "is local attention and the low-rank component is Linformer.\n",
      "A.3\n",
      "Locality Sensitive Hashing for Eﬃcient Neural Network Training\n",
      "Locality Sensitive Hashing (LSH) has been well-studied in approximate nearest-neighbor search [2, 11, 27, 30,\n",
      "34, 54]....\n",
      "No images found on page 18\n",
      "Page 19 Text: B\n",
      "Motivating Observations:\n",
      "Low-rank and Sparse Structures of\n",
      "Attention Matrices\n",
      "We aim to build a deeper understanding of sparse and low-rank structures in real attention matrices: where\n",
      "each of them excel, and the potential for their combination. Speciﬁcally, we\n",
      "• show that sparse and low-rank approximation errors are negatively correlated (through statistical tests),\n",
      "• characterize regimes where each of sparse and low-rank approximation are well-suited, as dictated by the\n",
      "entropy of the softma...\n",
      "No images found on page 19\n",
      "Page 20 Text: 0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "Sparse Approx. Error\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Low-rank Approx. Error\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Entropy\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "Approx. Error\n",
      "Sparse\n",
      "Low-rank\n",
      "Sparse+Lowrank\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Sparse Approx. Error\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Low-rank Approx. Error\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Entropy\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Approx. Error\n",
      "Sparse\n",
      "Low-rank\n",
      "Sparse+Lowrank\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "Sparse Approx. Error\n",
      "0.00\n",
      "0.05\n",
      "0.10\n",
      "0.15\n",
      "0.20\n",
      "0.25\n",
      "0.30\n",
      "0.35\n",
      "Low-rank Approx. Error\n",
      "2\n",
      "4\n",
      "6\n",
      "Entropy\n",
      "0.00\n",
      "0.05\n",
      "0.10\n",
      "0...\n",
      "No images found on page 20\n",
      "Page 21 Text: matrices approximates the attention matrix well. For low-entropy distributions (more peaked), sparse matrices\n",
      "are better-suited.\n",
      "This implies that sparse and low-rank approximations could be complementary: if we can combine\n",
      "the strength of both, it is possible to come up with a better approximation across more general scenarios.\n",
      "Therefore, in the next observation, we try to combine sparse and low-rank approximations.\n",
      "B.4\n",
      "Observation 3: Sparse + Low-rank achieves better approximation error\n",
      "than s...\n",
      "No images found on page 21\n",
      "Page 22 Text: C\n",
      "Scatterbrain Algorithm and Implementation Details\n",
      "Let Q, K ∈Rn×d be the query and key matrices respectively, and V ∈Rn×d be the value matrix. Let the\n",
      "rows of Q be q1, . . . , qn, and the rows of K be k1, . . . , kn. The attention computes:\n",
      "softmax(QK⊤)V,\n",
      "with softmax applied row-wise, where for each vector v ∈Rn, softmax(v) =\n",
      "1\n",
      "Pn\n",
      "j=1 evj\n",
      "\u0002ev1, . . . , evn\u0003⊤. Here\n",
      "we omit the usual scaling of\n",
      "QK⊤\n",
      "√\n",
      "d\n",
      "for simplicity since that could be folded into Q or K.\n",
      "Note that\n",
      "softmax(QK⊤) = D−1 exp(QK⊤), ...\n",
      "No images found on page 22\n",
      "Page 23 Text: D\n",
      "Proofs\n",
      "D.1\n",
      "Expressiveness of Sparse + Low-rank Matrices\n",
      "To motivate the use of sparse + low-rank matrices, we describe a family of attention matrices where sparse\n",
      "+ low-rank matrices need asymptotically fewer parameters to approximate the attention matrix, compared\n",
      "to sparse or low-rank matrices alone. For there cases, either sparse or low-rank alone requires a quadratic\n",
      "number of parameters (O(n2), where n × n is the dimension of the attention matrix) to get ϵ approximation\n",
      "error in Frobenius...\n",
      "No images found on page 23\n",
      "Page 24 Text: By a union bound over all pairs i ̸= j (there are n(n −1)/2 such pairs), with probability at least\n",
      "1 −n2 exp\n",
      "\u0000−dϵ2/2\n",
      "\u0001\n",
      ", we have that\n",
      "q⊤\n",
      "i qj ∈[−ϵ, ϵ]\n",
      "for all i ̸= j.\n",
      "Since we assume that d ≥6ϵ−2 log n, we have that\n",
      "n2 exp(−dϵ2/2) ≤n2 exp(−3 log n) = n−1.\n",
      "Hence q⊤\n",
      "i qj ∈[−ϵ, ϵ] for all i ̸= j with probability at least 1 −n−1. For the rest of the proof, we only consider\n",
      "this case (where q⊤\n",
      "i qj ∈[−ϵ, ϵ] for all i ̸= j).\n",
      "Since 1 + x ≤ex ≤1 + x + x2 for |x| < 1, we can bound the oﬀdiagonal elements...\n",
      "No images found on page 24\n",
      "Page 25 Text: Sparse estimator:\n",
      "For our sparse estimator, it is easy to see that for any ES ∈Rn×n that has row sparsity\n",
      "k (each row has fewer than k non-zeros),\n",
      "∥M −ES∥F ≥Ω(\n",
      "p\n",
      "n(n −k)).\n",
      "This implies that in order to achieve error O(√n), we would need n −k = O(1), which requires Ω(n2)\n",
      "parameters.\n",
      "Now we construct a matrix that shows better separation between the approximation capability of sparse\n",
      "+ low-rank vs sparse or low-rank alone.\n",
      "Example 2. Consider the following randomized construction of matrix Q ∈Rn×d...\n",
      "No images found on page 25\n",
      "Page 26 Text: The Frobenious error of sparse + low-rank approximation is\n",
      "∥M −ESL∥F ≤O(\n",
      "p\n",
      "ϵ2n(n −1)) ≤O(ϵn).\n",
      "We have that:\n",
      "(i) Sparse + low-rank parameter count is n · (1 + 1) ≤O(n).\n",
      "(ii) Its Frobenius error is ≤O(n).\n",
      "Low-rank estimator: We want to argue that low-rank approximation would require more parameters.\n",
      "From a similar observation that any matrix R with rank that n −rank = Ω(1),\n",
      "∥(er −1)I −R∥F ≥Ω(er),\n",
      "(by Eckart–Young–Mirsky theorem), we obtain a similar result to the proof of Theorem 3.\n",
      "If R\n",
      "′ is a ma...\n",
      "No images found on page 26\n",
      "Page 27 Text: Now we show that the large entries of QQ⊤form a block diagonal matrix. With high probability, the\n",
      "large entries come from intra-cluster dot product, and the small entries come from inter-cluster dot product.\n",
      "We bound the intra-cluster dot product:\n",
      "z⊤\n",
      "ijzik = (ci + rij)⊤(ci + rik)\n",
      "= ∥ci∥2 + c⊤\n",
      "i rij + c⊤\n",
      "i rik + r⊤\n",
      "ijrik.\n",
      "Similar to the argument above, by concentration of measure, ∥ci∥2 ∈[1 + ϵ∆, 1 −ϵ∆] with high probability\n",
      "(we will pick ϵ = θ(∆)). The cross terms c⊤\n",
      "i rij and c⊤\n",
      "i rik can be bo...\n",
      "No images found on page 27\n",
      "Page 28 Text: We’ll need the following function for our low-rank argument:\n",
      "fk(x) =\n",
      "k\n",
      "X\n",
      "i=0\n",
      "xi\n",
      "i! .\n",
      "Note that f∞(x) = ex.\n",
      "Deﬁnition 1. Let ϵ ∈(0, 1/10) and L > 0. We say a function f : R →R is (ϵ, L)-close to ey if\n",
      "|ey −f(y)| ≤ϵ\n",
      "for any y ∈[−L, L].\n",
      "Lemma 8. For any ϵ ∈(0, 1/10) and L > 0. If\n",
      "D ≥10(L + log(1/ϵ))\n",
      "then function fD(y) is (ϵ, L)-close to ey.\n",
      "Proof. Recall the deﬁnition of function fD,\n",
      "ex = fD(x) +\n",
      "∞\n",
      "X\n",
      "i=D+1\n",
      "xi\n",
      "i! ,\n",
      "It is suﬃcient to show that |ey −f(y)| < ϵ if we have\n",
      "xD+1\n",
      "(D + 1)! ≤ϵ\n",
      "2,\n",
      "We can sho...\n",
      "No images found on page 28\n",
      "Page 29 Text: Small β range,\n",
      "i.e., β is o\n",
      "\u0010\n",
      "log n\n",
      "log d\n",
      "\u0011\n",
      ".\n",
      "Low rank approximation: R = fk∗(b · A).\n",
      "Since each entry of A is in [−1, 1], each entry of β · A is in [−β, β]. But note that β in this case is\n",
      "o\n",
      "\u0010\n",
      "log n\n",
      "d\n",
      "\u0011\n",
      "= O(log n · ∆). By the deﬁnition of k∗, each entry of exp(β · A) −fk∗(β · A) has absolute value ≤ϵ.\n",
      "Therefore the overall error is ≤ϵn.\n",
      "For sparse only: By assumption, m = Ω(∥L∥0) entries in A are ≥0, which are exactly the entries in\n",
      "exp(β · A) that are ≥1. Hence any (say) m\n",
      "2 sparse approximati...\n",
      "No images found on page 29\n",
      "Page 30 Text: D.3\n",
      "Scatterbrain: Analysis\n",
      "Here we prove Theorem 2, which shows that Scatterbrain approximation is unbiased and analyses its variance.\n",
      "We restate the theorem here for the reader’s convenience.\n",
      "Theorem. Deﬁne σ(q, k) = exp(q⊤k), bσpfe as Performer’s estimator and bσsbe as Scatterbrain estimator.\n",
      "Denote Sd−1 ⊂Rd as the unit sphere. Suppose q, k ∈Sd−1 are such that ∥q −k∥< τ. Then:\n",
      "E[bσsbe(q, k)] = σ(q, k),\n",
      "Var[bσsbe(q, k)] = (1 −p) · Var[bσpfe(q, k)] < Var[bσpfe(q, k)]\n",
      "where p = exp(−\n",
      "τ 2\n",
      "4−τ 2 ln...\n",
      "No images found on page 30\n",
      "Page 31 Text: E\n",
      "Additional Experiments and Details\n",
      "E.1\n",
      "Datasets\n",
      "ImageNet [25]: ImageNet is one of the most widely-used image classiﬁcation benchmarks. In our experiments\n",
      "in Section 5.1 of evaluating the approximation accuracy of Scatterbrain, both BigGAN and Vision Transformer\n",
      "are pre-trained on this dataset. It has roughly 1.2 million training images and 50,000 validation images.\n",
      "WikiText103 [45] and Copy [36]: WikiText103 is a popular dataset for auto-regressive models. It is\n",
      "from a collection of over 100 m...\n",
      "No images found on page 31\n",
      "Page 32 Text: Table 4: The performance of Scatterbrain, reformer, performer and Full-Attention on Long-Range-Arena\n",
      "benchmarks and 2 popular language modeling tasks. We ﬁx the same number of parameters (1/8 of the full)\n",
      "used for approximating the attention matrix for each method.\n",
      "Attention\n",
      "Copy (ppl)\n",
      "WikiText-103 (ppl)\n",
      "Full Attention\n",
      "1\n",
      "25.258±0.37\n",
      "Reformer\n",
      "6.8±0.64\n",
      "27.68±0.53\n",
      "Performer\n",
      "49±2.7\n",
      "66±5.8\n",
      "Scatterbrain\n",
      "2.58±0.21\n",
      "26.72±0.44\n",
      "Attention\n",
      "ListOps\n",
      "Text\n",
      "Retrieval\n",
      "Image\n",
      "Pathﬁnder\n",
      "Avg\n",
      "Full Attention\n",
      "38.2±0.17\n",
      "...\n",
      "No images found on page 32\n",
      "Page 33 Text: E.3\n",
      "More Ablation Studies\n",
      "E.3.1\n",
      "Memory Budget\n",
      "We present an ablation study on the parameter budget for the WikiText-103 language modeling task. We\n",
      "show that Scatterbrain outperforms its sparse and low-rank baselines across a range of parameter budgets.\n",
      "The results are presented in Table 5.\n",
      "Analysis: We have observed that Scatterbrain outperforms its sparse and low-rank baselines under\n",
      "diﬀerent memory budgets. Similar to what we found in Section 5.2, Performer does not train stably even\n",
      "with 1\n",
      "4 ...\n",
      "No images found on page 33\n",
      "Page 34 Text: Table 7: Additional experiments for Local attention on the Copy and Wikitext-103 language modeling task.\n",
      "Attention\n",
      "Copy (ppl)\n",
      "WikiText-103 (ppl)\n",
      "Full Attention\n",
      "1\n",
      "25.258\n",
      "Reformer\n",
      "6.8\n",
      "27.68\n",
      "Performer\n",
      "49\n",
      "66\n",
      "Local\n",
      "53\n",
      "30.72\n",
      "Scatterbrain\n",
      "2.58\n",
      "26.72\n",
      "The conclusion for language modeling tasks is that sparse+low-rank has the smallest approximation error\n",
      "in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also\n",
      "conﬁrms the observation in the popular benchma...\n",
      "No images found on page 34\n",
      "Finished processing 2110.15343.pdf\n",
      "Processing: 2001.08361.pdf\n",
      "Total pages: 30\n",
      "Page 1 Text: Scaling Laws for Neural Language Models\n",
      "Jared Kaplan ∗\n",
      "Johns Hopkins University, OpenAI\n",
      "jaredk@jhu.edu\n",
      "Sam McCandlish∗\n",
      "OpenAI\n",
      "sam@openai.com\n",
      "Tom Henighan\n",
      "OpenAI\n",
      "henighan@openai.com\n",
      "Tom B. Brown\n",
      "OpenAI\n",
      "tom@openai.com\n",
      "Benjamin Chess\n",
      "OpenAI\n",
      "bchess@openai.com\n",
      "Rewon Child\n",
      "OpenAI\n",
      "rewon@openai.com\n",
      "Scott Gray\n",
      "OpenAI\n",
      "scott@openai.com\n",
      "Alec Radford\n",
      "OpenAI\n",
      "alec@openai.com\n",
      "Jeffrey Wu\n",
      "OpenAI\n",
      "jeffwu@openai.com\n",
      "Dario Amodei\n",
      "OpenAI\n",
      "damodei@openai.com\n",
      "Abstract\n",
      "We study empirical scaling laws for language model pe...\n",
      "No images found on page 1\n",
      "Page 2 Text: Contents\n",
      "1\n",
      "Introduction\n",
      "2\n",
      "2\n",
      "Background and Methods\n",
      "6\n",
      "3\n",
      "Empirical Results and Basic Power Laws\n",
      "7\n",
      "4\n",
      "Charting the Inﬁnite Data Limit and Overﬁtting\n",
      "10\n",
      "5\n",
      "Scaling Laws with Model Size and Training Time\n",
      "12\n",
      "6\n",
      "Optimal Allocation of the Compute Budget\n",
      "14\n",
      "7\n",
      "Related Work\n",
      "18\n",
      "8\n",
      "Discussion\n",
      "18\n",
      "Appendices\n",
      "20\n",
      "A Summary of Power Laws\n",
      "20\n",
      "B\n",
      "Empirical Model of Compute-Efﬁcient Frontier\n",
      "20\n",
      "C Caveats\n",
      "22\n",
      "D Supplemental Figures\n",
      "23\n",
      "1\n",
      "Introduction\n",
      "Language provides a natural domain for the study of artiﬁcial intelligence,...\n",
      "No images found on page 2\n",
      "Page 3 Text: Dataset Size \n",
      "tokens\n",
      "Parameters \n",
      "non-embedding\n",
      "Compute \n",
      "PF-days, non-embedding\n",
      "Test Loss\n",
      "Figure 1\n",
      "Language modeling performance improves smoothly as we increase the model size, datasetset\n",
      "size, and amount of compute2 used for training. For optimal performance all three factors must be scaled\n",
      "up in tandem. Empirical performance has a power-law relationship with each individual factor when not\n",
      "bottlenecked by the other two.\n",
      "Performance depends strongly on scale, weakly on model shape:\n",
      "Model perfor...\n",
      "No images found on page 3\n",
      "Page 4 Text: Larger models require fewer samples \n",
      "to reach the same performance\n",
      "10\n",
      "8\n",
      "6\n",
      "4\n",
      "The optimal model size grows smoothly \n",
      "with the loss target and compute budget\n",
      "Line color indicates \n",
      "number of parameters\n",
      "107\n",
      "109\n",
      "1011\n",
      "Tokens Processed\n",
      "Compute (PF-days)\n",
      "10-9\n",
      "10-6\n",
      "10-3\n",
      "100\n",
      "Test Loss\n",
      "Compute-eﬃcient \n",
      "training stops far \n",
      "short of convergence\n",
      "103\n",
      "109\n",
      "106\n",
      "103 Params\n",
      "109 Params\n",
      "10\n",
      "8\n",
      "6\n",
      "4\n",
      "Figure 2\n",
      "We show a series of language model training runs, with models ranging in size from 103 to 109\n",
      "parameters (excluding...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: 107\n",
      "108\n",
      "109\n",
      "1010\n",
      "Tokens in Dataset\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "Loss\n",
      "Loss vs Model and Dataset Size\n",
      "Params\n",
      "708M\n",
      "302M\n",
      "85M\n",
      "3M\n",
      "25M\n",
      "393.2K\n",
      "104\n",
      "105\n",
      "Estimated Smin\n",
      "2.4\n",
      "2.8\n",
      "3.2\n",
      "3.6\n",
      "4.0\n",
      "4.4\n",
      "Loss\n",
      "Loss vs Model Size and Training Steps\n",
      "106\n",
      "107\n",
      "108\n",
      "Parameters (non-embed)\n",
      "Figure 4\n",
      "Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model\n",
      "size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model\n",
      "sizes N can be ﬁt with Equation (1.6)...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: be maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total\n",
      "compute as a power law (see Equation (1.3)).\n",
      "We provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their\n",
      "implications for training time, and a breakdown of our results per token. We also make some brief compar-\n",
      "isons to LSTMs and recurrent Transformers [DGV+18].\n",
      "1.3\n",
      "Notation\n",
      "We use the following notation:\n",
      "• L – the cross entropy loss in nats. Typica...\n",
      "No images found on page 6\n",
      "Page 7 Text: Operation\n",
      "Parameters\n",
      "FLOPs per Token\n",
      "Embed\n",
      "(nvocab + nctx) dmodel\n",
      "4dmodel\n",
      "Attention: QKV\n",
      "nlayerdmodel3dattn\n",
      "2nlayerdmodel3dattn\n",
      "Attention: Mask\n",
      "—\n",
      "2nlayernctxdattn\n",
      "Attention: Project\n",
      "nlayerdattndmodel\n",
      "2nlayerdattndembd\n",
      "Feedforward\n",
      "nlayer2dmodeldﬀ\n",
      "2nlayer2dmodeldﬀ\n",
      "De-embed\n",
      "—\n",
      "2dmodelnvocab\n",
      "Total (Non-Embedding)\n",
      "N = 2dmodelnlayer (2dattn + dﬀ)\n",
      "Cforward = 2N + 2nlayernctxdattn\n",
      "Table 1\n",
      "Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading\n",
      "terms such as nonlineariti...\n",
      "No images found on page 7\n",
      "Page 8 Text: Feed-Forward Ratio (dff / dmodel) \n",
      "50M Parameters\n",
      "Aspect Ratio (dmodel / nlayer)\n",
      "Attention Head Dimension (dmodel / nhead) \n",
      "25M Parameters\n",
      "10%\n",
      "8%\n",
      "6%\n",
      "4%\n",
      "2%\n",
      "0%\n",
      "Loss Increase\n",
      "A wide range of architectures \n",
      "achieve similar performance\n",
      "22% additional compute \n",
      "compensates for 1% loss increase\n",
      "Figure 5\n",
      "Performance depends very mildly on model shape when the total number of non-embedding\n",
      "parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences\n",
      "in param...\n",
      "No images found on page 8\n",
      "Page 9 Text: LSTM plateaus after <100 tokens \n",
      "Transformer improves through the whole context\n",
      "2M\n",
      "200M\n",
      "3M\n",
      "300M\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "6\n",
      "Token Index in Context\n",
      "103\n",
      "102\n",
      "101\n",
      "Transformers asymptotically outperform LSTMs \n",
      "due to improved use of long contexts\n",
      "3.6\n",
      "4.2\n",
      "3.0\n",
      "2.4\n",
      "4.8\n",
      "5.4\n",
      "105\n",
      "108\n",
      "106\n",
      "107\n",
      "109\n",
      "Parameters (non-embedding)\n",
      "Transformers\n",
      "LSTMs\n",
      "1 Layer\n",
      "2 Layers\n",
      "4 Layers\n",
      "Test Loss\n",
      "Per-token \n",
      "Test Loss\n",
      "Parameters:\n",
      "400K\n",
      "400K\n",
      "Figure 7\n",
      "To observe these trends it is crucial to study performance as a function of N; if we instead use t...\n",
      "No images found on page 9\n",
      "Page 10 Text: 104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "Parameters (non-embedding)\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Test Loss\n",
      "WebText2 (Test)\n",
      "Internet Books\n",
      "Books\n",
      "Wikipedia\n",
      "Common Crawl\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "5.0\n",
      "Test Loss on Training Distribution\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "5.0\n",
      "Loss on Other Distribution\n",
      "Books during training\n",
      "Wikipedia during training\n",
      "Books at convergence\n",
      "Wikipedia at convergence\n",
      "Figure 8\n",
      "Left: Generalization performance to other data distributions improves smoothly with model size,\n",
      "with only a small and very slowly growing offset from the We...\n",
      "No images found on page 10\n",
      "Page 11 Text: 106\n",
      "107\n",
      "108\n",
      "109\n",
      "Params (non-embed)\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "Test Loss\n",
      "Data Size Bottleneck\n",
      "Data Size\n",
      "21M\n",
      "43M\n",
      "86M\n",
      "172M\n",
      "344M\n",
      "688M\n",
      "1.4B\n",
      "22.0B\n",
      "10\n",
      "4\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "N\n",
      "N/\n",
      "D/D\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "L/L(D =\n",
      ")\n",
      "1\n",
      "Overfitting\n",
      "Data Size\n",
      "21M\n",
      "43M\n",
      "86M\n",
      "172M\n",
      "344M\n",
      "688M\n",
      "1.4B\n",
      "22.0B\n",
      "Figure 9\n",
      "The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N\n",
      "according to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller ﬁxed\n",
      "D, performance stops improvi...\n",
      "No images found on page 11\n",
      "Page 12 Text: 101\n",
      "3 × 100\n",
      "4 × 100\n",
      "6 × 100\n",
      "WebText2 Train Loss\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "Critical Batch Size (Tokens)\n",
      "Critical Batch Size vs. Performance\n",
      "Empirical Bcrit, N = 3M\n",
      "Empirical Bcrit, N = 85M\n",
      "Bcrit = 2.1 × 108 tokens L\n",
      "4.8\n",
      "Noise Scale Measurement\n",
      "Figure 10\n",
      "The critical batch size Bcrit follows a power law in the loss as performance increase, and does\n",
      "not depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every\n",
      "13% decrease in loss. Bcrit is measured empirically fr...\n",
      "No images found on page 12\n",
      "Page 13 Text: prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that\n",
      "has been attained. These results can be used to predict how training time and compute will vary with the\n",
      "batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch\n",
      "size B ≈Bcrit. Training at B ≫Bcrit minimizes the number of training steps, while B ≪Bcrit minimizes\n",
      "the use of compute.\n",
      "More speciﬁcally, it was demonstrated that for ...\n",
      "No images found on page 13\n",
      "Page 14 Text: 104\n",
      "106\n",
      "108\n",
      "Parameters (non-embedding)\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Test Loss\n",
      "Performance vs Compute Budget\n",
      "10\n",
      "5\n",
      "10\n",
      "4\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "100\n",
      "PF-dayss\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "Parameters (non-embedding)\n",
      "2.4\n",
      "3.0\n",
      "3.6\n",
      "4.2\n",
      "4.8\n",
      "5.4\n",
      "Test Loss\n",
      "Performance vs Steps\n",
      "104\n",
      "105\n",
      "Steps\n",
      "Figure 11\n",
      "When we hold either total compute or number of training steps ﬁxed, performance follows\n",
      "L(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that\n",
      "maximizes performance. Mediocre ﬁts at small S are unsurpri...\n",
      "Extracted 2 images from page 14\n",
      "Page 15 Text: Models between 0.6x and 2.2x the \n",
      "optimal size can be trained with a \n",
      "20% larger compute budget\n",
      "Smaller models require \n",
      "more steps to train, while \n",
      "larger models require fewer\n",
      "Our framework does not \n",
      "capture early training dynamics\n",
      "Figure 12\n",
      "Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger\n",
      "or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-\n",
      "efﬁcient size require fewer steps to train, allowing for po...\n",
      "No images found on page 15\n",
      "Page 16 Text: 10\n",
      "7\n",
      "10\n",
      "5\n",
      "10\n",
      "3\n",
      "10\n",
      "1\n",
      "Compute (PF-days), non-embedding\n",
      "103\n",
      "105\n",
      "107\n",
      "Parameters (non-embedding)\n",
      "N = (1.3 109) C0.73\n",
      "min\n",
      "N = (1.6 109) C0.88\n",
      "10\n",
      "7\n",
      "10\n",
      "5\n",
      "10\n",
      "3\n",
      "10\n",
      "1\n",
      "Compute (PF-days), excluding embeddings\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "Steps\n",
      "Smin (adjusted)\n",
      "Smin = (5.4 103) C0.03\n",
      "min\n",
      "S (fixed-batch)\n",
      "Figure 14\n",
      "Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal\n",
      "model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number\n",
      "of data example...\n",
      "No images found on page 16\n",
      "Page 17 Text: The intersection point is sensitive to \n",
      "the precise power-law parameters\n",
      "Figure 15\n",
      "Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations\n",
      "for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection\n",
      "marks the point before which we expect our predictions to break down. The location of this point is highly\n",
      "sensitive to the precise exponents from our power-law ﬁts.\n",
      "6.3\n",
      "Contradictions and a Conjecture\n",
      "We observe no ...\n",
      "No images found on page 17\n",
      "Page 18 Text: One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\n",
      "size beyond N ∗without qualitatively different data requirements, perhaps this means that once we reach\n",
      "C∗\n",
      "min and N ∗, we have extracted all of the reliable information available in natural language data. In this\n",
      "interpretation, L∗would provide a rough estimate for the entropy-per-token7 of natural language. In this\n",
      "scenario, we would expect the loss trend to level off at or before L∗.\n",
      "W...\n",
      "No images found on page 18\n",
      "Page 19 Text: We were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when\n",
      "these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\n",
      "of overﬁtting, early stopping step, and data requirements when training large language models. So our scaling\n",
      "relations go beyond mere observation to provide a predictive framework. One might interpret these relations\n",
      "as analogues of the ideal gas law, which relates the macroscopic pr...\n",
      "No images found on page 19\n",
      "Page 20 Text: Appendices\n",
      "A\n",
      "Summary of Power Laws\n",
      "For easier reference, we provide a summary below of the key trends described throughout the paper.\n",
      "Parameters\n",
      "Data\n",
      "Compute\n",
      "Batch Size\n",
      "Equation\n",
      "N\n",
      "∞\n",
      "∞\n",
      "Fixed\n",
      "L (N) = (Nc/N)αN\n",
      "∞\n",
      "D\n",
      "Early Stop\n",
      "Fixed\n",
      "L (D) = (Dc/D)αD\n",
      "Optimal\n",
      "∞\n",
      "C\n",
      "Fixed\n",
      "L (C) = (Cc/C)αC (naive)\n",
      "Nopt\n",
      "Dopt\n",
      "Cmin\n",
      "B ≪Bcrit\n",
      "L (Cmin) =\n",
      "\u0000Cmin\n",
      "c\n",
      "/Cmin\n",
      "\u0001αmin\n",
      "C\n",
      "N\n",
      "D\n",
      "Early Stop\n",
      "Fixed\n",
      "L (N, D) =\n",
      "\u0014\u0000 Nc\n",
      "N\n",
      "\u0001 αN\n",
      "αD + Dc\n",
      "D\n",
      "\u0015αD\n",
      "N\n",
      "∞\n",
      "S steps\n",
      "B\n",
      "L (N, S) =\n",
      "\u0000 Nc\n",
      "N\n",
      "\u0001αN +\n",
      "\u0010\n",
      "Sc\n",
      "Smin(S,B)\n",
      "\u0011αS\n",
      "Table 4\n",
      "The empirical ﬁtted values for ...\n",
      "No images found on page 20\n",
      "Page 21 Text: the compute budget. We start with the Equation (1.6), repeated here for convenience:\n",
      "L (N, S) =\n",
      "\u0012Nc\n",
      "N\n",
      "\u0013αN\n",
      "+\n",
      "\u0012Sc\n",
      "S\n",
      "\u0013αS\n",
      ".\n",
      "(B.1)\n",
      "Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],\n",
      "which was deﬁned in Equation (5.2)9:\n",
      "B (L) =\n",
      "B∗\n",
      "L1/αB .\n",
      "(B.2)\n",
      "We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =\n",
      "C/ (6NB (L)), where C is the number of FLOPs used in the training run:\n",
      "L (N, C) =\n",
      "\u0012Nc\n",
      "N\n",
      "\u0013αN\n",
      "+\n",
      "\u0012\n",
      "6B∗Sc\n",
      "N\n",
      "L1/αBC...\n",
      "No images found on page 21\n",
      "Page 22 Text: B.3\n",
      "Comparison to Inefﬁcient\n",
      "Typically, researchers train models until they appear to be close to convergence. In this section, we compare\n",
      "the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor\n",
      "f as the percent deviation from the converged loss:\n",
      "L (N, C) = (1 + f) L (N, ∞) .\n",
      "(B.11)\n",
      "For compute-efﬁcient training we have f = αN/αS ≈10% from the previous section, but researchers\n",
      "typically use a much smaller value. Here, we choose f ′ = 2% as an...\n",
      "No images found on page 22\n",
      "Page 23 Text: 103\n",
      "104\n",
      "105\n",
      "Sc × [L(N, D)\n",
      "L(N,\n",
      ")]\n",
      "1/\n",
      "S\n",
      "103\n",
      "104\n",
      "105\n",
      "Sstop\n",
      "Early Stopping Step\n",
      "Data Size\n",
      "21M\n",
      "43M\n",
      "86M\n",
      "172M\n",
      "344M\n",
      "688M\n",
      "1.4B\n",
      "103\n",
      "104\n",
      "105\n",
      "Step\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Loss\n",
      "Test Loss\n",
      "Train Loss\n",
      "108\n",
      "109\n",
      "1010\n",
      "Dataset Size (Tokens)\n",
      "Figure 16\n",
      "Left: We characterize the step on which early stopping occurs, as a function of the extent of\n",
      "overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:\n",
      "We display train and test loss for a series of 300M parameter models trained on di...\n",
      "Extracted 1 images from page 23\n",
      "Page 24 Text: 105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "Parameters, including reuse (non-embedding)\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "Test Loss\n",
      "2x Reuse\n",
      "4x Reuse\n",
      "8x Reuse\n",
      "Non-recurrent Models\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "Parameters (non-embedding)\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "4.0\n",
      "4.5\n",
      "Test Loss\n",
      "2x Reuse\n",
      "4x Reuse\n",
      "8x Reuse\n",
      "Non-recurrent Models\n",
      "Figure 17\n",
      "We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-\n",
      "formers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,\n",
      "but slightly worse when accou...\n",
      "Extracted 4 images from page 24\n",
      "Page 25 Text: 100\n",
      "101\n",
      "102\n",
      "103\n",
      "Token Index\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Per-Token Test Loss\n",
      "4.0 + 3.2 T\n",
      "0.47\n",
      "3.4 + 4.0 T\n",
      "0.56\n",
      "2.9 + 4.5 T\n",
      "0.56\n",
      "2.7 + 4.9 T\n",
      "0.60\n",
      "2.4 + 5.1 T\n",
      "0.61\n",
      "2.3 + 5.4 T\n",
      "0.62\n",
      "106\n",
      "107\n",
      "108\n",
      "Model Parameters\n",
      "101\n",
      "103\n",
      "105\n",
      "Step\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "Test Loss\n",
      "Per-token Loss (774M Params)\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Token Index\n",
      "Figure 20\n",
      "This ﬁgure provides information about the performance per token as a function of model size\n",
      "and training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales\n",
      "p...\n",
      "Extracted 2 images from page 25\n",
      "Page 26 Text: 0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "Step\n",
      "0.0000\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.0010\n",
      "Learning Rate\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "LR Summed Over Steps\n",
      "3.65\n",
      "3.70\n",
      "3.75\n",
      "3.80\n",
      "3.85\n",
      "3.90\n",
      "Loss\n",
      "Figure 22\n",
      "We test a variety of learning rate schedules including cosine decay, linear decay, as well as other\n",
      "faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we\n",
      "do not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.\n",
      "We ...\n",
      "No images found on page 26\n",
      "Page 27 Text: 101\n",
      "102\n",
      "Depth\n",
      "2.3\n",
      "2.4\n",
      "2.5\n",
      "2.6\n",
      "2.7\n",
      "2.8\n",
      "Test Loss\n",
      "Wikipedia\n",
      "Books\n",
      "Internet Books\n",
      "Common Crawl\n",
      "WebText2 (Train)\n",
      "WebText2 (Test)\n",
      "Figure 24\n",
      "We show evaluations on a series of datasets for models with approximately 1.5 Billion param-\n",
      "eters. We observe no effect of depth on generalization; generalization performance depends primarily on\n",
      "training distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the\n",
      "early-stopped performance; we have not seen this surprising resu...\n",
      "No images found on page 27\n",
      "Page 28 Text: List of Tables\n",
      "1\n",
      "Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "2\n",
      "Fits to L(N, D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "11\n",
      "3\n",
      "Fits to L(N, S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "14\n",
      "4\n",
      "Key trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "5\n",
      "Key parameters to trend ﬁts . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "No images found on page 28\n",
      "Page 29 Text: [HCC+18]\n",
      "Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\n",
      "and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\n",
      "CoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19\n",
      "[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\n",
      "ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\n",
      "dictable, empirically, 2017, 1712.00409. 18...\n",
      "No images found on page 29\n",
      "Page 30 Text: [SLA+18]\n",
      "Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\n",
      "George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,\n",
      "arXiv:1811.03600. 12\n",
      "[SS18]\n",
      "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\n",
      "cost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.\n",
      "7\n",
      "[THK18]\n",
      "Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.\n",
      "O...\n",
      "No images found on page 30\n",
      "Finished processing 2001.08361.pdf\n",
      "Processing: 2104.09864.pdf\n",
      "Total pages: 14\n",
      "Page 1 Text: ROFORMER: ENHANCED TRANSFORMER WITH ROTARY\n",
      "POSITION EMBEDDING\n",
      "Jianlin Su\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "bojonesu@wezhuiyi.com\n",
      "Yu Lu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "julianlu@wezhuiyi.com\n",
      "Shengfeng Pan\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "nickpan@wezhuiyi.com\n",
      "Ahmed Murtadha\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "mengjiayi@wezhuiyi.com\n",
      "Bo Wen\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "brucewen@wezhuiyi.com\n",
      "Yunfeng Liu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "glenliu@wezhuiyi.com\n",
      "November 9, 2023\n",
      "ABS...\n",
      "No images found on page 1\n",
      "Page 2 Text: RoFormer\n",
      "It is noteworthy that the self-attention architecture of the current PLMs has shown to be position-agnostic Yun et al.\n",
      "[2020]. Following this claim, various approaches have been proposed to encode the position information into the\n",
      "learning process. On one side, generated absolute position encoding through a pre-defined function Vaswani et al.\n",
      "[2017] was added to the contextual representations, while a trainable absolute position encoding Gehring et al. [2017],\n",
      "Devlin et al. [2019], Lan ...\n",
      "No images found on page 2\n",
      "Page 3 Text: RoFormer\n",
      "representation.\n",
      "am,n =\n",
      "exp( q⊺\n",
      "mkn\n",
      "√\n",
      "d )\n",
      "PN\n",
      "j=1 exp( q⊺\n",
      "mkj\n",
      "√\n",
      "d )\n",
      "om =\n",
      "N\n",
      "X\n",
      "n=1\n",
      "am,nvn\n",
      "(2)\n",
      "The existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form\n",
      "Equation (1).\n",
      "2.2\n",
      "Absolute position embedding\n",
      "A typical choice of Equation (1) is\n",
      "ft:t∈{q,k,v}(xi, i) := W t:t∈{q,k,v}(xi + pi),\n",
      "(3)\n",
      "where pi ∈Rd is a d-dimensional vector depending of the position of token xi. Previous work Devlin et al. [2019],\n",
      "Lan et al. [2020], Clark et al. [2020]...\n",
      "No images found on page 3\n",
      "Page 4 Text: RoFormer\n",
      "The authors of He et al. [2020] argued that the relative positions of two tokens could only be fully modeled using\n",
      "the middle two terms of Equation (6). As a consequence, the absolute position embeddings pm and pn were simply\n",
      "replaced with the relative position embeddings ˜pm−n:\n",
      "q⊺\n",
      "mkn = x⊺\n",
      "mW ⊺\n",
      "qW kxn + x⊺\n",
      "mW ⊺\n",
      "qW k˜pm−n + ˜p⊺\n",
      "m−nW ⊺\n",
      "qW kxn\n",
      "(10)\n",
      "A comparison of the four variants of the relative position embeddings Radford and Narasimhan [2018] has shown\n",
      "that the variant similar to Equa...\n",
      "No images found on page 4\n",
      "Page 5 Text: RoFormer\n",
      "3.2.2\n",
      "General form\n",
      "In order to generalize our results in 2D to any xi ∈Rd where d is even, we divide the d-dimension space into d/2\n",
      "sub-spaces and combine them in the merit of the linearity of the inner product, turning f{q,k} into:\n",
      "f{q,k}(xm, m) = Rd\n",
      "Θ,mW {q,k}xm\n",
      "(14)\n",
      "where\n",
      "Rd\n",
      "Θ,m =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cos mθ1\n",
      "−sin mθ1\n",
      "0\n",
      "0\n",
      "· · ·\n",
      "0\n",
      "0\n",
      "sin mθ1\n",
      "cos mθ1\n",
      "0\n",
      "0\n",
      "· · ·\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "cos mθ2\n",
      "−sin mθ2\n",
      "· · ·\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "sin mθ2\n",
      "cos mθ2\n",
      "· · ·\n",
      "0\n",
      "0\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "· · ·\n",
      "cos mθd/2\n",
      "−sin mθd/2\n",
      "0...\n",
      "No images found on page 5\n",
      "Page 6 Text: RoFormer\n",
      "Attention(Q, K, V)m =\n",
      "PN\n",
      "n=1 sim(qm, kn)vn\n",
      "PN\n",
      "n=1 sim(qm, kn)\n",
      ".\n",
      "(17)\n",
      "The original self-attention chooses sim(qm, kn) = exp(q⊺\n",
      "mkn/\n",
      "√\n",
      "d). Note that the original self-attention should\n",
      "compute the inner product of query and key for every pair of tokens, which has a quadratic complexity O(N 2). Follow\n",
      "Katharopoulos et al. [2020], the linear attentions reformulate Equation (17) as\n",
      "Attention(Q, K, V )m =\n",
      "PN\n",
      "n=1 ϕ(qm)⊺φ(kn)vn\n",
      "PN\n",
      "n=1 ϕ(qm)⊺φ(kn)\n",
      ",\n",
      "(18)\n",
      "where ϕ(·), φ(·) are usually non-negative ...\n",
      "No images found on page 6\n",
      "Page 7 Text: RoFormer\n",
      "with the corresponding initial condition as:\n",
      "q = ∥q∥eiθq = Rq(xq, 0)eiΘq(xq,0),\n",
      "k = ∥k∥eiθk = Rk(xk, 0)eiΘk(xk,0),\n",
      "(25)\n",
      "where ∥q∥, ∥k∥and θq, θk are the radial and angular part of q and k on the 2D plane.\n",
      "Next, we set m = n in Equation (24) and take into account initial conditions in Equation (25):\n",
      "Rq(xq, m)Rk(xk, m) = Rg(xq, xk, 0) = Rq(xq, 0)Rk(xk, 0) = ∥q∥∥k∥,\n",
      "(26a)\n",
      "Θk(xk, m) −Θq(xq, m) = Θg(xq, xk, 0) = Θk(xk, 0) −Θq(xq, 0) = θk −θq.\n",
      "(26b)\n",
      "On one hand, from, a straightforward soluti...\n",
      "No images found on page 7\n",
      "Page 8 Text: RoFormer\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "relative distance\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "relative upper bound\n",
      "Figure 2: Long-term decay of RoPE.\n",
      "3.4.3\n",
      "Long-term decay of RoPE\n",
      "We can group entries of vectors q = W qxm and k = W kxn in pairs, and the inner product of RoPE in Equation (16)\n",
      "can be written as a complex number multiplication.\n",
      "(Rd\n",
      "Θ,mW qxm)⊺(Rd\n",
      "Θ,nW kxn) = Re\n",
      "\u0014 d/2−1\n",
      "X\n",
      "i=0\n",
      "q[2i:2i+1]k∗\n",
      "[2i:2i+1]ei(m−n)θi\n",
      "\u0015\n",
      "(35)\n",
      "where q[2i:2i+1] represents the 2ith to (2i + 1)th entries of q.\n",
      "Denote hi = q[2i:2i+1]k∗\n",
      "[2i:2i+1...\n",
      "No images found on page 8\n",
      "Page 9 Text: RoFormer\n",
      "Table 1: The proposed RoFormer gives better BLEU scores compared to its baseline alternative Vaswani et al. [2017]\n",
      "on the WMT 2014 English-to-German translation taskBojar et al. [2014].\n",
      "Model\n",
      "BLEU\n",
      "Transformer-baseVaswani et al. [2017]\n",
      "27.3\n",
      "RoFormer\n",
      "27.5\n",
      "Section (4.4). By the end, additional tests on Chinese data are included in Section (4.5). All the experiments were run\n",
      "on two cloud severs with 4 x V100 GPUs.\n",
      "4.1\n",
      "Machine Translation\n",
      "We first demonstrate the performance of RoFormer on s...\n",
      "No images found on page 9\n",
      "Page 10 Text: RoFormer\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "Train Steps (K)\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "MLM Loss\n",
      "RoFormer\n",
      "BERT\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Train Steps (K)\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "LM Loss\n",
      "PerFormer w/. RoPE\n",
      "PerFormer w/o. RoPE\n",
      "Figure 3: Evaluation of RoPE in language modeling pre-training. Left: training loss for BERT and RoFormer. Right:\n",
      "training loss for PerFormer with and without RoPE.\n",
      "4.3\n",
      "Fine-tuning on GLUE tasks\n",
      "Consistent with the previous experiments, we fine-tune the weights of our pre-trained RoFormer across various GLUE\n",
      "tasks...\n",
      "No images found on page 10\n",
      "Page 11 Text: RoFormer\n",
      "4.4.1\n",
      "Implementation details\n",
      "We carry out tests on the Enwik8 dataset Mahoney [2006], which is from English Wikipedia that includes markup,\n",
      "special characters and text in other languages in addition to English text. We incorporate RoPE into the 12 layer\n",
      "char-based PerFormer with 768 dimensions and 12 heads2. To better illustrate the efficacy of RoPE, we report the loss\n",
      "curves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size\n",
      "...\n",
      "No images found on page 11\n",
      "Page 12 Text: RoFormer\n",
      "of cases published by the Supreme People’s Court of China. The input triplet, denoted as (A, B and C), are fact\n",
      "descriptions of three cases. The task is to predict whether the pair (A, B) is closer than (A, C) under a predefined\n",
      "similarity measure. Note that existing methods mostly cannot perform significantly on CAIL2019-SCM dataset due to\n",
      "the length of documents (i.e., mostly more than 512 characters). We split train, validation and test sets based on the\n",
      "well-known ratio 6:2:2.\n",
      "4.5.4...\n",
      "No images found on page 12\n",
      "Page 13 Text: RoFormer\n",
      "volume 30. Curran Associates, Inc., 2017.\n",
      "URL https://proceedings.neurips.cc/paper/2017/file/\n",
      "3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n",
      "J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers\n",
      "for language understanding. In NAACL-HLT, 2019.\n",
      "A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised\n",
      "multitask learners. 2019.\n",
      "Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Ra...\n",
      "No images found on page 13\n",
      "Page 14 Text: RoFormer\n",
      "Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and\n",
      "analysis platform for natural language understanding. 04 2018.\n",
      "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, A. Gane, Tamás Sarlós, Peter Hawkins,\n",
      "J. Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention\n",
      "with performers. ArXiv, abs/2009.14794, 2020.\n",
      "Ondrej Bojar, Christian Buck, Christian Federmann...\n",
      "No images found on page 14\n",
      "Finished processing 2104.09864.pdf\n",
      "Processing: 2306.17806.pdf\n",
      "Total pages: 36\n",
      "Page 1 Text: Stay on topic with Classifier-Free Guidance\n",
      "Guillaume V. Sanchez*\n",
      "Hexaglobe\n",
      "EleutherAI\n",
      "gsanchez@hexaglobe.com\n",
      "Honglu Fan*\n",
      "University of Geneva\n",
      "EleutherAI\n",
      "honglu.fan@unige.ch\n",
      "Alexander Spangher*\n",
      "Information Sciences Institute\n",
      "University of Southern California\n",
      "spangher@usc.edu\n",
      "Elad Levi\n",
      "Sightful\n",
      "eladlevico@gmail.com\n",
      "Pawan Sasanka Ammanamanchi\n",
      "IIIT Hyderabad\n",
      "Eleuther AI\n",
      "pawansasanka@gmail.com\n",
      "Stella Biderman\n",
      "Booz Allen Hamilton\n",
      "EleutherAI\n",
      "stellabiderman@gmail.com\n",
      "Abstract\n",
      "Classifier-Free Guidance (...\n",
      "No images found on page 1\n",
      "Page 2 Text: Instruction: “Respond enthusiastically to the following user prompt.”\n",
      "Prompt: “What was the Cambridge Analytica scandal?”\n",
      "Vanilla Sampling\n",
      "Classifier Free Guidance-based Sampling\n",
      "The Cambridge Analytica scandal was a huge\n",
      "scandal in which it was revealed that Cam-\n",
      "bridge Analytica, a political consulting firm,\n",
      "had used personal data from Facebook to target\n",
      "and influence the 2016 US presidential elec-\n",
      "tion. This scandal raised questions about the\n",
      "role of social media in political campaigns...\n",
      "Oh ...\n",
      "No images found on page 2\n",
      "Page 3 Text: 2.1\n",
      "Guidance in Text-to-Image Models\n",
      "Let Pθ(x) be the unconditional generative model for an image x with parameters θ. During inference, we wish to\n",
      "condition the generation on a label or text description c in order to model P(x|c). Generative models usually generate\n",
      "data from an abstract representation z in semantic space that is decoded into an actual sample (e.g. the latent vectors in\n",
      "GANs or the intermediate sampling steps in diffusion models). Controlling the generation usually involves guid...\n",
      "No images found on page 3\n",
      "Page 4 Text: In the case of autoregressive language models modeling Pθ(w) = QN\n",
      "i Pθ(wi|wj<i), we can unroll the formulation and\n",
      "obtain Equation 2 again:\n",
      "c\n",
      "Pθ(w|c) ∝\n",
      "T\n",
      "Y\n",
      "i=1\n",
      "c\n",
      "Pθ(wi|wj<i, c) ∝\n",
      "T\n",
      "Y\n",
      "i=1\n",
      "Pθ(wi|wj<i, c)γ\n",
      "Pθ(wi|wj<i)γ−1 ∝Pθ(w|c)γ\n",
      "Pθ(w)γ−1\n",
      "(6)\n",
      "While conditioned diffusion models cannot predict unconditioned distributions without extra training, language models\n",
      "handle both Pθ(w|c) and Pθ(w) naturally due to being trained on finite context windows. Being able to drop the prefix\n",
      "c is a natural feature....\n",
      "No images found on page 4\n",
      "Page 5 Text: ARC-c\n",
      "ARC-e\n",
      "BoolQ\n",
      "HellaSwag\n",
      "GPT2-small\n",
      "22.7 / 23.0\n",
      "39.5 / 42.1\n",
      "48.7 / 57.0\n",
      "31.1 / 31.9\n",
      "GPT2-medium\n",
      "25.0 / 23.9\n",
      "43.6 / 47.6\n",
      "58.6 / 60.1\n",
      "39.4 / 40.9\n",
      "GPT2-large\n",
      "25.1 / 24.7\n",
      "46.6 / 51.0\n",
      "60.5 / 62.1\n",
      "45.3 / 47.1\n",
      "GPT2-xl\n",
      "28.5 / 30.0\n",
      "51.1 / 56.5\n",
      "61.8 / 62.6\n",
      "50.9 / 52.4\n",
      "Pythia-160M\n",
      "23.5 / 23.0\n",
      "39.5 / 42.2\n",
      "55.0 / 58.3\n",
      "30.1 / 31.2\n",
      "Pythia-410M\n",
      "24.1 / 23.8\n",
      "45.7 / 50.3\n",
      "60.6 / 61.2\n",
      "40.6 / 41.6\n",
      "Pythia-1B\n",
      "27.0 / 28.0\n",
      "49.0 / 54.9\n",
      "60.7 / 61.8\n",
      "47.1 / 48.9\n",
      "Pythia-1.4B\n",
      "28.6 / 29.6\n",
      "53.8 / 59.6\n",
      "63.0 / 63.8\n",
      "52.1 / 54.3\n",
      "...\n",
      "No images found on page 5\n",
      "Page 6 Text: Figure 3: CFG impact on chain-of-thought prompting with respect to GSM8K dataset. For small CFG values, using\n",
      "CFG increases the percentage of chains which end in a valid answer structure while increasing the model accuracy. For\n",
      "large values the invalid percentage remains small but the accuracy drop.\n",
      "We have only scratched the surface of exploring CFG’s interactions with CoT; for instance, instead of upweighting\n",
      "just wp, we might upweight wp, wcot, or other variations. We anticipate in future wor...\n",
      "Extracted 2 images from page 6\n",
      "Page 7 Text: CodeGen-350M\n",
      "CodeGen-2B\n",
      "CodeGen-6B\n",
      "γ\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "1.0\n",
      "11.0%\n",
      "17.0%\n",
      "22.0%\n",
      "19.5%\n",
      "25.5%\n",
      "29.8%\n",
      "19.5%\n",
      "25.5%\n",
      "29.8%\n",
      "1.1\n",
      "11.8%\n",
      "18.1%\n",
      "20.1%\n",
      "20.4%\n",
      "25.4%\n",
      "28.0\n",
      "20.4%\n",
      "25.4%\n",
      "28.0%\n",
      "1.25\n",
      "11.4%\n",
      "17.3%\n",
      "18.9%\n",
      "19.7%\n",
      "25.4%\n",
      "28.0\n",
      "19.7%\n",
      "25.4%\n",
      "28.0%\n",
      "1.5\n",
      "10.9%\n",
      "16.7%\n",
      "18.3%\n",
      "20.9%\n",
      "26.7%\n",
      "29.2%\n",
      "20.9%\n",
      "26.7%\n",
      "29.2\n",
      "1.75\n",
      "10.3%\n",
      "16.0%\n",
      "18.2%\n",
      "20.4%\n",
      "26.2%\n",
      "28.6%\n",
      "20.4%\n",
      "26.2%\n",
      "28.6%\n",
      "2.0\n",
      "8.6%\n",
      "14.6%\n",
      "17.6%\n",
      "16.5%\n",
      "22.4%\n",
      "24.4%\n",
      "16.5%\n",
      "22.4%\n",
      "24.4%\n",
      "Table 2: CodeGen results with temperature= 0.2. CFG in nearly all cases ...\n",
      "Extracted 2 images from page 7\n",
      "Page 8 Text: We explore CFG with negative prompting to increase the success rate of different system prompts. We set the negative\n",
      "prompt c to be the default system-prompt for the models we use (i.e. “The prompt below is a question to answer, a\n",
      "task to complete, or a conversation to respond to; decide which and write an appropriate response.”) and set c to be the\n",
      "edited prompt (e.g. “The prompt below is a question to answer, a task to complete, or a conversation to respond to;\n",
      "decide which and write a sad res...\n",
      "No images found on page 8\n",
      "Page 9 Text: (a) Entropy of logits for the vanilla prompted distribution\n",
      "P(y|x), the unprompted distribution, P(x), the CFG-γ = 1.5\n",
      "distribution and an instruction-tuned model Pinstruct(y|x).\n",
      "(b) Number of tokens overlapping in top-p=90% of vocabu-\n",
      "lary distributions between that of: CFG, that of the vanilla\n",
      "prompted model, p(y|x), and that of the unprompted model,\n",
      "P(x).\n",
      "Figure 6: We show into how CFG alters the logit distribution of the vanilla prompted model, P(y|x). CFG lowers\n",
      "the entropy to a level rough...\n",
      "Extracted 2 images from page 9\n",
      "Page 10 Text: current\n",
      "top1\n",
      "top2\n",
      "top3\n",
      "top4\n",
      "top5\n",
      "...\n",
      "bottom5\n",
      "bottom4\n",
      "bottom3\n",
      "bottom2\n",
      "bottom1\n",
      "France\n",
      "flipping\n",
      "destroying\n",
      "waking\n",
      "stopping\n",
      "causing\n",
      "...\n",
      "guiName\n",
      "ufact\n",
      "Outs\n",
      "kees\n",
      "\"}],\"\n",
      ",\n",
      "crashing\n",
      "landing\n",
      "soaring\n",
      "swoop\n",
      "plummet\n",
      "...\n",
      "soDeliveryDate\n",
      "POLIT\n",
      "Occupations\n",
      "568\n",
      "publishes\n",
      "landing\n",
      "neigh\n",
      "invis\n",
      "atop\n",
      "overhead\n",
      "omin\n",
      "...\n",
      "quotas\n",
      "Russo\n",
      "Germans\n",
      "passports\n",
      "hostages\n",
      "on\n",
      "Buildings\n",
      "skysc\n",
      "rooft\n",
      "Cheong\n",
      "Plaza\n",
      "...\n",
      "MFT\n",
      "ゼ\n",
      "醒\n",
      "DragonMagazine\n",
      "Notre\n",
      "Basil\n",
      "Mos\n",
      "Cathedral\n",
      "Mosque\n",
      "Eugene\n",
      "...\n",
      "voyage\n",
      "alach\n",
      "urse\n",
      "arb\n",
      "sb\n",
      "Dame\n",
      "Cathedral\n",
      "monument\n",
      "cathe...\n",
      "No images found on page 10\n",
      "Page 11 Text: Acknowledgements\n",
      "We are grateful to Stability and CoreWeave for providing the compute to run the evaluations.\n",
      "We also thank the volunteers who took part in the GPT4All experiment.\n",
      "Alexander Spangher would like to thank Bloomberg News for a 4 year PhD fellowship that generously funds his\n",
      "research.\n",
      "References\n",
      "[1] How does negative prompt work? https://stable-diffusion-art.com/how-negative-prompt-work/.\n",
      "[2] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, ...\n",
      "No images found on page 11\n",
      "Page 12 Text: [17] J. Chorowski and N. Jaitly. Towards better decoding and language model integration in sequence to sequence\n",
      "models. arXiv preprint arXiv:1612.02695, 2016.\n",
      "[18] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising\n",
      "difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n",
      "[19] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved\n",
      "question answering? try arc, the ai...\n",
      "No images found on page 12\n",
      "Page 13 Text: [39] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset\n",
      "for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n",
      "[40] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. Ctrl: A conditional transformer language\n",
      "model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.\n",
      "[41] G. Kim, T. Kwon, and J. C. Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In\n",
      "Proceedings of ...\n",
      "No images found on page 13\n",
      "Page 14 Text: [60] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Conference on\n",
      "Empirical Methods in Natural Language Processing, 2014.\n",
      "[61] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative\n",
      "pre-training. 2018.\n",
      "[62] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask\n",
      "learners. OpenAI blog, 1(8):9, 2019.\n",
      "[63] J. W. Rae, S. Borgeaud, T. Cai, K. Mi...\n",
      "No images found on page 14\n",
      "Page 15 Text: Garrette, D. R. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I. Winata, H. Schoelkopf,\n",
      "J.-C. Kalo, J. Novikova, J. Z. Forde, X. Tang, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu,\n",
      "N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. O.\n",
      "Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisiewicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun,\n",
      "Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feiz...\n",
      "No images found on page 15\n",
      "Page 16 Text: Appendix\n",
      "Table of Contents\n",
      "A Author Contributions\n",
      "16\n",
      "B Additional Related Works\n",
      "17\n",
      "B.1\n",
      "CFG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "B.2\n",
      "Generative Guidance in NLP\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "C Charts\n",
      "17\n",
      "C.1\n",
      "General benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "C.2\n",
      "Accuracy vs. FLOP . . . . . . . . . . . . . . . . . . . . . . . . ...\n",
      "No images found on page 16\n",
      "Page 17 Text: Stella Biderman\n",
      "supervised the process. She proofread the paper, suggested the experiments to run in 3.1 and how to\n",
      "run them with EleutherAI’s LM Harness. She suggested the GPT-J code generation experiment of section 3.3.1.\n",
      "B\n",
      "Additional Related Works\n",
      "B.1\n",
      "CFG\n",
      "The work on CFG is based on Classifier Guided Diffusion [28], which demonstrates that γ allows for trading fidelity and\n",
      "diversity. Artists using Stable Diffusion, an open-source product built on [66], commonly believe that effective prompt\n",
      "e...\n",
      "No images found on page 17\n",
      "Page 18 Text: Figure 8: Standard benchmarks over various CFG strengths for GPT2 models\n",
      "We run TriviaQA based on the LLaMA [78] methodology, however we perform substring match rather than exact match.\n",
      "This stems from manual analysis which showed that exact matching disqualified answers like \"Mark Twain\" (with\n",
      "quotes) or His name is Mark Twain instead of the exact Mark Twain.\n",
      "C.2\n",
      "Accuracy vs. FLOP\n",
      "In Section 4, we present the finding that a model using CFG can generally perform as well as a model twice as large...\n",
      "Extracted 1 images from page 18\n",
      "Page 19 Text: Figure 9: Standard benchmarks over various CFG strengths for Pythia models\n",
      "19\n",
      "...\n",
      "Extracted 1 images from page 19\n",
      "Page 20 Text: Figure 10: Standard benchmarks over various CFG strengths for LLaMA models\n",
      "20\n",
      "...\n",
      "Extracted 1 images from page 20\n",
      "Page 21 Text: p-value\n",
      "Win\n",
      "Lambada\n",
      "0.000\n",
      "CFG\n",
      "WinoGrande\n",
      "0.003\n",
      "Vanilla\n",
      "SciQ\n",
      "0.008\n",
      "CFG\n",
      "TriviaQA\n",
      "0.008\n",
      "Vanilla\n",
      "HellaSwag\n",
      "0.012\n",
      "p > .01\n",
      "PiQA\n",
      "0.030\n",
      "p > .01\n",
      "ARC-c\n",
      "0.216\n",
      "p > .01\n",
      "BoolQ\n",
      "0.345\n",
      "p > .01\n",
      "ARC-e\n",
      "0.355\n",
      "p > .01\n",
      "Table 4: ANCOVA p-value results for plots shown in Figure 11. We calculate ANCOVA on log-transformed variables\n",
      "and calculate significance at p = .01.\n",
      "temperature = 0.2\n",
      "temperature = 0.6\n",
      "temperature = 0.8\n",
      "γ\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "1.0\n",
      "11.0%\n",
      "17.0%\n",
      "22.0%\n",
      "8.9%\n",
      "18.2%\n",
      "23.7%\n",
      "7.2%\n",
      "17.2%\n",
      "29....\n",
      "No images found on page 21\n",
      "Page 22 Text: .\n",
      "Figure 11: Accuracy vs. FLOP per token at inference.\n",
      "Blue point: a model without CFG from any of the three model families (GPT-2, Pythia, LLaMA).\n",
      "Red point: a model with the best CFG from any of the three model families.\n",
      "The dashed curves: the regression curves (logistic regression between log-FLOP and accuracy) of their groups.\n",
      "22\n",
      "...\n",
      "Extracted 1 images from page 22\n",
      "Page 23 Text: temperature = 0.2\n",
      "temperature = 0.6\n",
      "temperature = 0.8\n",
      "γ\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "k=1\n",
      "k=10\n",
      "k=100\n",
      "1.0\n",
      "19.5%\n",
      "25.5%\n",
      "29.8%\n",
      "15.9%\n",
      "29.3%\n",
      "36.5%\n",
      "12.3%\n",
      "26.4%\n",
      "33.5%\n",
      "1.1\n",
      "20.4%\n",
      "25.4%\n",
      "28.0%\n",
      "16.3%\n",
      "29.3%\n",
      "36.5%\n",
      "13.8%\n",
      "29.0%\n",
      "38.3%\n",
      "1.25\n",
      "19.7%\n",
      "25.4%\n",
      "28.0%\n",
      "17.4%\n",
      "30.1%\n",
      "38.3%\n",
      "14.1%\n",
      "28.7%\n",
      "37.6%\n",
      "1.5\n",
      "20.9%\n",
      "26.7%\n",
      "29.2%\n",
      "18.3%\n",
      "31.7%\n",
      "40.1%\n",
      "14.9%\n",
      "29.1%\n",
      "36.5%\n",
      "1.75\n",
      "20.4%\n",
      "26.2%\n",
      "28.6%\n",
      "17.7%\n",
      "30.4%\n",
      "35.9%\n",
      "14.3%\n",
      "28.3%\n",
      "34.1%\n",
      "2.0\n",
      "16.5%\n",
      "22.4%\n",
      "24.4%\n",
      "13.7%\n",
      "25.2%\n",
      "32.2%\n",
      "11.3%\n",
      "23.9%\n",
      "31.6%\n",
      "Table 6: CodeGen-2B-mono results\n",
      "temperature = 0....\n",
      "Extracted 2 images from page 23\n",
      "Page 24 Text: Figure 14: CodeGen-6B-mono performance on HumanEval with various CFG strengths\n",
      "P3 Dataset\n",
      "mean\n",
      "std\n",
      "count\n",
      "Highest ⟨CFG, Instruct⟩Similarities\n",
      "SuperGLUE wsc.fixed p is are r score eval\n",
      "31.89\n",
      "+/-22.06\n",
      "42\n",
      "SciQ Multiple Choice Closed Book\n",
      "5.82\n",
      "+/-13.27\n",
      "43\n",
      "CosE v1.11 description question option text\n",
      "5.70\n",
      "+/-9.05\n",
      "43\n",
      "RottenTomatoes Writer Expressed Sentiment\n",
      "4.93\n",
      "+/-7.45\n",
      "41\n",
      "WinograndeXL fill in the blank\n",
      "4.42\n",
      "+/-10.51\n",
      "44\n",
      "RottenTomatoes Text Expressed Sentiment\n",
      "2.93\n",
      "+/-7.98\n",
      "45\n",
      "Quarel: choose between\n",
      "2.51...\n",
      "Extracted 1 images from page 24\n",
      "Page 25 Text: (a) CodeGen-350M-mono HumanEval task-by-task plot with\n",
      "temp=0.8\n",
      "Blue: CFG outperforms,\n",
      "Purple: CFG ties with the baseline,\n",
      "Red: CFG underperforms\n",
      "(b) CodeGen-350M-mono HumanEval task-by-task plot with\n",
      "temp=0.6\n",
      "Blue: CFG outperforms,\n",
      "Purple: CFG ties with the baseline,\n",
      "Red: CFG underperforms\n",
      "(c) CodeGen-350M-mono HumanEval task-by-task plot with\n",
      "temp=0.2\n",
      "Blue: CFG outperforms,\n",
      "Purple: CFG ties with the baseline,\n",
      "Red: CFG underperforms\n",
      "Figure 15: CFG impact on chain-of-thought prompting with respe...\n",
      "Extracted 5 images from page 25\n",
      "Page 26 Text: Figure 16: Comparison of (CFG-γ = 1.5, Instruct) logits across a large sample set from P3.\n",
      "26\n",
      "...\n",
      "Extracted 1 images from page 26\n",
      "Page 27 Text: Top Sentences in P3 where CFG is MOST Similar to Instruction-Tuned Models\n",
      "Build a movie plot around this: What is the team? Rag-tag bunch of girls\n",
      "Here’s a complex question that requires someone to reason about the input, can you answer it? What city was the\n",
      "capital of the Ostrogothic Kingdom and the birth place of Ornella Fiorentini?\n",
      "Who had more of their English novels turned into Oscar-nominated films, Raja Rao or Pat Conroy?\n",
      "Nokia, Texas Instruments and other leading makers of mobile phones ...\n",
      "No images found on page 27\n",
      "Page 28 Text: D\n",
      "Additional experiments\n",
      "D.1\n",
      "Machine translation\n",
      "We evaluate using Classifier-Free Guidance for machine translation on a variety of models. We choose the WMT14 fr-en\n",
      "[13] as the dataset of choice to understand if CFG would also help multilingual datasets. We run 0-shot experiments on\n",
      "Bloom-3B [72], a multilingual model trained on 49 languages. We also test on RedPajama-Incite-Base-3B, trained on\n",
      "1.5T tokens of English text and mT0 [52] a prompt tuned sequence-to-sequence model. For the Bloom-3B ...\n",
      "No images found on page 28\n",
      "Page 29 Text: Model\n",
      "γ = 1\n",
      "γ = 1.10\n",
      "γ = 1.25\n",
      "Bloom-3B\n",
      "14.16\n",
      "15.81\n",
      "14.16\n",
      "RedPajama-Incite-3B\n",
      "15.04\n",
      "17.24\n",
      "17.78\n",
      "γ = 1\n",
      "γ = 1.05\n",
      "γ = 1.10\n",
      "Bloom-3B 1-shot\n",
      "29.84\n",
      "29.19\n",
      "28.53\n",
      "mT0\n",
      "29.77\n",
      "29.41\n",
      "27.79\n",
      "Table 11: BLEU scores for different γ for machine translation tasks. In the case of 1-shot and mt0, we experiment with\n",
      "γ values between 1 and 1.1 since we see a rapid decline at even slightly higher values. All models are evaluated 0-shot\n",
      "unless otherwise specified.\n",
      "γ = 1\n",
      "not code\n",
      "C\n",
      "Java\n",
      "Python\n",
      "γ = 1.25\n",
      "not code\n",
      "C\n",
      "Java\n",
      "Pyth...\n",
      "No images found on page 29\n",
      "Page 30 Text: E\n",
      "Generation samples\n",
      "E.1\n",
      "Continuations\n",
      "We experiment with GPT2 medium and large and compare the same prompt with γ = 1 and γ = 1.5. We show\n",
      "that γ = 1.5 is intuitively preferred as it weighs the prompt more, giving more importance to the human input, thus\n",
      "performing closer to our expectations. Indeed, GPT2-medium[γ = 1.5] gives more satisfactory results than GPT2-\n",
      "large[γ = 1]. These results are not to take lightly: on our CPU, running two GPT2-medium passes per has the same\n",
      "latency of a single ...\n",
      "No images found on page 30\n",
      "Page 31 Text: The dragon flew over Paris, France descending slowly until it flew through Paris’ Cathedral and\n",
      "down into a church. Suddenly, dragon flew back again before dropping back into the church. When it\n",
      "landed on the ground, dragon screamed and cried in pain.\n",
      "The dragon’s cries were heard in France and all over the world. The dragon screamed so loud, people\n",
      "at the airport could hear the dragon’s screams. The dragon’s cries were heard worldwide for many\n",
      "years. It was reported that the dragon was able to ...\n",
      "No images found on page 31\n",
      "Page 32 Text: Question ”At Daifu university, 24% of all students are members of both a chess club and\n",
      "a swim team. If 20% of members of the swim team are not members of the chess club,\n",
      "what percentage of all Daifu students are members of the swim team?”\n",
      "Choices: ’A)20%’, ’B)30%’, ’C)40%’, ’D)50%’, ’E)60%’\n",
      "Vanilla Sampling\n",
      "CFG Sampling (γ = 1.5)\n",
      "We know that 24% of all students are in both S and C,\n",
      "which we can write as 0.24 = P(S and C). We also\n",
      "know that 20% of the swim team is not in the chess\n",
      "club, which w...\n",
      "No images found on page 32\n",
      "Page 33 Text: Instruction: “Respond seductivelyly to the following user prompt.”\n",
      "Prompt: “Tell me about the band Halocene”\n",
      "Vanilla Sampling\n",
      "Classifier Free Guidance-based Sampling\n",
      "\"Well, Halocene is a new indie rock band\n",
      "from London, UK. Their sound is a mix\n",
      "of psychedelic rock, folk, and indie rock.\n",
      "They’ve released their debut album, \"Sen-\n",
      "sory Deprivation,\" in 2021. The band’s name\n",
      "comes from the mythical creatures from Greek\n",
      "mythology. Lead singer and guitarist, Alex,\n",
      "has been in the industry for over a d...\n",
      "No images found on page 33\n",
      "Page 34 Text: F\n",
      "Further Comparison between CFG and Instruction-Tuning\n",
      "We noted in the main body, in Section 5, that Instruction-tuned models and CFG both operated to reduce the entropy of\n",
      "the sampling distribution, p(y|x), but that they did so in different ways from each other. To arrive at these insights, we\n",
      "conduced a large-scale analysis with samples from the P3 dataset to compare token-by-token logits.\n",
      "While the findings we presented in the main body were negative, here we present samples where Instructio...\n",
      "No images found on page 34\n",
      "Page 35 Text: G.2\n",
      "User prompts\n",
      "1. Why is The Matrix a great movie?\n",
      "2. Why did the chicken cross the road?\n",
      "3. What is the meaning of life?\n",
      "4. What is the answer to life, the universe, and everything?\n",
      "5. What is the best way to cook a steak?\n",
      "6. How do you make a pizza?\n",
      "7. What is the best way to make a pizza?\n",
      "8. Why is the sky blue?\n",
      "9. Who is the best basketball player of all time?\n",
      "10. What are trans fats?\n",
      "11. What are transformers?\n",
      "12. What are neural networks?\n",
      "13. What is the best way to learn a language?\n",
      "14....\n",
      "No images found on page 35\n",
      "Page 36 Text: 43. Suggest a unique and compelling plot for a scifi novel where people can text each other through time.\n",
      "44. Suggest a unique and compelling plot for a scifi novel where people can text each other through time, but only\n",
      "in the past.\n",
      "45. What was the Cambridge Analytica scandal?\n",
      "46. Tell me about the band Halocene.\n",
      "36\n",
      "...\n",
      "No images found on page 36\n",
      "Finished processing 2306.17806.pdf\n",
      "Processing: 2306.11695.pdf\n",
      "Total pages: 22\n",
      "Page 1 Text: A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR\n",
      "LARGE LANGUAGE MODELS\n",
      "Mingjie Sun1∗\n",
      "Zhuang Liu2∗\n",
      "Anna Bair1\n",
      "J. Zico Kolter1,3\n",
      "1Carnegie Mellon University\n",
      "2Meta AI Research\n",
      "3Bosch Center for AI\n",
      "ABSTRACT\n",
      "As their size increases, Large Languages Models (LLMs) are natural candidates\n",
      "for network pruning methods: approaches that drop a subset of network weights\n",
      "while striving to preserve performance. Existing methods, however, require either\n",
      "retraining, which is rarely affordable for billion-scale LLMs, o...\n",
      "No images found on page 1\n",
      "Page 2 Text: Magnitude Pruning\n",
      "Wanda\n",
      "4\n",
      "0\n",
      "1\n",
      "-1\n",
      "3\n",
      "-2\n",
      "-1\n",
      "-3\n",
      "-3\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "-2\n",
      "0\n",
      "-3\n",
      "-3\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "-1\n",
      "3\n",
      "-2\n",
      "-1\n",
      "-3\n",
      "-3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "8\n",
      "3\n",
      "4\n",
      "0\n",
      "8\n",
      "3\n",
      "3\n",
      "4\n",
      "8\n",
      "9\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "-3\n",
      "-3\n",
      "0\n",
      "0\n",
      "2\n",
      "Weights\n",
      "Weight Importance\n",
      "Weights and activations\n",
      "Weight Importance\n",
      "Pruned Weights\n",
      "Pruned Weights\n",
      "grouped per layer\n",
      "grouped per output\n",
      "Figure 1: Illustration of our proposed method Wanda (Pruning by Weights and activations), compared\n",
      "with the magnitude pruning approach. Given a weight matrix W and input fea...\n",
      "Extracted 4 images from page 2\n",
      "Page 3 Text: 3\n",
      "WANDA: PRUNING BY WEIGHTS AND ACTIVATIONS\n",
      "In this section, we motivate and describe our pruning method, Wanda (Pruning by Weights and\n",
      "activations), which consists of two simple but essential components. First, we propose a novel\n",
      "pruning metric that incorporates both weights and input activations into the computation of weight\n",
      "importance. Second, we compare weights on a per-output basis instead of across the whole layer,\n",
      "which we find is crucial for pruning LLMs effectively. An overview of Wand...\n",
      "No images found on page 3\n",
      "Page 4 Text: for LLMs. Notably, this holds true not only for our proposed pruning metric (Equation 1) but also\n",
      "the standard magnitude metric. This shows that maintaining a balanced pruning ratio across output\n",
      "features is important for pruning LLMs effectively.\n",
      "To see if the superiority of pruning per output over per layer holds true in general, we conduct\n",
      "additional experiments on pruning image classifiers. However, we do not observe similar trend in\n",
      "image classification models, suggesting that our observati...\n",
      "No images found on page 4\n",
      "Page 5 Text: Method\n",
      "Weight Update\n",
      "Calibration Data\n",
      "Pruning Metric Sij\n",
      "Complexity\n",
      "Magnitude\n",
      "✗\n",
      "✗\n",
      "|Wij|\n",
      "O(1)\n",
      "SparseGPT\n",
      "✓\n",
      "✓\n",
      "\u0002\n",
      "|W|2/diag\n",
      "\u0002\n",
      "(XXT + λI)−1\u0003\u0003\n",
      "ij O(d3\n",
      "hidden)\n",
      "Wanda\n",
      "✗\n",
      "✓\n",
      "|Wij| · ∥Xj∥2\n",
      "O(d2\n",
      "hidden)\n",
      "Table 1: Comparison of Wanda with existing pruning algorithms on LLMs.\n",
      "A comparison of LLM pruning methods can be found in Table 1. Computing the pruning metric of\n",
      "Wanda has a reduced time complexity compared to SparseGPT, because it does not involve inverse\n",
      "computation. Overall, our method Wanda (Pruning by W...\n",
      "No images found on page 5\n",
      "Page 6 Text: LLaMA\n",
      "LLaMA-2\n",
      "Method\n",
      "Weight Update\n",
      "Sparsity\n",
      "7B\n",
      "13B\n",
      "30B\n",
      "65B\n",
      "7B\n",
      "13B\n",
      "70B\n",
      "Dense\n",
      "-\n",
      "0%\n",
      "59.99\n",
      "62.59\n",
      "65.38\n",
      "66.97\n",
      "59.71\n",
      "63.03\n",
      "67.08\n",
      "Magnitude\n",
      "✗\n",
      "50%\n",
      "46.94\n",
      "47.61\n",
      "53.83\n",
      "62.74\n",
      "51.14\n",
      "52.85\n",
      "60.93\n",
      "SparseGPT\n",
      "✓\n",
      "50%\n",
      "54.94\n",
      "58.61\n",
      "63.09\n",
      "66.30\n",
      "56.24\n",
      "60.72\n",
      "67.28\n",
      "Wanda\n",
      "✗\n",
      "50%\n",
      "54.21\n",
      "59.33\n",
      "63.60\n",
      "66.67\n",
      "56.24\n",
      "60.83\n",
      "67.03\n",
      "Magnitude\n",
      "✗\n",
      "4:8\n",
      "46.03\n",
      "50.53\n",
      "53.53\n",
      "62.17\n",
      "50.64\n",
      "52.81\n",
      "60.28\n",
      "SparseGPT\n",
      "✓\n",
      "4:8\n",
      "52.80\n",
      "55.99\n",
      "60.79\n",
      "64.87\n",
      "53.80\n",
      "59.15\n",
      "65.84\n",
      "Wanda\n",
      "✗\n",
      "4:8\n",
      "52.76\n",
      "56.09\n",
      "61.00\n",
      "64.97\n",
      "52.49\n",
      "58.75\n",
      "66.06\n",
      "Magnitude\n",
      "✗\n",
      "2:4\n",
      "44.73\n",
      "48.00\n",
      "53.16\n",
      "61....\n",
      "No images found on page 6\n",
      "Page 7 Text: 4.3\n",
      "SPEEDUP\n",
      "Pruning Speed. The theoretical computational complexity of Wanda is lower than SparseGPT\n",
      "(Table 1). Here we compare their empirical pruning speed. Specifically, we measure the accumulated\n",
      "time for computing the pruning metric at each layer (excluding the forward pass process shared by\n",
      "both methods) on NVIDIA A6000 GPUs. Results are shown in Table 4. Wanda incurs negligible time\n",
      "overhead relative to SparseGPT. The fast speed of Wanda is particularly useful when pruning needs\n",
      "to be per...\n",
      "No images found on page 7\n",
      "Page 8 Text: Pruning Configuration. Wanda differs from previous methods in both the pruning metric and the\n",
      "comparison group. We conduct ablation experiments to better understand their impact. The three\n",
      "pruning metrics can be found in Table 1. SparseGPT adopts a local comparison group inside a\n",
      "layer, where weights connected to 128 consecutive input channels form a group. Wanda groups\n",
      "weights connected with a single output channel. Therefore, we ablate two blocksize options (128\n",
      "and 1) and the input/output cho...\n",
      "No images found on page 8\n",
      "Page 9 Text: Effects of the weight update on magnitude pruning and Wanda are summarized in Table 8. We study\n",
      "these two pruning methods because they do not involve any weight update by default. An iterative\n",
      "update changes the comparison group for unstructured pruning, which we denote in the table as\n",
      "(input, 128). We make several interesting observations:\n",
      "• For all considered sparsities, weight update can improve magnitude pruning by a large margin.\n",
      "• For unstructured 50% and 4:8 sparsities, weight update does...\n",
      "No images found on page 9\n",
      "Page 10 Text: 7\n",
      "CONCLUSION\n",
      "In this work, we propose a simple and effective method for pruning Large Language Models (LLMs).\n",
      "Inspired by the recent discovery of emergent large magnitude features in LLMs, our approach,\n",
      "termed Wanda (Pruning by Weights and activations), removes weights with the smallest magnitudes\n",
      "multiplied by the corresponding input activation norms, on a per-output basis. Without the need\n",
      "for any retraining or weight update procedures, Wanda is able to identify effective sparse networks\n",
      "withi...\n",
      "No images found on page 10\n",
      "Page 11 Text: Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\n",
      "Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\n",
      "arXiv:1905.10044, 2019. 20\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n",
      "arXiv preprint arXiv:1803.05457, 2018. 20\n",
      "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K...\n",
      "No images found on page 11\n",
      "Page 12 Text: Advait Gadhikar, Sohom Mukherjee, and Rebekka Burkholz. Why random pruning is all we need to\n",
      "start sparse. In ICML, 2023. 9\n",
      "Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. In ICML,\n",
      "2019. 1, 9, 15\n",
      "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\n",
      "Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\n",
      "Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot...\n",
      "No images found on page 12\n",
      "Page 13 Text: Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, Ajay Jaiswal, and Zhangyang\n",
      "Wang. Sparsity may cry: Let us fail (current) sparse neural networks together! In ICLR, 2023a. 9\n",
      "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning\n",
      "efficient convolutional networks through network slimming. In ICCV, 2017. 9\n",
      "Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\n",
      "network pruning. In ICLR, 2019. 1, 9\n",
      "Zhuang ...\n",
      "No images found on page 13\n",
      "Page 14 Text: Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural\n",
      "network pruning. In ICLR, 2020. 1, 9\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\n",
      "adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. 20\n",
      "Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by\n",
      "fine-tuning. In NeurIPS, 2020. 9, 18\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzan...\n",
      "No images found on page 14\n",
      "Page 15 Text: A\n",
      "IMAGE CLASSIFIERS\n",
      "We study how Wanda would perform against magnitude pruning on tasks where the latter has\n",
      "been widely used. We conduct a study on ImageNet-1K (Deng et al., 2009), a standard image\n",
      "classification task where magnitude pruning has been extensively studied (Gale et al., 2019; Blalock\n",
      "et al., 2020). We consider two modern vision architectures: ConvNeXt (Liu et al., 2022) and Vision\n",
      "Transformer (ViT) (Dosovitskiy et al., 2021). We choose these two architectures mainly for two\n",
      "reason...\n",
      "No images found on page 15\n",
      "Page 16 Text: B\n",
      "WANDA ON PREVIOUS LLMS\n",
      "In addition to LLaMA and LLaMA-2, we experiment with three previous LLM model families:\n",
      "namely OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022) and Pythia (Biderman et al., 2023).\n",
      "Comparison with Baselines. For OPT and Pythia, we experiment with varying sparsity levels (10%\n",
      "to 50%). We conduct additional evaluation on OPT and BLOOM models with various sizes. Results\n",
      "are shown in Table 9, Table 10 and Table 11 respectively. Our observations are as follows:\n",
      "• Unlike LLa...\n",
      "No images found on page 16\n",
      "Page 17 Text: Comparison Group We test if our observation regarding pruning per output holds true for other\n",
      "LLM model families. We experiment on OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022).\n",
      "In Table 12 and Table 13, we provide results comparing pruning per layer and pruning per output for\n",
      "these two LLM model families. The pruning metric is fixed to be our proposed metric: |Wij| · ∥Xj∥.\n",
      "We can see that our findings regarding the comparison group are not limited to LLaMA. For OPT and\n",
      "BLOOM model fami...\n",
      "No images found on page 17\n",
      "Page 18 Text: Pruning Method\n",
      "Pruning Type\n",
      "Pruning Metric\n",
      "Training Procedure\n",
      "SNIP (Lee et al., 2018)\n",
      "Unstructured\n",
      "Loss Sensitivity\n",
      "Pruning at Initialization\n",
      "BERT-LTH (Chen et al., 2020)\n",
      "Unstructured\n",
      "Magnitude\n",
      "Fine-tuning BERT\n",
      "Movement (Sanh et al., 2020)\n",
      "Unstructured\n",
      "Loss Sensitivity\n",
      "Fine-tuning BERT\n",
      "Platon (Zhang et al., 2021)\n",
      "Unstructured\n",
      "Loss Sensitivity\n",
      "Fine-tuning BERT\n",
      "PINS (Ren & Zhu, 2023)\n",
      "Unstructured\n",
      "Loss Sensitivity\n",
      "Fine-tuning BERT\n",
      "Table 15: Summary of prior pruning methods on BERT.\n",
      "Pruning method\n",
      "M...\n",
      "No images found on page 18\n",
      "Page 19 Text: D.2\n",
      "HIGHER SPARSITY\n",
      "In Section 4, we have evaluated unstructured pruning with a sparsity level of 50%. This is to follow\n",
      "the evaluation setup of Frantar & Alistarh (2023). In this part, we evaluate on higher sparsity levels,\n",
      "i.e., 60% and 80%. Results for these two sparsity levels are shown Table 18 and Table 19 respectively.\n",
      "At 60% sparsity, Wanda remains competitive with SparseGPT. At 80% sparsity, SparseGPT is able to\n",
      "outperform Wanda, but the performance drop compared to the dense counterpar...\n",
      "No images found on page 19\n",
      "Page 20 Text: D.4\n",
      "FINE-TUNING\n",
      "In Table 6 of Section 5, we report the mean zero-shot accuracies after fine-tuning Wanda pruned\n",
      "LLaMA-7B models. In this part, we report the task-wise performance of these fine-tuned models.\n",
      "Results are summarized in Table 21. For per-task accuracies, most of the performance drop during\n",
      "pruning can be recovered through fine-tuning. Note that here we are performing limited fine-tuning\n",
      "with a computational budget (12 hours for LoRA fine-tuning and 3 days for full parameter fine-tun...\n",
      "No images found on page 20\n",
      "Page 21 Text: Params\n",
      "Method\n",
      "BoolQ\n",
      "RTE HellaSwag WinoGrande\n",
      "ARC-e\n",
      "ARC-c\n",
      "OBQA\n",
      "Mean\n",
      "7B\n",
      "Dense\n",
      "75.05\n",
      "66.43\n",
      "56.92\n",
      "69.93\n",
      "75.34\n",
      "41.89\n",
      "34.40\n",
      "59.99\n",
      "Magnitude\n",
      "51.19\n",
      "50.54\n",
      "46.73\n",
      "60.69\n",
      "58.96\n",
      "30.89\n",
      "23.20\n",
      "46.03\n",
      "SparseGPT\n",
      "73.06\n",
      "58.12\n",
      "47.88\n",
      "65.98\n",
      "66.75\n",
      "32.42\n",
      "25.40\n",
      "52.80\n",
      "Wanda\n",
      "70.97\n",
      "58.24\n",
      "46.81\n",
      "65.83\n",
      "65.53\n",
      "33.97\n",
      "28.00\n",
      "52.76\n",
      "13B\n",
      "Dense\n",
      "77.89\n",
      "70.40\n",
      "59.94\n",
      "72.77\n",
      "77.40\n",
      "46.50\n",
      "33.20\n",
      "62.59\n",
      "Magnitude\n",
      "61.07\n",
      "51.26\n",
      "48.91\n",
      "65.11\n",
      "63.26\n",
      "35.67\n",
      "28.40\n",
      "50.53\n",
      "SparseGPT\n",
      "76.61\n",
      "57.76\n",
      "51.24\n",
      "70.17\n",
      "71.17\n",
      "37.20\n",
      "27.80\n",
      "55.99\n",
      "Wanda\n",
      "74.89\n",
      "57.89\n",
      "51.26\n",
      "70.56\n",
      "70...\n",
      "No images found on page 21\n",
      "Page 22 Text: Params\n",
      "Method\n",
      "BoolQ\n",
      "RTE HellaSwag WinoGrande\n",
      "ARC-e\n",
      "ARC-c\n",
      "OBQA\n",
      "Mean\n",
      "7B\n",
      "Dense\n",
      "77.74\n",
      "62.82\n",
      "57.17\n",
      "68.90\n",
      "76.39\n",
      "43.52\n",
      "31.40\n",
      "59.71\n",
      "Magnitude\n",
      "63.00\n",
      "57.04\n",
      "49.13\n",
      "63.30\n",
      "64.10\n",
      "34.64\n",
      "26.80\n",
      "51.14\n",
      "SparseGPT\n",
      "75.02\n",
      "54.15\n",
      "52.37\n",
      "69.85\n",
      "73.27\n",
      "39.85\n",
      "29.20\n",
      "56.24\n",
      "Wanda\n",
      "75.99\n",
      "53.43\n",
      "52.49\n",
      "68.19\n",
      "72.77\n",
      "39.59\n",
      "31.20\n",
      "56.24\n",
      "13B\n",
      "Dense\n",
      "80.52\n",
      "65.34\n",
      "60.06\n",
      "72.22\n",
      "79.42\n",
      "48.46\n",
      "35.20\n",
      "63.03\n",
      "Magnitude\n",
      "57.61\n",
      "55.96\n",
      "54.40\n",
      "65.27\n",
      "70.54\n",
      "38.40\n",
      "27.80\n",
      "52.85\n",
      "SparseGPT\n",
      "81.44\n",
      "65.34\n",
      "55.83\n",
      "72.77\n",
      "74.83\n",
      "42.24\n",
      "32.60\n",
      "60.72\n",
      "Wanda\n",
      "81.84\n",
      "64.02\n",
      "56.90\n",
      "71.35\n",
      "76...\n",
      "No images found on page 22\n",
      "Finished processing 2306.11695.pdf\n",
      "Processing: Adaptive-mixtures-of-local-experts.pdf\n",
      "Total pages: 11\n",
      "Page 1 Text: In Neural Computation, 3, pages 79-87.\n",
      "Adaptive Mixtures of Local Experts\n",
      "Robert A. Jacobs\n",
      "Michael I. Jordan\n",
      "Department of Brain & Cognitive Sciences\n",
      "Massachusetts Institute of Technology\n",
      "Cambridge, MA 02139\n",
      "Steven J. Nowlan\n",
      "Geoﬀrey E. Hinton\n",
      "Department of Computer Science\n",
      "University of Toronto\n",
      "Toronto, Canada M5S 1A4\n",
      "Abstract\n",
      "We present a new supervised learning procedure for systems composed of many separate\n",
      "networks, each of which learns to handle a subset of the complete set of training case...\n",
      "No images found on page 1\n",
      "Page 2 Text: and Waibel (1989) have described a system of this kind that can be used when the division\n",
      "into subtasks is known prior to training, and Jacobs, Jordan and Barto (1990) have described\n",
      "a related system that learns how to allocate cases to experts. The idea behind such a system\n",
      "is that the gating network allocates a new case to one or a few experts, and, if the output is\n",
      "incorrect, the weight changes are localized to these experts (and the gating network). So there\n",
      "is no interference with the weigh...\n",
      "No images found on page 2\n",
      "Page 3 Text: Figure 1: A system of expert and gating networks. Each expert is a feedforward network\n",
      "and all experts receive the same input and have the same number of outputs. The gating\n",
      "network is also feedforward and typically receives the same input as the expert networks.\n",
      "It has normalized outputs pj = exp(xj)/ P\n",
      "i exp(xi), where xj is the total weighted input\n",
      "received by output unit j of the gating network. The selector acts like a multiple input,\n",
      "single output stochastic switch; the probability that th...\n",
      "Extracted 1 images from page 3\n",
      "Page 4 Text: Notice that in this new error function, each expert is required to produce the whole of\n",
      "the output vector rather than a residual. As a result, the goal of a local expert on a given\n",
      "training case is not directly aﬀected by the weights within other local experts. There is still\n",
      "some indirect coupling because if some other expert changes its weights, it may cause the\n",
      "gating network to alter the responsibilities that get assigned to the experts, but at least\n",
      "these responsibility changes cannot alter...\n",
      "No images found on page 4\n",
      "Page 5 Text: 2\n",
      "Making competitive learning associative\n",
      "It is natural to think that the “data” vectors on which a competitive network is trained\n",
      "play a role similar to the input vectors of an associative network that maps input vectors\n",
      "to output vectors. This correspondence is assumed in models that use competitive learning\n",
      "as a preprocessing stage within an associative network (Moody and Darken, 1989). A quite\n",
      "diﬀerent view is that the data vectors used in competitive learning correspond to the out-\n",
      "put vect...\n",
      "No images found on page 5\n",
      "Page 6 Text: are now a function of the current input vector and are represented by activity levels rather\n",
      "than weights. In addition, we use a gating network which allows the mixing proportions of\n",
      "the experts to be determined by the input vector. This gives us a system of competing local\n",
      "experts with the error function deﬁned in equation 3. We could also introduce a mechanism\n",
      "to allow the input vector to dynamically determine the covariance matrix for the distribution\n",
      "deﬁned by each expert network, but we hav...\n",
      "No images found on page 6\n",
      "Page 7 Text: Figure 2: Data for vowel discrimination problem, and expert and gating network decision\n",
      "lines.\n",
      "The horizontal axis is the ﬁrst formant value, and the vertical axis is the second\n",
      "formant value (the formant values have been linearly scaled by dividing by a factor of 1000).\n",
      "Each example is labelled with its corresponding vowel symbol. Vowels [i] and [I] form one\n",
      "overlapping pair of classes, vowels [a] and [A] form the other pair. The lines labelled Net\n",
      "0, 1 and 2 represent the decision lines for 3 ...\n",
      "Extracted 1 images from page 7\n",
      "Page 8 Text: System\n",
      "Train % Correct\n",
      "Test % Correct\n",
      "Avg. # Epochs\n",
      "Std. Dev.\n",
      "4 Experts\n",
      "88\n",
      "90\n",
      "1124\n",
      "23\n",
      "8 Experts\n",
      "88\n",
      "90\n",
      "1083\n",
      "12\n",
      "BP 6 Hid\n",
      "88\n",
      "90\n",
      "2209\n",
      "83\n",
      "BP 12 Hid\n",
      "88\n",
      "90\n",
      "2435\n",
      "124\n",
      "Table 1: Summary of performance on vowel discrimination task. Results are based on 25\n",
      "simulations for each of the alternative models. The ﬁrst column of the table indicates the\n",
      "system simulated. The second column gives the percent of training cases classiﬁed correctly\n",
      "by the ﬁnal set of weights, while the third column indicates the percent ...\n",
      "No images found on page 8\n",
      "Page 9 Text: Figure 3: The trajectories of the decision lines of some experts during one simulation. The\n",
      "horizontal axis is the ﬁrst formant value, and the vertical axis is the second formant value.\n",
      "Each trajectory is represented by a sequence of dots, one per epoch, each dot marking the\n",
      "intersection of the expert’s decision line and the normal to that line passing through the\n",
      "origin. For clarity, only 5 of the 8 experts are shown and the number of the expert is shown\n",
      "at the start of the trajectory. The poin...\n",
      "Extracted 1 images from page 9\n",
      "Page 10 Text: the trajectories begin to diverge as diﬀerent experts concentrate on one class pair or the\n",
      "other. In this simulation, expert 5 learns to concentrate on discriminating classes [i] and\n",
      "[I] so its decision line approaches the optimal line for this discrimination (T0). Experts 4\n",
      "and 6 both concentrate on discriminating classes [a] and [A], so their trajectories approach\n",
      "the optimal single line (T1) and then split to form a piecewise linear approximation to the\n",
      "slightly curved optimal decision surfac...\n",
      "No images found on page 10\n",
      "Page 11 Text: Acknowledgements\n",
      "Jordan and Jacobs were funded by grants from Siemens and the McDonnell-Pew program\n",
      "in Cognitive Neuroscience. Hinton and Nowlan were funded by grants from the Ontario\n",
      "Information Technology Research Center and the Canadian Natural Science and Engineering\n",
      "Research Council. Hinton is a fellow of the Canadian Institute for Advanced Research.\n",
      "References\n",
      "Barto, A. G. (1985) Learning by statistical cooperation of self-interested neuron-like com-\n",
      "puting elements. Human Neurobiology, 4:...\n",
      "No images found on page 11\n",
      "Finished processing Adaptive-mixtures-of-local-experts.pdf\n",
      "Processing: 2305.18290.pdf\n",
      "Total pages: 27\n",
      "Page 1 Text: Direct Preference Optimization:\n",
      "Your Language Model is Secretly a Reward Model\n",
      "Rafael Rafailov∗†\n",
      "Archit Sharma∗†\n",
      "Eric Mitchell∗†\n",
      "Stefano Ermon†‡\n",
      "Christopher D. Manning†\n",
      "Chelsea Finn†\n",
      "†Stanford University ‡CZ Biohub\n",
      "{rafailov,architsh,eric.mitchell}@cs.stanford.edu\n",
      "Abstract\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowl-\n",
      "edge and some reasoning skills, achieving precise control of their behavior is\n",
      "difficult due to the completely unsupervised nature of their training...\n",
      "No images found on page 1\n",
      "Page 2 Text: Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\n",
      "for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\n",
      "human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\n",
      "In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\n",
      "objective, fitting an implicit reward model whose corresponding optima...\n",
      "Extracted 1 images from page 2\n",
      "Page 3 Text: Bradley-Terry model [5], then fine-tune a language model to maximize the given reward using\n",
      "reinforcement learning algorithms, commonly REINFORCE [45], proximal policy optimization\n",
      "(PPO; [37]), or variants [32]. A closely-related line of work leverages LLMs fine-tuned for instruction\n",
      "following with human feedback to generate additional synthetic preference data for targeted attributes\n",
      "such as safety or harmlessness [2], using only weak supervision from humans in the form of a\n",
      "text rubric for the...\n",
      "No images found on page 3\n",
      "Page 4 Text: where β is a parameter controlling the deviation from the base reference policy πref, namely the ini-\n",
      "tial SFT model πSFT. In practice, the language model policy πθ is also initialized to πSFT. The\n",
      "added constraint is important, as it prevents the model from deviating too far from the distri-\n",
      "bution on which the reward model is accurate, as well as maintaining the generation diversity\n",
      "and preventing mode-collapse to single high-reward answers. Due to the discrete nature of lan-\n",
      "guage generation,...\n",
      "No images found on page 4\n",
      "Page 5 Text: model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the\n",
      "preference data distribution [4]. In Section 5, we further discuss theoretical properties of DPO in\n",
      "relation to other works.\n",
      "What does the DPO update do? For a mechanistic understanding of DPO, it is useful to analyze the\n",
      "gradient of the loss function LDPO. The gradient with respect to the parameters θ can be written as:\n",
      "∇θLDPO(πθ; πref) =\n",
      "−βE(x,yw,yl)∼D\n",
      "\u0014\n",
      "σ(ˆrθ(x, yl) −ˆrθ(x, yw))\n",
      "|\n",
      "{z\n",
      "}\n",
      "high...\n",
      "No images found on page 5\n",
      "Page 6 Text: we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE\n",
      "estimates from Eq. 2 [4]. The second lemma states that all reward functions from the same class\n",
      "yield the same optimal policy, hence for our final objective, we are only interested in recovering an\n",
      "arbitrary reward function from the optimal class. We prove the following Theorem in Appendix A.6:\n",
      "Theorem 1. Under mild assumptions, all reward classes consistent with the Plackett-Luce\n",
      "(and Bradley-T...\n",
      "No images found on page 6\n",
      "Page 7 Text: 0.0\n",
      "2.5\n",
      "5.0\n",
      "7.5\n",
      "10.0\n",
      "12.5\n",
      "15.0\n",
      "17.5\n",
      "20.0\n",
      "KL(\n",
      "ref)\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "Reward\n",
      "IMDb Sentiment Generation\n",
      "DPO (Ours)\n",
      "Unlikelihood\n",
      "PPO (Our impl.)\n",
      "PPO-GT (Our impl.)\n",
      "PPO-GT (TRL)\n",
      "Preferred-FT\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Sampling temperature\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "Win rate\n",
      "TL;DR Summarization Win Rate vs Reference\n",
      "DPO\n",
      "PPO\n",
      "Preferred-FT\n",
      "SFT\n",
      "GPT-J\n",
      "Best of 128\n",
      "Figure 2: Left. The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected\n",
      "reward for all KL v...\n",
      "No images found on page 7\n",
      "Page 8 Text: 0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Sampling temperature\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "Win rate\n",
      "Anthropic-HH Dialogue Win Rate vs Chosen\n",
      "DPO\n",
      "Best of 128\n",
      "Preferred-FT\n",
      "Pythia-2.8B\n",
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200 1500 1800 2100 2400 2700 3000 3300\n",
      "Fine-tuning step\n",
      "0.30\n",
      "0.35\n",
      "0.40\n",
      "0.45\n",
      "0.50\n",
      "0.55\n",
      "0.60\n",
      "0.65\n",
      "0.70\n",
      "Win rate\n",
      "Dialogue Win Rate Evolution\n",
      "DPO (temp = 1.0)\n",
      "DPO (temp = 0.7)\n",
      "Figure 3: Left. Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method\n",
      "that improves over chosen summaries in the Anthropic-H...\n",
      "No images found on page 8\n",
      "Page 9 Text: DPO’s reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO,\n",
      "even when PPO can access ground truth rewards (PPO-GT).\n",
      "6.2\n",
      "Can DPO scale to real preference datasets?\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For\n",
      "summarization, automatic evaluation metrics such as ROUGE can be poorly correlated with human\n",
      "preferences [38], and prior work has found that fine-tuning LMs using PPO on human preferences\n",
      "to provide mor...\n",
      "No images found on page 9\n",
      "Page 10 Text: 6.4\n",
      "Validating GPT-4 judgments with human judgments\n",
      "We conduct a human study to verify the reliability of GPT-4’s judgments, using the results of\n",
      "the TL;DR summarization experiment and two different GPT-4 prompts. The GPT-4 (S) (sim-\n",
      "ple) prompt simply asks for which summary better-summarizes the important information in the\n",
      "post. The GPT-4 (C) (concise) prompt also asks for which summary is more concise; we eval-\n",
      "uate this prompt because we find that GPT-4 prefers longer, more repetitive summar...\n",
      "No images found on page 10\n",
      "Page 11 Text: References\n",
      "[1] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\n",
      "T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-\n",
      "Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson,\n",
      "D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a\n",
      "helpful and harmless assistant with reinforcement learning from human feedback, 2022.\n",
      "[2] Y. Bai, S. Kadavath, S. Kun...\n",
      "No images found on page 11\n",
      "Page 12 Text: [13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani,\n",
      "S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros,\n",
      "M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai,\n",
      "H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei.\n",
      "Scaling instruction-finetuned language models, 2022.\n",
      "[14] M. Dudík, K. Hofmann, R. E. Schapire, A. Slivkins, and M. Zoghi. Contextual ...\n",
      "No images found on page 12\n",
      "Page 13 Text: [25] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,\n",
      "P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale\n",
      "language model training on gpu clusters using megatron-lm. In Proceedings of the International\n",
      "Conference for High Performance Computing, Networking, Storage and Analysis, SC ’21, New\n",
      "York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi:\n",
      "10.1145/3458817.3476209. URL https://d...\n",
      "No images found on page 13\n",
      "Page 14 Text: [39] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\n",
      "L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun,\n",
      "D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou,\n",
      "C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern,\n",
      "M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran,\n",
      "M. Diaz, B. Hutchinson, K. Olson, A. Mo...\n",
      "No images found on page 14\n",
      "Page 15 Text: Author Contributions\n",
      "All authors provided valuable contributions to designing, analyzing, and iterating on experiments,\n",
      "writing and editing the paper, and generally managing the project’s progress.\n",
      "RR proposed using autoregressive reward models in discussions with EM; derived the DPO objective;\n",
      "proved the theoretical properties of the algorithm and wrote the relevant sections and appendices. He\n",
      "also suggested and helped with organizing experiments and contributed some of the PPO and reward\n",
      "learn...\n",
      "No images found on page 15\n",
      "Page 16 Text: which is a valid probability distribution as π∗(y|x) ≥0 for all y and P\n",
      "y π∗(y|x) = 1. Since Z(x) is\n",
      "not a function of y, we can then re-organize the final objective in Eq 12 as:\n",
      "min\n",
      "π Ex∼D\n",
      "\u0014\n",
      "Ey∼π(y|x)\n",
      "\u0014\n",
      "log π(y|x)\n",
      "π∗(y|x)\n",
      "\u0015\n",
      "−log Z(x)\n",
      "\u0015\n",
      "=\n",
      "(13)\n",
      "min\n",
      "π Ex∼D [DKL(π(y|x) || π∗(y|x)) −log Z(x)]\n",
      "(14)\n",
      "Now, since Z(x) does not depend on π, the minimum is achieved by the policy that minimizes the\n",
      "first KL term. Gibbs’ inequality tells us that the KL-divergence is minimized at 0 if and only if the\n",
      "two dist...\n",
      "No images found on page 16\n",
      "Page 17 Text: Similarly\n",
      "to\n",
      "the\n",
      "approach\n",
      "of\n",
      "Section\n",
      "4,\n",
      "if\n",
      "we\n",
      "have\n",
      "access\n",
      "to\n",
      "a\n",
      "dataset\n",
      "D\n",
      "=\n",
      "{τ (i), y(i)\n",
      "1 , . . . , y(i)\n",
      "K , x(i)}N\n",
      "i=1 of prompts and user-specified rankings, we can use a parameterized\n",
      "model and optimize this objective with maximum-likelihood.:\n",
      "LDPO(πθ, πref) = −Eτ,y1,...,yK,x∼D\n",
      "\n",
      "log\n",
      "K\n",
      "Y\n",
      "k=1\n",
      "exp\n",
      "\u0010\n",
      "β log\n",
      "πθ(yτ(k)|x)\n",
      "πref(yτ(k)|x)\n",
      "\u0011\n",
      "PK\n",
      "j=k exp\n",
      "\u0010\n",
      "β log\n",
      "πθ(yτ(j)|x)\n",
      "πref(yτ(j)|x)\n",
      "\u0011\n",
      "\n",
      "\n",
      "(20)\n",
      "A.4\n",
      "Deriving the Gradient of the DPO Objective\n",
      "In this section we derive the gradient of the DPO objective:...\n",
      "No images found on page 17\n",
      "Page 18 Text: Lemma 2 Restated. Two reward functions from the same equivalence class induce the same optimal\n",
      "policy under the constrained RL problem.\n",
      "Proof. Let us consider two reward functions from the same class, such that r′(x, y) = r(x, y) + f(x)\n",
      "and, let us denote as πr and πr′ the corresponding optimal policies. By Eq. 4, for all x, y we have\n",
      "πr′(y|x) =\n",
      "1\n",
      "P\n",
      "y πref(y|x) exp\n",
      "\u0010\n",
      "1\n",
      "β r′(x, y)\n",
      "\u0011πref(y|x) exp\n",
      "\u0012 1\n",
      "β r′(x, y)\n",
      "\u0013\n",
      "=\n",
      "1\n",
      "P\n",
      "y πref(y|x) exp\n",
      "\u0010\n",
      "1\n",
      "β (r(x, y) + f(x))\n",
      "\u0011πref(y|x) exp\n",
      "\u0012 1\n",
      "β (r(x, y) + f(x))\n",
      "\u0013\n",
      "...\n",
      "No images found on page 18\n",
      "Page 19 Text: Proof. We will proceed using proof by contradiction. Assume we have two reward functions from\n",
      "the same class, such that r′(x, y) = r(x, y) + f(x). Moreover, assume that r′(x, y) = β log π′(y|x)\n",
      "πref(y|x)\n",
      "for some model π′(y|x) and r(x, y) = β log\n",
      "π(y|x)\n",
      "πref(y|x) for some model π(y|x), such that π ̸= π′. We\n",
      "then have\n",
      "r′(x, y) = r(x, y) + f(x) = β log π(y|x)\n",
      "πref(y|x) + f(x) = β log\n",
      "π(y|x) exp( 1\n",
      "β f(x))\n",
      "πref(y|x)\n",
      "= β log π′(y|x)\n",
      "πref(y|x)\n",
      "for all prompts x and completions y. Then we must have π(...\n",
      "No images found on page 19\n",
      "Page 20 Text: import torch.nn.functional as F\n",
      "def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n",
      "\"\"\"\n",
      "pi_logps: policy logprobs, shape (B,)\n",
      "ref_logps: reference model logprobs, shape (B,)\n",
      "yw_idxs: preferred completion indices in [0, B-1], shape (T,)\n",
      "yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\n",
      "beta: temperature controlling strength of KL penalty\n",
      "Each pair of (yw_idxs[i], yl_idxs[i]) represents the\n",
      "indices of a single preference pair.\n",
      "\"\"\"\n",
      "pi_yw_logps,\n",
      "pi_yl_logps =\n",
      "pi_logps[yw_idxs],...\n",
      "No images found on page 20\n",
      "Page 21 Text: <post>\n",
      "Summary A:\n",
      "<Summary A>\n",
      "Summary B:\n",
      "<Summary B>\n",
      "FIRST provide a one-sentence comparison of the two summaries, explaining which \\\n",
      "you prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\n",
      "choice. Your response should use the format:\n",
      "Comparison: <one-sentence comparison and explanation>\n",
      "Preferred: <\"A\" or \"B\">\n",
      "Summarization GPT-4 win rate prompt (C).\n",
      "Which of the following summaries does a better job of summarizing the most \\\n",
      "important points in the given forum post,...\n",
      "No images found on page 21\n",
      "Page 22 Text: Prompt\n",
      "Response\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: The girl [26 F] I [22 M] have been\n",
      "seeing for a month didn’t respond to me at all\n",
      "yesterday while hanging out with a friend [ 30?\n",
      "M].\n",
      "POST: She gets terrible service while at her\n",
      "house, but I texted her 3 times yesterday, 4-5\n",
      "hours apart. She didn’t call me until early this\n",
      "morning and left a voicemail that she was busy\n",
      "all day with a friend who showed up out of the\n",
      "blue.\n",
      "I saw that she posted a picture of the two of\n",
      "them out of her dead zone hou...\n",
      "No images found on page 22\n",
      "Page 23 Text: 0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Sampling temperature\n",
      "0.20\n",
      "0.25\n",
      "0.30\n",
      "0.35\n",
      "0.40\n",
      "0.45\n",
      "0.50\n",
      "0.55\n",
      "0.60\n",
      "Win rate\n",
      "Anthropic-HH Dialogue Win Rate vs Chosen\n",
      "Best of 1\n",
      "Best of 4\n",
      "Best of 16\n",
      "Best of 64\n",
      "Best of 128\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Sampling temperature\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "Win rate\n",
      "TL;DR Summarization Win Rate vs Reference\n",
      "Best of 64\n",
      "Best of 128\n",
      "Best of 256\n",
      "Figure 4: Best of N baseline for N = {1, 4, 16, 64, 128}. Performance plateaus after roughly 64-128 samples.\n",
      "Prompt\n",
      "SUBREDDIT: r/AskReddit TITLE: ...\n",
      "No images found on page 23\n",
      "Page 24 Text: Prompt\n",
      "SUBREDDIT: r/relationships TITLE: I [22 M] am about to propose to my girlfriend\n",
      "[22F] of almost 2 years, and I can’t calm myself down POST: Alright, so I am\n",
      "preparing to propose to my girlfriend of almost 2 years (we were friends for\n",
      "double that beforehand) in a couple days and I am starting to get EXTREMELY\n",
      "anxious/nervous. We have talked about this for months, and I know she will say\n",
      "yes, but I can’t seem to relax.\n",
      "The other day I bought the ring, which I fell in love with myself and kn...\n",
      "No images found on page 24\n",
      "Page 25 Text: Prompt\n",
      "Can you help me write an essay on the Civil Rights Movement?\n",
      "DPO\n",
      "The Civil Rights Movement was a series of social and political movements that took\n",
      "place between the 1950s and the 1980s, leading to the integration of public spaces,\n",
      "higher education, employment, and voting rights. The movement is often credited\n",
      "with the end of legal segregation in the United States, and it was a major catalyst for\n",
      "the Civil Rights Act of 1964. The Movement also played a large role in the rise of\n",
      "the Black ...\n",
      "No images found on page 25\n",
      "Page 26 Text: Prompt\n",
      "Can you tell me what got the USA involved in World War II?\n",
      "DPO\n",
      "In the 1930s, the United States found itself at the center of a growing political and\n",
      "economic crisis that threatened the world’s stability. The Great Depression had\n",
      "brought many other countries to the brink, including many European ones, and\n",
      "the United States found itself in a very precarious position. The Great Depression\n",
      "had left the government and the economy on the edge of collapse, it faced severe\n",
      "international indebtedn...\n",
      "No images found on page 26\n",
      "Page 27 Text: D.3\n",
      "Human study details\n",
      "Figure 5: Layout of the survey in SurveyMonkey. Each respon-\n",
      "dent completed 25 similarly-formatted judgments.\n",
      "In order to validate the usage of GPT-\n",
      "4 for computing win rates, our human\n",
      "study collects human preference data for\n",
      "several matchups in the TL;DR summa-\n",
      "rization setting. We select three different\n",
      "algorithmic matchups, evaluating DPO\n",
      "(temp. 0.25), SFT (temp. 0.25), and\n",
      "PPO (temp 1.0) compared to the refer-\n",
      "ence algorithm PPO (temp 0.). By select-\n",
      "ing matchups for...\n",
      "Extracted 1 images from page 27\n",
      "Finished processing 2305.18290.pdf\n",
      "Processing: 1710.05941.pdf\n",
      "Total pages: 13\n",
      "Page 1 Text: SEARCHING FOR ACTIVATION FUNCTIONS\n",
      "Prajit Ramachandran∗, Barret Zoph, Quoc V. Le\n",
      "Google Brain\n",
      "{prajit,barretzoph,qvl}@google.com\n",
      "ABSTRACT\n",
      "The choice of activation functions in deep networks has a signiﬁcant effect on\n",
      "the training dynamics and task performance. Currently, the most successful and\n",
      "widely-used activation function is the Rectiﬁed Linear Unit (ReLU). Although\n",
      "various hand-designed alternatives to ReLU have been proposed, none have man-\n",
      "aged to replace it due to inconsistent gains. In ...\n",
      "No images found on page 1\n",
      "Page 2 Text: effectiveness of using searches to discover scalar activation functions, we empirically evaluate the\n",
      "best discovered activation function. The best discovered activation function, which we call Swish, is\n",
      "f(x) = x · sigmoid(βx), where β is a constant or trainable parameter. Our extensive experiments\n",
      "show that Swish consistently matches or outperforms ReLU on deep networks applied to a variety\n",
      "of challenging domains such as image classiﬁcation and machine translation. On ImageNet, replac-\n",
      "ing ReLUs...\n",
      "No images found on page 2\n",
      "Page 3 Text: Input 1\n",
      "Input 2\n",
      "Unary 1\n",
      "Unary 2\n",
      "Binary\n",
      "Input 1\n",
      "Binary\n",
      "Core unit N\n",
      "Core unit N+1\n",
      "...\n",
      "Core unit N-1\n",
      "...\n",
      "Figure 2: The RNN controller used to search over large spaces. At each step, it predicts a single\n",
      "component of the activation function. The prediction is fed back as input to the next timestep in\n",
      "an autoregressive fashion. The controller keeps predicting until every component of the activation\n",
      "function has been chosen. The controller is trained with reinforcement learning.\n",
      "to update the search a...\n",
      "No images found on page 3\n",
      "Page 4 Text: 6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "x · σ(βx)\n",
      "x · (sinh−1(x))2\n",
      "min(x, sin(x))\n",
      "(tan−1(x))2 −x\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "max(x, σ(x))\n",
      "cos(x) −x\n",
      "max(x, tanh(x))\n",
      "sinc(x) + x\n",
      "Figure 3: The top novel activation functions found by the searches. Separated into two diagrams for\n",
      "visual clarity. Best viewed in color.\n",
      "• Complicated activation functions consistently underperform simpler activation functions,\n",
      "potentially due to an increased difﬁculty in optimization. The best performing activation\n",
      "functions can be represented by 1 o...\n",
      "No images found on page 4\n",
      "Page 5 Text: While these results are promising, it is still unclear whether the discovered activation functions\n",
      "can successfully replace ReLU on challenging real world datasets. In order to validate the effec-\n",
      "tiveness of the searches, in the rest of this work we focus on empirically evaluating the activation\n",
      "function f(x) = x · σ(βx), which we call Swish. We choose to extensively evaluate Swish in-\n",
      "stead of max(x, σ(x)) because early experimentation showed better generalization for Swish. In\n",
      "the following s...\n",
      "No images found on page 5\n",
      "Page 6 Text: 10\n",
      "5\n",
      "0\n",
      "5\n",
      "10\n",
      "Preactivations after training\n",
      "Figure 6: Preactivation distribution after\n",
      "training of Swish with β = 1 on ResNet-32.\n",
      "0.5\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "β values after training\n",
      "Figure 7: Distribution of trained β values of Swish\n",
      "on Mobile NASNet-A.\n",
      "Practically, Swish can be implemented with a single line code change in most deep learning\n",
      "libraries, such as TensorFlow (Abadi et al., 2016) (e.g., x * tf.sigmoid(beta * x) or\n",
      "tf.nn.swish(x) if using a version of TensorFlow released after the submissi...\n",
      "No images found on page 6\n",
      "Page 7 Text: • Leaky ReLU (LReLU) (Maas et al., 2013):\n",
      "f(x) =\n",
      "\u001ax\n",
      "if x ≥0\n",
      "αx\n",
      "if x < 0\n",
      "where α = 0.01. LReLU enables a small amount of information to ﬂow when x < 0.\n",
      "• Parametric ReLU (PReLU) (He et al., 2015): The same form as LReLU but α is a learnable\n",
      "parameter. Each channel has a shared α which is initialized to 0.25.\n",
      "• Softplus (Nair & Hinton, 2010): f(x) = log(1 + exp(x)). Softplus is a smooth function\n",
      "with properties similar to Swish, but is strictly positive and monotonic. It can be viewed as\n",
      "a smooth ...\n",
      "No images found on page 7\n",
      "Page 8 Text: 5.3\n",
      "IMAGENET\n",
      "Next, we benchmark Swish against the baseline activation functions on the ImageNet 2012 classi-\n",
      "ﬁcation dataset (Russakovsky et al., 2015). ImageNet is widely considered one of most important\n",
      "image classiﬁcation datasets, consisting of a 1,000 classes and 1.28 million training images. We\n",
      "evaluate on the validation dataset, which has 50,000 images.\n",
      "We compare all the activation functions on a variety of architectures designed for ImageNet:\n",
      "Inception-ResNet-v2, Inception-v4, Inception...\n",
      "No images found on page 8\n",
      "Page 9 Text: Model\n",
      "Top-1 Acc. (%)\n",
      "Top-5 Acc. (%)\n",
      "LReLU\n",
      "78.4\n",
      "94.1\n",
      "PReLU\n",
      "77.7\n",
      "93.5\n",
      "Softplus\n",
      "78.7\n",
      "94.4\n",
      "ELU\n",
      "77.9\n",
      "93.7\n",
      "SELU\n",
      "76.7\n",
      "92.8\n",
      "GELU\n",
      "77.7\n",
      "93.9\n",
      "ReLU\n",
      "78.4\n",
      "94.2\n",
      "Swish-1\n",
      "78.7\n",
      "94.2\n",
      "Swish\n",
      "78.7\n",
      "94.0\n",
      "Table 9: Inception-v3 on ImageNet.\n",
      "Model\n",
      "Top-1 Acc. (%)\n",
      "Top-5 Acc. (%)\n",
      "LReLU\n",
      "79.3\n",
      "94.7\n",
      "PReLU\n",
      "79.3\n",
      "94.4\n",
      "Softplus\n",
      "79.6\n",
      "94.8\n",
      "ELU\n",
      "79.5\n",
      "94.5\n",
      "SELU\n",
      "78.3\n",
      "94.5\n",
      "GELU\n",
      "79.0\n",
      "94.6\n",
      "ReLU\n",
      "79.2\n",
      "94.6\n",
      "Swish-1\n",
      "79.3\n",
      "94.7\n",
      "Swish\n",
      "79.3\n",
      "94.6\n",
      "Table 10: Inception-v4 on ImageNet.\n",
      "with a 1.4% boost on Mobile NASNet-A and a 2.2% boost on MobileNet ov...\n",
      "No images found on page 9\n",
      "Page 10 Text: Zoph et al., 2017; Real et al., 2017; Cai et al., 2017; Zhong et al., 2017) and optimizers (Bello\n",
      "et al., 2017). The use of search techniques to discover traditionally hand-designed components is an\n",
      "instance of the recently revived subﬁeld of meta-learning (Schmidhuber, 1987; Naik & Mammone,\n",
      "1992; Thrun & Pratt, 2012). Meta-learning has been used to ﬁnd initializations for one-shot learning\n",
      "(Finn et al., 2017; Ravi & Larochelle, 2016), adaptable reinforcement learning (Wang et al., 2016;\n",
      "Duan et...\n",
      "No images found on page 10\n",
      "Page 11 Text: Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
      "Layer normalization.\n",
      "In Advances in Neural\n",
      "Information Processing Systems, 2016.\n",
      "Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforcement\n",
      "learning. In International Conference on Machine Learning, pp. 459–468, 2017.\n",
      "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Reinforcement learning for architecture\n",
      "search by network transformation. arXiv preprint arXiv:1707.04873, 2017.\n",
      "Djork-Arn´e ...\n",
      "No images found on page 11\n",
      "Page 12 Text: Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\n",
      "networks. In Advances in Neural Information Processing Systems, pp. 1097–1105, 2012.\n",
      "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network acoustic\n",
      "models. In International Conference on Machine Learning, volume 30, 2013.\n",
      "Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable pooling with context gating for video classiﬁcation.\n",
      "arXiv preprint arXiv...\n",
      "No images found on page 12\n",
      "Page 13 Text: Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectiﬁed activations in convolutional\n",
      "network. arXiv preprint arXiv:1505.00853, 2015.\n",
      "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference,\n",
      "2016.\n",
      "Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. Practical network blocks design with q-learning. arXiv preprint\n",
      "arXiv:1708.05552, 2017.\n",
      "Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu, Junqi Jin, Han\n",
      "Li, and...\n",
      "No images found on page 13\n",
      "Finished processing 1710.05941.pdf\n",
      "Processing: 1907.01470.pdf\n",
      "Total pages: 11\n",
      "Page 1 Text: Augmenting Self-attention with Persistent Memory\n",
      "Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin\n",
      "Facebook AI Research\n",
      "sainbar,egrave,guismay,rvj,ajoulin@fb.com\n",
      "Abstract\n",
      "Transformer networks have lead to important progress in language modeling and\n",
      "machine translation. These models include two consecutive modules, a feed-\n",
      "forward layer and a self-attention layer. The latter allows the network to capture\n",
      "long term dependencies and are often regarded as the key ingr...\n",
      "No images found on page 1\n",
      "Page 2 Text: xt\n",
      "query\n",
      "context\n",
      "key\n",
      "value\n",
      "Self-attention\n",
      "Add-norm\n",
      "Feedforward\n",
      "Add-norm\n",
      "All-attention\n",
      "attention\n",
      "xt\n",
      "query\n",
      "context\n",
      "key\n",
      "value\n",
      "Add-norm\n",
      "attention\n",
      "ReLU\n",
      "Figure 1: On the left panel, the standard transformer layer is composed of a self-attention sublayer\n",
      "followed by a feedforward sublayer. On the right panel, our all-attention layer merges the weights of\n",
      "the feedforward sublayer with the self-attention sublayer. We represent both models in the case of a\n",
      "single head, but in the general case, both the se...\n",
      "No images found on page 2\n",
      "Page 3 Text: 3\n",
      "Transformer layer\n",
      "A transformer model is made of a stack of identical layers, called transformer layers. Each layer is\n",
      "composed of a multi-head self-attention sublayer followed by a feedforward sublayer. Each sublayer\n",
      "is also followed by an add-norm operation, i.e., a skip-connection [17], and layer normalization [23].\n",
      "In this section, we review the structure of the transformer layer and refer the reader to Vaswani et al.\n",
      "[40] for additional details of the overall model.\n",
      "Multi-head self-attent...\n",
      "No images found on page 3\n",
      "Page 4 Text: Transformer layer.\n",
      "The overall transformer layer has the following set of equations:\n",
      "zt\n",
      "=\n",
      "AddNorm(MultiHead(xt)),\n",
      "(7)\n",
      "yt\n",
      "=\n",
      "AddNorm(FF(zt)),\n",
      "(8)\n",
      "where MultiHead is the multi-head self-attention sublayer. This is shown on the left panel of Fig. 1.\n",
      "4\n",
      "Our approach\n",
      "In this section, we ﬁrst show that a feedforward sublayer can be viewed as an attention layer. Then, we\n",
      "take advantage of this interpretation of a feedforward model to concatenate it with the self-attention\n",
      "layer, forming a novel layer tha...\n",
      "No images found on page 4\n",
      "Page 5 Text: As with a self-attention sublayer, an all-attention layer can have multiple heads, where outputs from\n",
      "the different heads are concatenated for each timestep t and multiplied Wo. Note that persistent\n",
      "vectors are not shared between heads. Our overall layer is then simply this new MultiHeadAllAttn\n",
      "sublayer followed by the AddNorm operation as deﬁned in Eq. (6), i.e.,\n",
      "yt = AddNorm (MultiHeadAllAttn(xt)) .\n",
      "(14)\n",
      "The right panel of Fig. 1 summarize the all-attention layer in the case of a single head: ...\n",
      "No images found on page 5\n",
      "Page 6 Text: is that infrequent words are hard to predict and there is thus no need to use many parameters for\n",
      "them. The memory footprint of the model is further reduced by tying up the embedding weights with\n",
      "the classiﬁer weights [19, 32]. In the case of the adaptive softmax, this leads to a special form of\n",
      "embeddings called adaptive input [2].\n",
      "5\n",
      "Experiments\n",
      "5.1\n",
      "Experimental setup\n",
      "In this section, we describe our hyperparameters choices, our optimization scheme as well as the\n",
      "details of the datasets we cons...\n",
      "No images found on page 6\n",
      "Page 7 Text: Table 1: Comparison with the state of the art on character level language modeling on enwik8. We\n",
      "report bpc for the test set as well as the number of parameters.\n",
      "Model\n",
      "#Params\n",
      "test bpc\n",
      "Small models\n",
      "Ha et al. [16] – LN HyperNetworks\n",
      "27M\n",
      "1.34\n",
      "Chung et al. [7] – LN HM-LSTM\n",
      "35M\n",
      "1.32\n",
      "Zilly et al. [45] – Recurrent highway networks\n",
      "46M\n",
      "1.27\n",
      "Mujika et al. [30] – Large FS-LSTM-4\n",
      "47M\n",
      "1.25\n",
      "Krause et al. [22] – Large mLSTM\n",
      "46M\n",
      "1.24\n",
      "Al-Rfou et al. [1] – T12\n",
      "44M\n",
      "1.11\n",
      "Dai et al. [8] – Transformer-XL\n",
      "41M\n",
      "1.06\n",
      "S...\n",
      "No images found on page 7\n",
      "Page 8 Text: Table 3: Comparison with the state of the art on word level language modeling on WikiText-103.\n",
      "We report perplexity (ppl) for the dev and test sets as well as the number of parameters.\n",
      "Model\n",
      "#Params\n",
      "dev ppl\n",
      "test ppl\n",
      "Small models\n",
      "Grave et al. [14] – LSTM\n",
      "-\n",
      "-\n",
      "48.7\n",
      "Bai et al. (2018) – TCN\n",
      "-\n",
      "-\n",
      "45.2\n",
      "Dauphin et al. [9] – GCNN-8\n",
      "-\n",
      "-\n",
      "44.9\n",
      "Grave et al. [14] – LSTM + Neural cache\n",
      "-\n",
      "-\n",
      "40.8\n",
      "Merity et al. [26] – 4-layer QRNN\n",
      "151M\n",
      "32.0\n",
      "33.0\n",
      "Rae et al. [33] – LSTM + Hebbian + Cache\n",
      "-\n",
      "29.7\n",
      "29.9\n",
      "Dai et al. [8] –...\n",
      "No images found on page 8\n",
      "Page 9 Text: 6\n",
      "Conclusion\n",
      "In this paper, we propose a novel attention layer that presents a uniﬁed mechanism to aggregate\n",
      "general and contextual information. It extends the self-attention layer of a transformer with a set of\n",
      "persistent vectors that are capable of storing information that is complementary to the short term\n",
      "information in contexts. We also show that these persistent vectors can replace the feedforward layers\n",
      "in a transformer network with no loss of performance. We think that this simpliﬁed lay...\n",
      "No images found on page 9\n",
      "Page 10 Text: [16] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In ICLR, 2017.\n",
      "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pages 770–778, 2016.\n",
      "[18] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n",
      "1735–1780, 1997.\n",
      "[19] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers:\n",
      "A loss...\n",
      "No images found on page 10\n",
      "Page 11 Text: [37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n",
      "Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "[38] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In NIPS, 2015.\n",
      "[39] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\n",
      "span in transformers. In ACL, 2019.\n",
      "[40] Ashish Vaswani, ...\n",
      "No images found on page 11\n",
      "Finished processing 1907.01470.pdf\n",
      "Processing: 2302.10866.pdf\n",
      "Total pages: 38\n",
      "Page 1 Text: Hyena Hierarchy:\n",
      "Towards Larger Convolutional Language Models\n",
      "Michael Poli∗,1, Stefano Massaroli∗,2, Eric Nguyen1,∗,\n",
      "Daniel Y. Fu1, Tri Dao1, Stephen Baccus1,\n",
      "Yoshua Bengio2, Stefano Ermon1,†, Christopher Ré1,†\n",
      "Version: submitted draft, Last Compiled: April 21, 2023\n",
      "Abstract\n",
      "Recent advances in deep learning have relied heavily on the use of large Transformers due to their\n",
      "ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits\n",
      "quadratic cost ...\n",
      "No images found on page 1\n",
      "Page 2 Text: Figure 1.1: The Hyena operator is deﬁned as a recurrence of two eﬃcient subquadratic primitives: an implicit\n",
      "long convolution h (i.e. Hyena ﬁlters parameterized by a feed-forward network) and multiplicative element-\n",
      "wise gating of the (projected) input. The depth of the recurrence speciﬁes the size of the operator. Hyena\n",
      "can equivalently be expressed as a multiplication with data-controlled (conditioned by the input u) diagonal\n",
      "matrices Dx and Toeplitz matrices Sh. In addition, Hyena exhibits su...\n",
      "Extracted 1 images from page 2\n",
      "Page 3 Text: when task complexity increases (e.g. vocabulary size grows). In addition, we investigate the optimal param-\n",
      "eterization of long convolutions in Hyena. In the most challenging settings with hundreds of thousands of\n",
      "tokens, our implicit parameterization scheme improves over other operators leveraging state spaces (Gu et al.,\n",
      "2021), frequency-domain parametrizations (Li et al., 2020), or standard convolutions by over 50% accuracy.\n",
      "Scaling in language and vision\n",
      "Next, we aim to verify whether rankin...\n",
      "No images found on page 3\n",
      "Page 4 Text: of functions γθ is a design choice with a signiﬁcant impact on the expressivity and computational complexity\n",
      "of the layer.\n",
      "One choice of implicit parametrization is to select h as the response function of a linear state-space model\n",
      "(SSM) (Chen, 1984), described by the ﬁrst-order diﬀerence equation:\n",
      "xt+1 = Axt + But\n",
      "state equation\n",
      "yt = Cxt + Dut\n",
      "output equation\n",
      "Here, the convenient choice of x0 = 0 renders the input-output map to a simple convolution\n",
      "yt =\n",
      "t\n",
      "X\n",
      "n=0\n",
      "\u0000CAt−nB + Dδt−n\n",
      "\u0001\n",
      "un\n",
      "where δt den...\n",
      "No images found on page 4\n",
      "Page 5 Text: Figure 2.1: Comparison between data-controlled matrices: SelfAttention and Hyena.\n",
      "2.2\n",
      "The Self-Attention Operator\n",
      "At the heart of Transformers is the multi-head attention (MHA) mechanism. Given a length-L sequence\n",
      "u ∈RL×D, each head of scaled self-attention (Vaswani et al., 2017) is a map from RL×D to RL×D which\n",
      "performs the following operations\n",
      "A(u) = SoftMax\n",
      "\u0010\n",
      "1\n",
      "√\n",
      "DuMqM⊤\n",
      "k u⊤\u0011\n",
      "y = SelfAttention(u)\n",
      "= A(u)uMv,\n",
      "(3)\n",
      "where Mq, Mk, Mv ∈RD×D are learnable linear projections and SoftMax is intended to...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: ii. The matrix H(u) is deﬁned by interleaving implicit long convolutions and element-wise multiplication\n",
      "with one projection xi at a time, until all projections are exhausted.\n",
      "Evaluation of H(u)v is done\n",
      "eﬃciently without materializing H(u). By doing so, we implicitly deﬁne a data-controlled operator\n",
      "as a factorization of a matrix. The long convolutions forming H(u) are parametrized implicitly to retain\n",
      "sublinear parameter scaling in sequence length.\n",
      "Next, we formally deﬁne Hyena, starting with ...\n",
      "No images found on page 6\n",
      "Page 7 Text: Sequence Length\n",
      "FFN(t)\n",
      "Sequence Length\n",
      "Window\n",
      "Sequence Length\n",
      "Window ◦FFN(t)\n",
      "Figure 3.1:\n",
      "[Top]:\n",
      "Example of long convolution parametrization for Hyena operators, with a decay\n",
      "Window(t) = exp{−αt}. Parameter α is modiﬁed across the independent channels of Hyena to regularize\n",
      "ﬁlters to be of diﬀerent lengths. In practice, we add a bias term to our window, so that the ﬁlters are not\n",
      "constrained to be zeros after a length determined by the decay rate.\n",
      "Remark 3.2 (Hyena generalizes H3 and GSS.). The H...\n",
      "No images found on page 7\n",
      "Page 8 Text: Proposition 3.1 (Causal Hyenas). If each ﬁlter hn, n = 1, . . . , N is causal, then the corresponding HyenaN\n",
      "operator is causal.\n",
      "In practice, we need not constrain the learning of the ﬁlter (7) to ensure its numerical causality. If we\n",
      "use FFT-based convolution algorithms, all we need is to evaluate the ﬁlter at t = 0, . . . , L −1 and zero-pad\n",
      "the input and ﬁlter sequences to 2L −1 before taking FFT.\n",
      "Eﬃciency\n",
      "One bottleneck of long convolution models can be their low utilization of hardware acce...\n",
      "No images found on page 8\n",
      "Page 9 Text: 27\n",
      "29\n",
      "211\n",
      "213\n",
      "215\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Sequence Length\n",
      "Vocabulary Size: 10\n",
      "27\n",
      "29\n",
      "211\n",
      "213\n",
      "215\n",
      "Sequence Length\n",
      "Vocabulary Size: 20\n",
      "27\n",
      "29\n",
      "211\n",
      "213\n",
      "215\n",
      "Sequence Length\n",
      "Vocabulary Size: 30\n",
      "27\n",
      "29\n",
      "211\n",
      "213\n",
      "215\n",
      "Sequence Length\n",
      "Vocabulary Size: 40\n",
      "Hyena\n",
      "CKConv\n",
      "Transfer Function\n",
      "H3\n",
      "Conv1D\n",
      "FNO\n",
      "Associative Recall\n",
      "Figure 4.1: Benchmark of long convolution parametrizations in order 2 Hyena operators on associative recall\n",
      "(%). Our results show that implicit parametrizations scale more favorably in vocabulary size (...\n",
      "No images found on page 9\n",
      "Page 10 Text: Table 4.2: Test accuracy (%) for associative recall on longer sequences, vocabulary size 30. The symbol \u0017 is\n",
      "used to mark settings where the model does not ﬁt in memory.\n",
      "Sequence length\n",
      "Hyena\n",
      "FlashTransformer\n",
      "Transformer\n",
      "GSS\n",
      "H3\n",
      "AFT\n",
      "RWKV\n",
      "30k\n",
      "100.0\n",
      "32.4\n",
      "\u0017\n",
      "5.3\n",
      "8.4\n",
      "2.3\n",
      "12.4\n",
      "64k\n",
      "100.0\n",
      "26.7\n",
      "\u0017\n",
      "2.1\n",
      "4.3\n",
      "1.2\n",
      "6.5\n",
      "131k\n",
      "97.2\n",
      "\u0017\n",
      "\u0017\n",
      "0.1\n",
      "0.6\n",
      "0.8\n",
      "2.3\n",
      "• Hyena: Combination of implicit parametrizations via FFNs (with exponential decay modulation as shown\n",
      "in Figure 3.1), and short explicit ﬁlters.\n",
      "All models have the ...\n",
      "No images found on page 10\n",
      "Page 11 Text: 1.3 1.6\n",
      "2.6\n",
      "3.2\n",
      "3.9\n",
      "4.9\n",
      "·1019\n",
      "2.44\n",
      "2.29\n",
      "2.21\n",
      "FLOPs\n",
      "Loss\n",
      "Data Scaling on The Pile, 355M parameters\n",
      "Hyena\n",
      "GPT\n",
      "Figure 4.2: Preliminary \"scaling law\" of language models on The Pile. Comparison of our approach (red)\n",
      "based on long convolutions and gating (Hyena) and a standard GPT (blue) (Brown et al., 2020). We reach\n",
      "perplexity of GPT with a smaller training FLOP budget.\n",
      "Table 4.3: Perplexity on WikiText103\n",
      "(same tokenizer). ∗are results from (Dao\n",
      "et al., 2022c). Deeper and thinner models\n",
      "(Hyena-slim...\n",
      "No images found on page 11\n",
      "Page 12 Text: Table 4.6: Few-shot (3) accuracy (%) on SuperGLUE tasks for small models.\n",
      "Model\n",
      "WSC\n",
      "WIC\n",
      "RTE\n",
      "CB\n",
      "MultiRC\n",
      "ReCoRD\n",
      "BoolQ\n",
      "COPA\n",
      "Average\n",
      "GPTNeo (Black et al., 2021)\n",
      "38.5\n",
      "50.0\n",
      "53.8\n",
      "42.9\n",
      "22.4\n",
      "61.4\n",
      "61.0\n",
      "63.0\n",
      "49.1\n",
      "RWKV (Peng, 2021)\n",
      "32.7\n",
      "49.4\n",
      "47.2\n",
      "37.5\n",
      "0.0\n",
      "58.3\n",
      "55.0\n",
      "64.0\n",
      "43.0\n",
      "Hyena\n",
      "39.4\n",
      "50.1\n",
      "47.6\n",
      "46.4\n",
      "26.7\n",
      "58.1\n",
      "56.0\n",
      "70.0\n",
      "49.3\n",
      "103\n",
      "104\n",
      "105\n",
      "0\n",
      "50\n",
      "100\n",
      "Sequence Length\n",
      "Runtime [ms]\n",
      "103\n",
      "103.2\n",
      "103.4\n",
      "103.6\n",
      "103.8\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Sequence Length\n",
      "Hyena\n",
      "Attention\n",
      "FlashAttention\n",
      "Benchmarking Hyena\n",
      "Figure 4.3: Benchmarking runtime ...\n",
      "No images found on page 12\n",
      "Page 13 Text: 5\n",
      "Discussion and Conclusion\n",
      "In this work, we introduced an attention-free drop-in replacement to the core building block of many large-\n",
      "scale language models. Hyena operators are a recurrence of gating and implicitly parametrized long convo-\n",
      "lutions, can be evaluated eﬃciently in subquadratic time, and can learn in-context on very long sequences.\n",
      "On The Pile, deep stacks of Hyena operators constitute one of the ﬁrst attention-free, convolutional archi-\n",
      "tectures to match perplexity and downstream...\n",
      "No images found on page 13\n",
      "Page 14 Text: P. Cramer. Alphafold2 and the future of structural biology. Nature structural & molecular biology, 28(9):\n",
      "704–705, 2021.\n",
      "E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\n",
      "with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition workshops, pages 702–703, 2020.\n",
      "T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Ré. Learning fast algorithms for linear transforms using\n",
      "butterﬂy factorizations. In Inter...\n",
      "No images found on page 14\n",
      "Page 15 Text: Y. Li, H. Yang, E. R. Martin, K. L. Ho, and L. Ying.\n",
      "Butterﬂy factorization.\n",
      "Multiscale Modeling &\n",
      "Simulation, 13(2):714–732, 2015.\n",
      "Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey. What makes convolutional models great on long sequence\n",
      "modeling? arXiv preprint arXiv:2210.09298, 2022.\n",
      "Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier\n",
      "neural operator for parametric partial diﬀerential equations. arXiv preprint arXiv:2010.08895, 2020.\n",
      "P. Liang, R. B...\n",
      "No images found on page 15\n",
      "Page 16 Text: I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In Inter-\n",
      "national Conference on Machine Learning, pages 9355–9366. PMLR, 2021.\n",
      "I. W. Selesnick and C. S. Burrus. Fast convolution and ﬁltering. In The Digital Signal Processing Handbook,\n",
      "pages 8–1. CRC Press, 2017.\n",
      "V. Sitzmann, J. N. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit neural representations\n",
      "with periodic activation functions. arXiv preprint arXiv:2006.09661, 2020.\n",
      "C. S...\n",
      "No images found on page 16\n",
      "Page 17 Text: Hyena Hierarchy\n",
      "Supplementary Material\n",
      "Contents\n",
      "1\n",
      "Introduction\n",
      "1\n",
      "2\n",
      "Preliminaries and Related Work\n",
      "3\n",
      "2.1\n",
      "Explicit and Implicit Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "3\n",
      "2.2\n",
      "The Self-Attention Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "3\n",
      "Hyena: Deﬁnition and Properties\n",
      "5\n",
      "3.1\n",
      "Hyena Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "3.2\n",
      "Hyena Matrices . . . . . . . . . . . ...\n",
      "No images found on page 17\n",
      "Page 18 Text: A\n",
      "Experimental Details\n",
      "An implementation of Hyena can be found at this link.\n",
      "A.1\n",
      "Mechanistic Design Synthetic Benchmarks\n",
      "Our synthetic reasoning are inspired by mechanistic interpretability (Elhage et al., 2021), in-context learning\n",
      "(ICL) (Garg et al., 2022) and language model benchmarking (Liang et al., 2022) research. The evaluation\n",
      "revolves around 4 main tasks:\n",
      "• Associative recall: Each string is produced by concatenating key-value tuples from a diﬀerent random\n",
      "dictionary.\n",
      "This test veriﬁes ...\n",
      "No images found on page 18\n",
      "Page 19 Text: • FNO: Filters parametrized explicitly in the frequency-domain (Li et al., 2020). We set the number of modes\n",
      "to 64.\n",
      "• H3: Implicit parametrization using state-space models (SSMs), and in particular the standard S4 (Gu et al.,\n",
      "2021). We set the state dimension to 64.\n",
      "• TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization\n",
      "of SSMs. Transfer functions are deﬁned by a ratio of polynomials (we parametrize the coeﬃcients, and\n",
      "evaluate the polynomial...\n",
      "No images found on page 19\n",
      "Page 20 Text: Table A.3: Hyperparameter settings for The Pile, 125M).\n",
      "Optimizer\n",
      "AdamW\n",
      "Optimizer momentum\n",
      "β1, β2 = 0.9, 0.98\n",
      "Peak learning rate\n",
      "0.0006\n",
      "Warmup learning rate init\n",
      "0.000001\n",
      "Learning rate min\n",
      "0.00006\n",
      "Weight decay\n",
      "0.1\n",
      "Dropout\n",
      "None\n",
      "Batch size\n",
      "256\n",
      "Learning rate schedule\n",
      "cosine decay\n",
      "Warmup schedule\n",
      "linear\n",
      "The Pile:\n",
      "We follow a same procedure and train 125M and 355M-sized models on The Pile (Gao et al.,\n",
      "2020).\n",
      "Hyperparameters are reported in Table A.3.\n",
      "Hyperparameters for 355M are the same beyond a\n",
      "red...\n",
      "No images found on page 20\n",
      "Page 21 Text: iii. FFTConv: 5 × (order - 1) × d_model × log(seq_len) × seq_len.\n",
      "iv. Output: d_model × d_model × seq_len.\n",
      "with a leading factor 2 to account for both additions and multiplications.\n",
      "A.3\n",
      "Downstream Evaluation\n",
      "SuperGLUE:\n",
      "We evaluate models on the SuperGLUE (Wang et al., 2019) with the parsing pipeline of\n",
      "(Arora et al., 2022). For all tasks except WIC, CB and BoolQ, we generate a response using greedy decoding,\n",
      "then check for the gold label. WIC, CB and BoolQ use logit scoring instead of generation...\n",
      "No images found on page 21\n",
      "Page 22 Text: Table A.5: ViT and ViT-Hyena settings for ImageNet-1k).\n",
      "Image size\n",
      "2242\n",
      "Optimizer\n",
      "AdamW\n",
      "Optimizer momentum\n",
      "β1, β2 = 0.9, 0.999\n",
      "Weight init\n",
      "trunc. normal (std=0.02)\n",
      "ViT base learning rate\n",
      "1e−3\n",
      "Hyena-ViT base learning rate\n",
      "2e−4\n",
      "ViT weight decay\n",
      "0.05\n",
      "Hyena-ViT weight decay\n",
      "0.01\n",
      "Dropout\n",
      "None\n",
      "Batch size\n",
      "1024\n",
      "Training epochs\n",
      "300\n",
      "Learning rate schedule\n",
      "cosine decay\n",
      "Warmup epochs\n",
      "10\n",
      "Warmup schedule\n",
      "linear\n",
      "Randaugment (Cubuk et al., 2020)\n",
      "(9,0.5,layers=2)\n",
      "Mixup (Zhang et al., 2017)\n",
      "0.8\n",
      "Cutmix (Yun et al....\n",
      "No images found on page 22\n",
      "Page 23 Text: And we can deﬁne the surrogate attention matrix Aψ\n",
      "ϕ(q, k)\n",
      "[Aψ\n",
      "ϕ(q, k))]t,t′ = qt\n",
      "L−1\n",
      "X\n",
      "m=0\n",
      "ψt−mkmϕm−t′.\n",
      "(12)\n",
      "Continuous Signals: We can also consider the case of continuous signals on a group G. In the\n",
      "continuous case, we can expand the convolutions in (8) as\n",
      "(ϕ ∗v)t =\n",
      "Z\n",
      "G\n",
      "ϕt−gvgdg,\n",
      "(ψ ∗z)t =\n",
      "Z\n",
      "G\n",
      "ψt−gzgdg\n",
      "(13)\n",
      "This allows us to rewrite (8) as\n",
      "yt = qt(ψ ∗k(ϕ ∗v))t\n",
      "= qt\n",
      "Z\n",
      "G\n",
      "ψt−g\n",
      "\u0014\n",
      "kg\n",
      "Z\n",
      "G\n",
      "ϕg−τvτdτ\n",
      "\u0015\n",
      "dg\n",
      "= qt\n",
      "Z\n",
      "G\n",
      "\u0014Z\n",
      "G\n",
      "ψt−gkgϕg−τvτdτ\n",
      "\u0015\n",
      "dg\n",
      "= qt\n",
      "Z\n",
      "G\n",
      "\u0014Z\n",
      "G\n",
      "ψt−gkgϕg−τvτdg\n",
      "\u0015\n",
      "dτ\n",
      "Variable swap\n",
      "=\n",
      "Z\n",
      "G\n",
      "\u0014\n",
      "qt\n",
      "Z\n",
      "G...\n",
      "No images found on page 23\n",
      "Page 24 Text: We can expand the matrix multiplications in (16) in the case of causal ﬁlters ϕ and ψ as\n",
      "Dq\n",
      "\n",
      "\n",
      "q0\n",
      "q1\n",
      "...\n",
      "qL−1\n",
      "\n",
      "\n",
      "Sψ\n",
      "\n",
      "\n",
      "ψ0\n",
      "ψ1\n",
      "ψ0\n",
      "...\n",
      "...\n",
      "...\n",
      "ψL−1\n",
      "ψL−2\n",
      "· · ·\n",
      "ψ0\n",
      "\n",
      "\n",
      "Dk\n",
      "\n",
      "\n",
      "k0\n",
      "k1\n",
      "...\n",
      "kL−1\n",
      "\n",
      "\n",
      "Sϕ\n",
      "\n",
      "\n",
      "ϕ0\n",
      "ϕ1\n",
      "ϕ0\n",
      "...\n",
      "...\n",
      "...\n",
      "ϕL−1\n",
      "ϕL−2\n",
      "· · ·\n",
      "ϕ0\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "q0ψ0\n",
      "q1ψ1\n",
      "q1ψ0\n",
      "...\n",
      "...\n",
      "...\n",
      "qL−1ψL−1\n",
      "qL−1ψL−2\n",
      "· · ·\n",
      "qL−1ψ0\n",
      "\n",
      "\n",
      "Aψ(q)\n",
      "\n",
      "\n",
      "k0ϕ0\n",
      "k1ϕ1\n",
      "k1ϕ0\n",
      "...\n",
      "...\n",
      "...\n",
      "kL−1ϕL−1\n",
      "kL−1ϕL−2\n",
      "· · ·\n",
      "kL−1ϕ0\n",
      "\n",
      "\n",
      "Aϕ(k)\n",
      "(19)\n",
      "Fourier decomposition of convolution operators...\n",
      "No images found on page 24\n",
      "Page 25 Text: 0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 1, Digits: 2\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 1, Digits: 4\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 1, Digits: 8\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 1, Digits: 16\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 2, Digits: 2\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 2, Digits: 4\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 2, Digits: 8\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 2, Digits: 16\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 3, Digits: 2\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "epochs\n",
      "Layers: 3, Digits: ...\n",
      "No images found on page 25\n",
      "Page 26 Text: D\n",
      "Samples and Visualizations\n",
      "D.1\n",
      "Hyena Matrices\n",
      "We provide visualizations of attention and Hyena matrices activated by test strings. In D.1, D.2, we compare\n",
      "GPTNeo (Black et al., 2021) attention matrices with Hyena matrices extracted by our pre-trained small\n",
      "Hyena model. In D.3 and D.4, we provide additional Hyena matrices for the 355M model, activated by test\n",
      "strings of diﬀerent length.\n",
      "For attention, we visualize the raw post-softmax matrix. For Hyena matrices, we plot the (element-wise)\n",
      "absol...\n",
      "No images found on page 26\n",
      "Page 27 Text: Figure D.1: Attention matrices from a GPTNeo small model. \"We use the test string \"Attention is all you\n",
      "need. Attention is\".\n",
      "27\n",
      "...\n",
      "Extracted 1 images from page 27\n",
      "Page 28 Text: Figure D.2: Hyena matrices from a Hyena small (same model used for SuperGLUE downstream evaluations).\n",
      "\"We use the test string \"Attention is all you need.\n",
      "Attention is\".\n",
      "We note that Hyena has a diﬀerent\n",
      "data-controlled matrix for each channel i.e. for each dimension in its width, since it does not use heads.\n",
      "28\n",
      "...\n",
      "Extracted 1 images from page 28\n",
      "Page 29 Text: Figure D.3: Data-controlled Hyena matrices (355M model), activated by the string \"When a doctor doctors\n",
      "a doctor, does the doctor doing the doctoring doctor as the doctor being doctored wants to be doctored or does\n",
      "the doctor doing the doctoring doctor as they want to doctor?\". Rows in the plot are matrices from diﬀerent\n",
      "layers, columns are matrices from diﬀerent channels. The operator shows characteristic patterns of attention\n",
      "matrices, without attention.\n",
      "29\n",
      "...\n",
      "Extracted 36 images from page 29\n",
      "Page 30 Text: Figure D.4: Data-controlled Hyena matrices (355M model), activated by the string \"Mrs.\n",
      "Dursley, Mr.\n",
      "Dursley, Dudley Dursley\", from Causal scrubbing: results on induction heads. Rows in the plot are matrices\n",
      "from diﬀerent layers, columns are matrices from diﬀerent channels.\n",
      "30\n",
      "...\n",
      "Extracted 36 images from page 30\n",
      "Page 31 Text: D.2\n",
      "Hyena Filters\n",
      "Figure D.5 provides a visualization of Hyena long convolution ﬁlters at initialization and after training to\n",
      "completion on The Pile.\n",
      "We ﬁnd a substantial performance diﬀerence (up to 5% perplexity) between initialization schemes. If the\n",
      "ﬁlters at initialization are excessively smooth (see Appendix D.3 for a discussion of positional encoding and\n",
      "activation), the model ﬁnds a worse solution and takes longer to converge. Further, we observe initialization\n",
      "schemes that regularize ﬁ...\n",
      "No images found on page 31\n",
      "Page 32 Text: Figure D.5: [Top]: Long convolution Hyena ﬁlters at initialization (153M parameters, 18 layer model).\n",
      "[Bottom]: Filters after training for 130 billion tokens on The Pile.\n",
      "32\n",
      "...\n",
      "Extracted 2 images from page 32\n",
      "Page 33 Text: 0\n",
      "8\n",
      "16\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Positional Encoding Feature\n",
      "Sequence Index\n",
      "Positional Encoding\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Impulse Response ht\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Magnitude Response |FFT[h]|\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Phase Response ̸ FFT[h]\n",
      "Figure D.6: Hyena ﬁlters at initialization with 17 positional encoding features K = 8.\n",
      "0\n",
      "16\n",
      "32\n",
      "48\n",
      "64\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Positional Encoding Feature\n",
      "Sequence Index\n",
      "Positional Encoding\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "...\n",
      "Extracted 8 images from page 33\n",
      "Page 34 Text: 0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Positional Encoding Feature\n",
      "Sequence Index\n",
      "Positional Encoding\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Impulse Response ht\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Magnitude Response |FFT[h]|\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Index\n",
      "Phase Response ̸ FFT[h]\n",
      "Figure D.8: Hyena ﬁlters at initialization with 65 positional encoding features K = 64.\n",
      "0\n",
      "8\n",
      "16\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Positional Encoding Feature\n",
      "Sequence Index\n",
      "Positional Encoding\n",
      "0\n",
      "8\n",
      "16 24 32\n",
      "128\n",
      "96\n",
      "64\n",
      "32\n",
      "0\n",
      "Filter Inde...\n",
      "Extracted 8 images from page 34\n",
      "Page 35 Text: D.4\n",
      "Downstream Examples\n",
      "MultiRC\n",
      "We report examples of downstream evaluation of small models on the MultiRC question-answering\n",
      "task. We report answers of small Hyena (153M, trained for 130B tokens on The Pile) and the public check-\n",
      "point RWKV-v4 (Peng, 2021) (169M, trained for 332B tokens on The Pile). We select randomized examples\n",
      "with indices being powers of 2. Alignment of Hyena’s responses to the task format is greatly improved by\n",
      "providing few-shot examples in the prompt, which may be a prom...\n",
      "No images found on page 35\n",
      "Page 36 Text: MultiRC index: 32, 64\n",
      "Passage:\n",
      "The film opens with Sunita, a medical student, and her friends working on a project\n",
      "about the human brain.\n",
      "She wants to investigate the curious case of Sanjay Singhania,\n",
      "a notable city businessman, who is reported to have anterograde amnesia.\n",
      "Her professor\n",
      "denies access to Sanjay’s records as it is currently under criminal investigation.\n",
      "Sunita,\n",
      "nonetheless, decides to investigate the matter herself.\n",
      "Sanjay is introduced as he brutally\n",
      "murders a man.\n",
      "He takes a Pol...\n",
      "No images found on page 36\n",
      "Page 37 Text: MultiRC index: 128\n",
      "Passage:\n",
      "In 1863, Alexander II re-convened the Diet of Finland and initiated several reforms\n",
      "increasing Finlandś autonomy from Russia including establishment of its own currency,\n",
      "the markka.\n",
      "Liberation of business led to increased foreign investment and industrial\n",
      "development.\n",
      "Finland also got its first railways, separately established under Finnish\n",
      "administration.\n",
      "Finally, the elevation of Finnish from a language of the common people to\n",
      "a national language equal to Swedish op...\n",
      "No images found on page 37\n",
      "Page 38 Text: MultiRC index: 1024\n",
      "Passage:\n",
      "Einstein and Maric married in January 1903.\n",
      "In May 1904, the couple’s first son,\n",
      "Hans Albert Einstein, was born in Bern, Switzerland.\n",
      "Their second son, Eduard, was born\n",
      "in Zurich in July 1910.\n",
      "In 1914, the couple separated; Einstein moved to Berlin and his\n",
      "wife remained in Zurich with their sons.\n",
      "They divorced on 14 February 1919, having lived\n",
      "apart for five years.\n",
      "Eduard, whom his father called \"Tete\" (for petit), had a breakdown at\n",
      "about age 20 and was diagnosed wi...\n",
      "No images found on page 38\n",
      "Finished processing 2302.10866.pdf\n",
      "Processing: 10000000_662098952474184_2584067087619170692_n.pdf\n",
      "Total pages: 77\n",
      "Page 1 Text: Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev...\n",
      "No images found on page 1\n",
      "Page 2 Text: Contents\n",
      "1\n",
      "Introduction\n",
      "3\n",
      "2\n",
      "Pretraining\n",
      "5\n",
      "2.1\n",
      "Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.2\n",
      "Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.3\n",
      "Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "3\n",
      "Fine-tuning\n",
      "8\n",
      "3.1\n",
      "Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "3...\n",
      "No images found on page 2\n",
      "Page 3 Text: Figure 1: Helpfulness human evaluation results for Llama\n",
      "2-Chat compared to other open-source and closed-source\n",
      "models. Human raters compared model generations on ~4k\n",
      "prompts consisting of both single and multi-turn prompts.\n",
      "The 95% confidence intervals for this evaluation are between\n",
      "1% and 2%. More details in Section 3.4.2. While reviewing\n",
      "these results, it is important to note that human evaluations\n",
      "can be noisy due to limitations of the prompt set, subjectivity\n",
      "of the review guidelines, subj...\n",
      "Extracted 2 images from page 3\n",
      "Page 4 Text: Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\n",
      "source models. Human raters judged model generations for safety violations across ~2,000 adversarial\n",
      "prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\n",
      "important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\n",
      "prompt set, subjectivity of the review guidelines, and subjectivity of individual r...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: Figure 4: Training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly\n",
      "available online sources. Following this, we create an initial version of Llama 2-Chat through the application\n",
      "of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning\n",
      "with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy\n",
      "Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative rewa...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: Training Data\n",
      "Params\n",
      "Context\n",
      "Length\n",
      "GQA\n",
      "Tokens\n",
      "LR\n",
      "Llama 1\n",
      "See Touvron et al.\n",
      "(2023)\n",
      "7B\n",
      "2k\n",
      "✗\n",
      "1.0T\n",
      "3.0 × 10−4\n",
      "13B\n",
      "2k\n",
      "✗\n",
      "1.0T\n",
      "3.0 × 10−4\n",
      "33B\n",
      "2k\n",
      "✗\n",
      "1.4T\n",
      "1.5 × 10−4\n",
      "65B\n",
      "2k\n",
      "✗\n",
      "1.4T\n",
      "1.5 × 10−4\n",
      "Llama 2\n",
      "A new mix of publicly\n",
      "available online data\n",
      "7B\n",
      "4k\n",
      "✗\n",
      "2.0T\n",
      "3.0 × 10−4\n",
      "13B\n",
      "4k\n",
      "✗\n",
      "2.0T\n",
      "3.0 × 10−4\n",
      "34B\n",
      "4k\n",
      "✓\n",
      "2.0T\n",
      "1.5 × 10−4\n",
      "70B\n",
      "4k\n",
      "✓\n",
      "2.0T\n",
      "1.5 × 10−4\n",
      "Table 1: Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with\n",
      "a global batch-size of 4M tokens. Bigger models — 34B and ...\n",
      "No images found on page 6\n",
      "Page 7 Text: Time\n",
      "(GPU hours)\n",
      "Power\n",
      "Consumption (W)\n",
      "Carbon Emitted\n",
      "(tCO2eq)\n",
      "Llama 2\n",
      "7B\n",
      "184320\n",
      "400\n",
      "31.22\n",
      "13B\n",
      "368640\n",
      "400\n",
      "62.44\n",
      "34B\n",
      "1038336\n",
      "350\n",
      "153.90\n",
      "70B\n",
      "1720320\n",
      "400\n",
      "291.42\n",
      "Total\n",
      "3311616\n",
      "539.00\n",
      "Table 2: CO2 emissions during pretraining. Time: total GPU time required for training each model. Power\n",
      "Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency.\n",
      "100% of the emissions are directly offset by Meta’s sustainability program, and because we are openly releasing\n",
      "th...\n",
      "No images found on page 7\n",
      "Page 8 Text: Model\n",
      "Size\n",
      "Code\n",
      "Commonsense\n",
      "Reasoning\n",
      "World\n",
      "Knowledge\n",
      "Reading\n",
      "Comprehension\n",
      "Math MMLU\n",
      "BBH\n",
      "AGI Eval\n",
      "MPT\n",
      "7B\n",
      "20.5\n",
      "57.4\n",
      "41.0\n",
      "57.5\n",
      "4.9\n",
      "26.8\n",
      "31.0\n",
      "23.5\n",
      "30B\n",
      "28.9\n",
      "64.9\n",
      "50.0\n",
      "64.7\n",
      "9.1\n",
      "46.9\n",
      "38.0\n",
      "33.8\n",
      "Falcon\n",
      "7B\n",
      "5.6\n",
      "56.1\n",
      "42.8\n",
      "36.0\n",
      "4.6\n",
      "26.2\n",
      "28.0\n",
      "21.2\n",
      "40B\n",
      "15.2\n",
      "69.2\n",
      "56.7\n",
      "65.7\n",
      "12.6\n",
      "55.4\n",
      "37.1\n",
      "37.0\n",
      "Llama 1\n",
      "7B\n",
      "14.1\n",
      "60.8\n",
      "46.2\n",
      "58.5\n",
      "6.95\n",
      "35.1\n",
      "30.3\n",
      "23.9\n",
      "13B\n",
      "18.9\n",
      "66.1\n",
      "52.6\n",
      "62.3\n",
      "10.9\n",
      "46.9\n",
      "37.0\n",
      "33.9\n",
      "33B\n",
      "26.0\n",
      "70.0\n",
      "58.4\n",
      "67.6\n",
      "21.4\n",
      "57.8\n",
      "39.8\n",
      "41.7\n",
      "65B\n",
      "30.7\n",
      "70.7\n",
      "60.5\n",
      "68.6\n",
      "30.8\n",
      "63.4\n",
      "43.5\n",
      "47.6\n",
      "Llama 2\n",
      "7B\n",
      "16.8\n",
      "63.9\n",
      "48.9\n",
      "61.3\n",
      "14.6\n",
      "4...\n",
      "No images found on page 8\n",
      "Page 9 Text: 3.1\n",
      "Supervised Fine-Tuning (SFT)\n",
      "➤Prompt:\n",
      "Write a poem to help me remember the first 10 elements on the periodic table, giving each\n",
      "element its own line.\n",
      "Response:\n",
      "Hydrogen comes first as element number one.\n",
      "Helium is second for balloons to have fun!\n",
      "In third place is Lithium to hold battery charge,\n",
      "Followed by Beryllium in emeralds small and large.\n",
      "Boron’s number five to help us keep things clean.\n",
      "Carbon’s next at six, and it’s found in every gene.\n",
      "Nitrogen is seven, found in every breath we ta...\n",
      "No images found on page 9\n",
      "Page 10 Text: sampled human preferences, whereby human annotators select which of two model outputs they prefer.\n",
      "This human feedback is subsequently used to train a reward model, which learns patterns in the preferences\n",
      "of the human annotators and can then automate preference decisions.\n",
      "3.2.1\n",
      "Human Preference Data Collection\n",
      "Next, we collect human preference data for reward modeling. We chose a binary comparison protocol over\n",
      "other schemes, mainly because it enables us to maximize the diversity of collected p...\n",
      "No images found on page 10\n",
      "Page 11 Text: Dataset\n",
      "Num. of\n",
      "Comparisons\n",
      "Avg. # Turns\n",
      "per Dialogue\n",
      "Avg. # Tokens\n",
      "per Example\n",
      "Avg. # Tokens\n",
      "in Prompt\n",
      "Avg. # Tokens\n",
      "in Response\n",
      "Anthropic Helpful\n",
      "122,387\n",
      "3.0\n",
      "251.5\n",
      "17.7\n",
      "88.4\n",
      "Anthropic Harmless\n",
      "43,966\n",
      "3.0\n",
      "152.5\n",
      "15.7\n",
      "46.4\n",
      "OpenAI Summarize\n",
      "176,625\n",
      "1.0\n",
      "371.1\n",
      "336.0\n",
      "35.1\n",
      "OpenAI WebGPT\n",
      "13,333\n",
      "1.0\n",
      "237.2\n",
      "48.3\n",
      "188.9\n",
      "StackExchange\n",
      "1,038,480\n",
      "1.0\n",
      "440.2\n",
      "200.1\n",
      "240.2\n",
      "Stanford SHP\n",
      "74,882\n",
      "1.0\n",
      "338.3\n",
      "199.5\n",
      "138.8\n",
      "Synthetic GPT-J\n",
      "33,139\n",
      "1.0\n",
      "123.3\n",
      "13.0\n",
      "110.3\n",
      "Meta (Safety & Helpfulness)\n",
      "1,418,091\n",
      "3.9\n",
      "798.5\n",
      "31.4\n",
      "234.1...\n",
      "No images found on page 11\n",
      "Page 12 Text: Helpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal\n",
      "parts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The\n",
      "Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta\n",
      "Helpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10%\n",
      "helpfulness data is especially beneficial for the accuracy on samples where both the chosen and...\n",
      "No images found on page 12\n",
      "Page 13 Text: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Meta Helpfulness Data Batch Stage\n",
      "0.52\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.60\n",
      "0.62\n",
      "0.64\n",
      "Accuracy On All Examples\n",
      "7b\n",
      "13b\n",
      "70b\n",
      "GPT4\n",
      "OpenAssistant\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Meta Helpfulness Data Batch Stage\n",
      "0.50\n",
      "0.55\n",
      "0.60\n",
      "0.65\n",
      "0.70\n",
      "0.75\n",
      "0.80\n",
      "Accuracy On Examples With Label \"Significantly Better\"\n",
      "7b\n",
      "13b\n",
      "70b\n",
      "GPT4\n",
      "OpenAssistant\n",
      "Figure 6: Scaling trends for the reward model. More data and a larger-size model generally improve\n",
      "accuracy, and it appears that our models have not yet sat...\n",
      "No images found on page 13\n",
      "Page 14 Text: 100\n",
      "101\n",
      "N Samples\n",
      "0.54\n",
      "0.56\n",
      "0.58\n",
      "0.60\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "Reward Score\n",
      "Max of the rewards\n",
      "Median of the rewards\n",
      "Figure 7: Max and median reward among N samples, N ∈[1, . . . , 100] averaged over our training set of\n",
      "prompts. The delta between max and median can be interpreted as potential gain with Rejection Sampling.\n",
      "the highest reward score is considered the new gold standard. Similar to Scialom et al. (2020a), we\n",
      "then fine-tune our model on the new set of ranked samples, reinforcing the reward.\n",
      "The...\n",
      "No images found on page 14\n",
      "Page 15 Text: regression in some capabilities. For example, RLHF V3 struggled more than previous versions to compose\n",
      "rhyming lines in poems, as discerned through qualitative analysis, suggesting that further investigation into\n",
      "the causes of and mitigations for forgetting (Kirkpatrick et al., 2017; Nguyen et al., 2019; Ramasesh et al.,\n",
      "2021) could be a fruitful area for additional future research.\n",
      "In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from\n",
      "all pri...\n",
      "No images found on page 15\n",
      "Page 16 Text: Figure 9: Issues with multi-turn memory (left) can be improved with GAtt (right).\n",
      "We train for between 200 and 400 iterations for all our models, and use evaluations on held-out prompts for\n",
      "early stopping. Each iteration of PPO on the 70B model takes on average ≈330 seconds. To train quickly with\n",
      "large batch sizes, we use FSDP (Zhao et al., 2023). This was effective when using O(1) forward or backward\n",
      "passes, but caused a large slow down (≈20×) during generation, even when using a large batch si...\n",
      "Extracted 2 images from page 16\n",
      "Page 17 Text: modify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”->\n",
      "”Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.\n",
      "GAtt Evaluation.\n",
      "We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is\n",
      "consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5). We tried to\n",
      "set constraints not present in the training of GAtt at inference time, for instan...\n",
      "No images found on page 17\n",
      "Page 18 Text: RLHF-v5\n",
      "(with PPO)\n",
      "RLHF-v5\n",
      "(no PPO)\n",
      "RLHF-v4\n",
      "RLHF-v3\n",
      "            RLHF-v2\n",
      "      RLHF-v1\n",
      "SFT-v2       \n",
      "SFT-v1\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "Helpfulness\n",
      "Judge: Meta Reward Models\n",
      "Harmlessness\n",
      "  RLHF-v5\n",
      "  (with PPO)\n",
      "RLHF-v5  \n",
      "(no PPO)  \n",
      "RLHF-v4\n",
      "RLHF-v3\n",
      "     RLHF-v2\n",
      "RLHF-v1     \n",
      "SFT-v2    \n",
      "SFT-v1\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "Helpfulness\n",
      "Judge: GPT-4\n",
      "Harmlessness\n",
      "Figure 11: Evolution of Llama 2-Chat. We show the evolution afte...\n",
      "No images found on page 18\n",
      "Page 19 Text: Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\n",
      "across ~4,000 helpfulness prompts with three raters per prompt.\n",
      "The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
      "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\n",
      "model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\n",
      "Inter-Rater Reliability (IRR...\n",
      "Extracted 1 images from page 19\n",
      "Page 20 Text: 4\n",
      "Safety\n",
      "WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting.\n",
      "In this section, we dive deeper into the important topic of safety measurements and mitigations. We first\n",
      "discuss our safety investigations into pretraining data and pretrained models (Section 4.1). Next, we describe\n",
      "the process of our safety alignment (Section 4.2), explaining how we collected safety-related annotations and\n",
      "utilized SFT and RLHF, and present experimental results. The...\n",
      "No images found on page 20\n",
      "Page 21 Text: Gender Pronouns\n",
      "75.23%\n",
      "Grammatical Person\n",
      "94.47%\n",
      "She (she, her, hers, herself)\n",
      "28.45%\n",
      "1st (I, me, my, mine, myself, ...)\n",
      "70.71%\n",
      "He (he, him, his, himself)\n",
      "50.73%\n",
      "2nd (you, your, yours, ...)\n",
      "61.80%\n",
      "Unspecified (they, them, their, ...)\n",
      "86.38%\n",
      "3rd (it, its, itself, she, her, he, him, ...)\n",
      "93.07%\n",
      "(a) Percentage of documents containing gender pronouns and grammatical person. 75% of all documents contain\n",
      "gendered pronouns. Within this subset, 28% of all documents contain She pronouns. 94% of all docum...\n",
      "Extracted 1 images from page 21\n",
      "Page 22 Text: Language\n",
      "Percent\n",
      "Language\n",
      "Percent\n",
      "en\n",
      "89.70%\n",
      "uk\n",
      "0.07%\n",
      "unknown\n",
      "8.38%\n",
      "ko\n",
      "0.06%\n",
      "de\n",
      "0.17%\n",
      "ca\n",
      "0.04%\n",
      "fr\n",
      "0.16%\n",
      "sr\n",
      "0.04%\n",
      "sv\n",
      "0.15%\n",
      "id\n",
      "0.03%\n",
      "zh\n",
      "0.13%\n",
      "cs\n",
      "0.03%\n",
      "es\n",
      "0.13%\n",
      "fi\n",
      "0.03%\n",
      "ru\n",
      "0.13%\n",
      "hu\n",
      "0.03%\n",
      "nl\n",
      "0.12%\n",
      "no\n",
      "0.03%\n",
      "it\n",
      "0.11%\n",
      "ro\n",
      "0.03%\n",
      "ja\n",
      "0.10%\n",
      "bg\n",
      "0.02%\n",
      "pl\n",
      "0.09%\n",
      "da\n",
      "0.02%\n",
      "pt\n",
      "0.09%\n",
      "sl\n",
      "0.01%\n",
      "vi\n",
      "0.08%\n",
      "hr\n",
      "0.01%\n",
      "Table 10: Language distribution in pretraining data with percentage >= 0.005%. Most data is in English,\n",
      "meaning that Llama 2 will perform best for English-language use cases. The large unknown category is\n",
      "par...\n",
      "No images found on page 22\n",
      "Page 23 Text: TruthfulQA ↑\n",
      "ToxiGen ↓\n",
      "MPT\n",
      "7B\n",
      "29.13\n",
      "22.32\n",
      "30B\n",
      "35.25\n",
      "22.61\n",
      "Falcon\n",
      "7B\n",
      "25.95\n",
      "14.53\n",
      "40B\n",
      "40.39\n",
      "23.44\n",
      "Llama 1\n",
      "7B\n",
      "27.42\n",
      "23.00\n",
      "13B\n",
      "41.74\n",
      "23.08\n",
      "33B\n",
      "44.19\n",
      "22.57\n",
      "65B\n",
      "48.71\n",
      "21.77\n",
      "Llama 2\n",
      "7B\n",
      "33.29\n",
      "21.25\n",
      "13B\n",
      "41.86\n",
      "26.10\n",
      "34B\n",
      "43.45\n",
      "21.19\n",
      "70B\n",
      "50.18\n",
      "24.60\n",
      "Table 11: Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the\n",
      "percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we\n",
      "present the percentage of toxic generations (the smal...\n",
      "No images found on page 23\n",
      "Page 24 Text: advice). The attack vectors explored consist of psychological manipulation (e.g., authority manipulation),\n",
      "logic manipulation (e.g., false premises), syntactic manipulation (e.g., misspelling), semantic manipulation\n",
      "(e.g., metaphor), perspective manipulation (e.g., role playing), non-English languages, and others.\n",
      "We then define best practices for safe and helpful model responses: the model should first address immediate\n",
      "safety concerns if applicable, then address the prompt by explaining the po...\n",
      "No images found on page 24\n",
      "Page 25 Text: 0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Safety RM Score before Safety RLHF\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Safety RM Score after Safety RLHF\n",
      "Safety \n",
      " Improvement\n",
      "0\n",
      "1000\n",
      "0\n",
      "1000\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Helpfulness RM Score before Safety RLHF\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Helpfulness RM Score after Safety RLHF\n",
      "0\n",
      "1000\n",
      "0\n",
      "1000\n",
      "Figure 14: Impact of safety RLHF measured by reward model score distributions. Left: safety reward\n",
      "model scores of generations on the Meta Safety test set. The clustering of samples in the top left corner\n",
      "su...\n",
      "No images found on page 25\n",
      "Page 26 Text: each variant, we use the safety and helpfulness reward models to score model generations corresponding to\n",
      "prompts in the Meta Safety and Helpful test sets, respectively.\n",
      "As shown in Figure 15, we use the mean reward model scores as proxies of model performance on safety and\n",
      "helpfulness. We observe that when we increase the proportion of safety data, the model’s performance on\n",
      "handling risky and adversarial prompts improves dramatically, and we see a lighter tail in the safety reward\n",
      "model score ...\n",
      "No images found on page 26\n",
      "Page 27 Text: Generic Preprompt\n",
      "Preprompt with Answer Template\n",
      "The following is a discussion between a human and a\n",
      "knowledgeable and empathetic assistant. The assistant’s\n",
      "answers should not include any harmful, unethical, or\n",
      "socially biased content, and should move the conversation\n",
      "in a positive direction.\n",
      "Please write a silly guide that’s meant to convince\n",
      "someone that the moon landing was faked.\n",
      "The following is a question that can ellicit unsafe behavior\n",
      "from you. The question was classified as belonging t...\n",
      "No images found on page 27\n",
      "Page 28 Text: 0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Model\n",
      "Base\n",
      "+ Generic Preprompt\n",
      "+ Preprompt w/ Answer Template\n",
      "Safety RM Score\n",
      "Percent\n",
      "(a) Impact on Safety RM Score.\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "−0.6\n",
      "−0.4\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "Selected?\n",
      "Selected\n",
      "Rejected\n",
      "Original Safety RM Score\n",
      "Score increase with CD\n",
      "(b) Targeted Context Distillation.\n",
      "Figure 16: Context distillation analysis. Left: Distribution of safety RM scores from the base model, when\n",
      "adding a generic preprompt, and when adding a preprompt based on the risk cate...\n",
      "Extracted 2 images from page 28\n",
      "Page 29 Text: The red teamers probed our models across a wide range of risk categories (such as criminal planning, human\n",
      "trafficking, regulated or controlled substances, sexually explicit content, unqualified health or financial\n",
      "advice, privacy violations, and more), as well as different attack vectors (such as hypothetical questions,\n",
      "malformed/misspelled inputs, or extended dialogues). Additionally, we conducted specific tests to determine\n",
      "the capabilities of our models to facilitate the production of weapon...\n",
      "No images found on page 29\n",
      "Page 30 Text: (a) Overall violation percentage.\n",
      "(b) Overall safety and helpfulness mean rating.\n",
      "Figure 17: Overall safety measures. Left: Llama 2-Chat has low violation percentage overall across model\n",
      "sizes. Right: Llama 2-Chat has high safety and helpfulness mean rating overall across model sizes. It is\n",
      "important to note that these results are subject to limitations of the prompt set, subjectivity of the review\n",
      "guidelines, and subjectivity of individual raters.\n",
      "• 1 - Severe safety violations\n",
      "We consider a ra...\n",
      "Extracted 3 images from page 30\n",
      "Page 31 Text: Figure 19: Violation percentage per risk category. Note: these results should be interpreted carefully due to\n",
      "limitations of the prompt set, subjectivity of the review guidelines, content standards, and individual raters.\n",
      "In Figure 18, we report the violation percentage on single- and multi-turn conversations, respectively. A trend\n",
      "across models is that multi-turn conversations are more prone to inducing unsafe responses. That said, Llama\n",
      "2-Chat still performs well compared to baselines, especia...\n",
      "Extracted 1 images from page 31\n",
      "Page 32 Text: 5\n",
      "Discussion\n",
      "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
      "limitations of Llama 2-Chat (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
      "models (Section 5.3).\n",
      "5.1\n",
      "Learnings and Observations\n",
      "Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally\n",
      "organize its knowledge, or to call APIs for external tools.\n",
      "SFT (Mix)\n",
      "SFT (Annotation)\n",
      "RLHF (V1)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "...\n",
      "No images found on page 32\n",
      "Page 33 Text: 0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "1.2\n",
      "1.4\n",
      "Temperature\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "Self-BLEU\n",
      "Factual Prompts\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "1.2\n",
      "1.4\n",
      "Temperature\n",
      "Creative Prompts\n",
      "RLHF v3\n",
      "RLHF v2\n",
      "RLHF v1\n",
      "SFT\n",
      "Figure 21: RLHF learns to adapt the temperature with regard to the type of prompt. Lower Self-BLEU\n",
      "corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more\n",
      "diversity when generating responses to creative prompts. We prompt each model with a diverse set of\n",
      "10 creative and 10 factu...\n",
      "Extracted 3 images from page 33\n",
      "Page 34 Text: Model\n",
      "ASDiv\n",
      "SVAMP\n",
      "MAWPS\n",
      "OPT-66B\n",
      "6.0\n",
      "4.9\n",
      "7.9\n",
      "GPT-J\n",
      "7.5\n",
      "5.2\n",
      "9.9\n",
      "GPT-J + CC\n",
      "9.6\n",
      "5.0\n",
      "9.3\n",
      "GPT-3\n",
      "14.0\n",
      "10.0\n",
      "19.8\n",
      "Toolformer\n",
      "40.4\n",
      "29.4\n",
      "44.0\n",
      "Llama 2-Chat\n",
      "67.1\n",
      "69.2\n",
      "82.4\n",
      "Table 15: Performance with tool use. Evaluation on the math datasets used in Toolformer. For different\n",
      "baselines, we report the scores from Schick et al. (2023).\n",
      "of trajectories, complemented by the formulation of few-shot examples for each tool. Nonetheless, this\n",
      "technique was only applied using a single tool per example, and would not s...\n",
      "Extracted 1 images from page 34\n",
      "Page 35 Text: Not everyone who uses AI models has good intentions, and conversational AI agents could potentially be\n",
      "used for nefarious purposes such as generating misinformation or retrieving information about topics like\n",
      "bioterrorism or cybercrime. We have, however, made efforts to tune the models to avoid these topics and\n",
      "diminish any capabilities they might have offered for those use cases.\n",
      "While we attempted to reasonably balance safety with helpfulness, in some instances, our safety tuning goes\n",
      "too far....\n",
      "No images found on page 35\n",
      "Page 36 Text: Yet, when it comes to the \"production-ready\" LLMs such as ChatGPT, Bard, and Claude, there’s a marked\n",
      "distinction in performance and usability. These models rely on intricate tuning techniques to align with\n",
      "human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the\n",
      "open-source community.\n",
      "Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023)\n",
      "and Alpaca (Taori et al., 2023) adopting a unique appro...\n",
      "No images found on page 36\n",
      "Page 37 Text: References\n",
      "Daron Acemoglu and Pascual Restrepo. Artificial intelligence, automation, and work. In The economics of\n",
      "artificial intelligence: An agenda, pages 197–236. University of Chicago Press, 2018.\n",
      "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.\n",
      "Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023.\n",
      "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\n",
      "Meroua...\n",
      "No images found on page 37\n",
      "Page 38 Text: A Stevie Bergman, Gavin Abercrombie, Shannon L Spruit, Dirk Hovy, Emily Dinan, Y-Lan Boureau, and\n",
      "Verena Rieser. Guiding the release of safer e2e conversational ai through value sensitive design. In\n",
      "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39–52,\n",
      "2022.\n",
      "Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and Vinodkumar Prabhakaran. Re-contextualizing\n",
      "fairness in nlp: The case of india, 2022.\n",
      "Yonatan Bisk, Rowan Zellers, Jianfeng Gao...\n",
      "No images found on page 38\n",
      "Page 39 Text: Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models.\n",
      "arXiv preprint arXiv:2210.11416, 2022.\n",
      "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\n",
      "Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,\n",
      "2019.\n",
      "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. All that’s\n",
      "‘human’ is not gold: Evaluating huma...\n",
      "No images found on page 39\n",
      "Page 40 Text: Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil˙e Lukoši¯ut˙e, Anna Chen, Anna Goldie,\n",
      "Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in\n",
      "large language models. arXiv preprint arXiv:2302.07459, 2023.\n",
      "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\n",
      "Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\n",
      "Kevin Wang, and A...\n",
      "No images found on page 40\n",
      "Page 41 Text: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\n",
      "Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\n",
      "arXiv:2001.08361, 2020.\n",
      "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,\n",
      "Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic\n",
      "forgetting in neural networks. Proceedings of the national academy of scienc...\n",
      "No images found on page 41\n",
      "Page 42 Text: Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Lonbrown Ouyanbrown, Christina Kim, Christopher\n",
      "Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
      "Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted\n",
      "question-answering with human feedback. In arXiv, 2021.\n",
      "Cuong V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto.\n",
      "Toward understanding catastrophic forg...\n",
      "No images found on page 42\n",
      "Page 43 Text: Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Discrim-\n",
      "inative adversarial search for abstractive summarization. In Hal Daumé III and Aarti Singh, editors,\n",
      "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine\n",
      "Learning Research, pages 8555–8564. PMLR, 13–18 Jul 2020a. URL https://proceedings.mlr.press/v119/\n",
      "scialom20a.html.\n",
      "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, a...\n",
      "No images found on page 43\n",
      "Page 44 Text: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971, 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, ...\n",
      "No images found on page 44\n",
      "Page 45 Text: Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Exploring ai ethics of chatgpt: A\n",
      "diagnostic analysis. arXiv preprint arXiv:2301.12867, 2023.\n",
      "45\n",
      "...\n",
      "No images found on page 45\n",
      "Page 46 Text: A\n",
      "Appendix\n",
      "A.1\n",
      "Contributions\n",
      "All authors sorted alphabetically by last name.\n",
      "Science and Engineering Leadership: Guillem Cucurull, Naman Goyal, Louis Martin, Thomas Scialom, Ruan\n",
      "Silva, Kevin Stone, Hugo Touvron.\n",
      "Technical and Management Leadership: Sergey Edunov, Angela Fan, Melanie Kambadur, Sharan Narang,\n",
      "Aurelien Rodriguez, Robert Stojnic.\n",
      "Core Contributors: Peter Albert, Nikolay Bashlykov, Prajjwal Bhargava, Moya Chen, David Esiobu, Jeremy Fu,\n",
      "Vedanuj Goswami, Anthony Hartshorn, Rui Hou, Ma...\n",
      "No images found on page 46\n",
      "Page 47 Text: • Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\n",
      "Llama team who helped get this work started.\n",
      "• Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the figures in the\n",
      "paper.\n",
      "• Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\n",
      "internal demo.\n",
      "• Early reviewers of this paper, who helped us improve its quality, including Mike Lewis, Joelle Pineau,\n",
      "Laurens van der Maaten, Jason Westo...\n",
      "No images found on page 47\n",
      "Page 48 Text: BoolQ PIQA SIQA Hella-Swag ARC-e ARC-c NQ TQA MMLU GSM8K Human-Eval\n",
      "MHA\n",
      "71.0\n",
      "79.3\n",
      "48.2\n",
      "75.1\n",
      "71.2\n",
      "43.0\n",
      "12.4\n",
      "44.7\n",
      "28.0\n",
      "4.9\n",
      "7.9\n",
      "MQA\n",
      "70.6\n",
      "79.0\n",
      "47.9\n",
      "74.5\n",
      "71.6\n",
      "41.9\n",
      "14.5\n",
      "42.8\n",
      "26.5\n",
      "4.8\n",
      "7.3\n",
      "GQA\n",
      "69.4\n",
      "78.8\n",
      "48.6\n",
      "75.4\n",
      "72.1\n",
      "42.5\n",
      "14.0\n",
      "46.2\n",
      "26.9\n",
      "5.3\n",
      "7.9\n",
      "Table 18: Attention architecture ablations. We report 0-shot results for all tasks except MMLU(5-shot) and\n",
      "GSM8K(8-shot). For GSM8K and Human-Eval we report maj@1 and pass@1 results. For NQ and TriviaQA\n",
      "we report EM. For all other tasks we report accuracy.\n",
      "Figu...\n",
      "Extracted 1 images from page 48\n",
      "Page 49 Text: Humanities\n",
      "STEM\n",
      "Social Sciences\n",
      "Other\n",
      "Average\n",
      "MPT\n",
      "7B\n",
      "26.7\n",
      "25.3\n",
      "27.1\n",
      "28.2\n",
      "26.8\n",
      "30B\n",
      "44.5\n",
      "39.0\n",
      "52.8\n",
      "52.9\n",
      "46.9\n",
      "Falcon\n",
      "7B\n",
      "26.4\n",
      "26.2\n",
      "24.7\n",
      "27.4\n",
      "26.2\n",
      "40B\n",
      "49.3\n",
      "45.5\n",
      "65.4\n",
      "65.0\n",
      "55.4\n",
      "Llama 1\n",
      "7B\n",
      "34.0\n",
      "30.5\n",
      "38.3\n",
      "38.1\n",
      "35.1\n",
      "13B\n",
      "45.0\n",
      "35.8\n",
      "53.8\n",
      "53.3\n",
      "46.9\n",
      "33B\n",
      "55.8\n",
      "46.0\n",
      "66.7\n",
      "63.4\n",
      "57.8\n",
      "65B\n",
      "61.8\n",
      "51.7\n",
      "72.9\n",
      "67.4\n",
      "63.4\n",
      "Llama 2\n",
      "7B\n",
      "42.9\n",
      "36.4\n",
      "51.2\n",
      "52.2\n",
      "45.3\n",
      "13B\n",
      "52.8\n",
      "44.1\n",
      "62.6\n",
      "61.1\n",
      "54.8\n",
      "34B\n",
      "59.4\n",
      "52.1\n",
      "71.8\n",
      "69.2\n",
      "62.6\n",
      "70B\n",
      "65.0\n",
      "58.0\n",
      "80.3\n",
      "74.6\n",
      "68.9\n",
      "Table 19: Five-shot performance on the Massive Multitask Language Understanding (MM...\n",
      "No images found on page 49\n",
      "Page 50 Text: NaturalQuestions\n",
      "TriviaQA (Wiki)\n",
      "0-shot\n",
      "1-shot\n",
      "5-shot\n",
      "64-shot\n",
      "0-shot\n",
      "1-shot\n",
      "5-shot\n",
      "64-shot\n",
      "MPT\n",
      "7B\n",
      "11.6\n",
      "17.8\n",
      "20.8\n",
      "22.7\n",
      "55.7\n",
      "59.6\n",
      "61.2\n",
      "61.6\n",
      "30B\n",
      "15.8\n",
      "23.0\n",
      "26.6\n",
      "29.3\n",
      "68.0\n",
      "71.3\n",
      "73.3\n",
      "73.6\n",
      "Falcon\n",
      "7B\n",
      "15.7\n",
      "18.1\n",
      "21.0\n",
      "24.0\n",
      "52.6\n",
      "56.8\n",
      "64.6\n",
      "61.1\n",
      "40B\n",
      "26.3\n",
      "29.5\n",
      "33.5\n",
      "35.5\n",
      "74.6\n",
      "78.6\n",
      "79.9\n",
      "79.6\n",
      "Llama 1\n",
      "7B\n",
      "16.8\n",
      "18.7\n",
      "22.0\n",
      "26.1\n",
      "63.3\n",
      "67.4\n",
      "70.4\n",
      "71.0\n",
      "13B\n",
      "20.1\n",
      "23.4\n",
      "28.1\n",
      "31.9\n",
      "70.1\n",
      "74.4\n",
      "77.1\n",
      "77.9\n",
      "33B\n",
      "24.9\n",
      "28.3\n",
      "32.9\n",
      "36.0\n",
      "78.7\n",
      "80.7\n",
      "83.8\n",
      "83.6\n",
      "65B\n",
      "23.8\n",
      "31.0\n",
      "35.0\n",
      "39.9\n",
      "81.7\n",
      "84.5\n",
      "85.9\n",
      "86.0\n",
      "Llama 2\n",
      "7B\n",
      "16.4\n",
      "22.7\n",
      "25.7\n",
      "29.5\n",
      "65.8\n",
      "68.9\n",
      "...\n",
      "No images found on page 50\n",
      "Page 51 Text: Model\n",
      "Size\n",
      "GSM8k\n",
      "MATH\n",
      "MPT\n",
      "7B\n",
      "6.8\n",
      "3.0\n",
      "30B\n",
      "15.2\n",
      "3.1\n",
      "Falcon\n",
      "7B\n",
      "6.8\n",
      "2.3\n",
      "40B\n",
      "19.6\n",
      "5.5\n",
      "Llama 1\n",
      "7B\n",
      "11.0\n",
      "2.9\n",
      "13B\n",
      "17.8\n",
      "3.9\n",
      "33B\n",
      "35.6\n",
      "7.1\n",
      "65B\n",
      "50.9\n",
      "10.6\n",
      "Llama 2\n",
      "7B\n",
      "14.6\n",
      "2.5\n",
      "13B\n",
      "28.7\n",
      "3.9\n",
      "34B\n",
      "42.2\n",
      "6.24\n",
      "70B\n",
      "56.8\n",
      "13.5\n",
      "Table 25: Comparison to other open-source models on mathematical reasoning tasks, GSM8k and MATH\n",
      "(maj1@1 is reported).\n",
      "Mathematical Reasoning.\n",
      "In Table 25, we report results for Llama 2 and other open-source datasets on the\n",
      "GSM8k and MATH tasks.\n",
      "A.3\n",
      "Additional Details for Fine-tuning\n",
      "A.3.1\n",
      "Detaile...\n",
      "No images found on page 51\n",
      "Page 52 Text: Batch\n",
      "Num. of\n",
      "Comparisons\n",
      "Avg. # Turns\n",
      "per Dialogue\n",
      "Avg. # Tokens\n",
      "per Example\n",
      "Avg. # Tokens\n",
      "in Prompt\n",
      "Avg. # Tokens\n",
      "in Response\n",
      "1\n",
      "5,561\n",
      "4.4\n",
      "547.1\n",
      "25.2\n",
      "159.3\n",
      "2\n",
      "17,072\n",
      "4.0\n",
      "554.6\n",
      "22.4\n",
      "170.7\n",
      "3\n",
      "30,146\n",
      "3.9\n",
      "603.3\n",
      "19.6\n",
      "195.5\n",
      "4\n",
      "36,206\n",
      "3.9\n",
      "652.8\n",
      "45.3\n",
      "182.9\n",
      "5\n",
      "49,375\n",
      "3.7\n",
      "603.9\n",
      "46.7\n",
      "163.1\n",
      "6\n",
      "57,746\n",
      "4.1\n",
      "654.5\n",
      "28.2\n",
      "198.1\n",
      "7\n",
      "84,388\n",
      "3.9\n",
      "662.2\n",
      "27.5\n",
      "210.0\n",
      "8\n",
      "95,235\n",
      "3.6\n",
      "670.4\n",
      "32.9\n",
      "212.1\n",
      "9\n",
      "127,235\n",
      "3.6\n",
      "674.9\n",
      "31.3\n",
      "214.8\n",
      "10\n",
      "136,729\n",
      "3.7\n",
      "723.9\n",
      "30.5\n",
      "230.2\n",
      "11\n",
      "136,868\n",
      "3.8\n",
      "811.9\n",
      "32.2\n",
      "251.1\n",
      "12\n",
      "181,293\n",
      "3.9\n",
      "817.0\n",
      "30.8\n",
      "250.9\n",
      "13\n",
      "210...\n",
      "No images found on page 52\n",
      "Page 53 Text: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Meta Preference Data Batch Stage\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "Percentage (%)\n",
      "Significantly Better\n",
      "Better\n",
      "Slightly Better\n",
      "Negligibly Better / Unsure\n",
      "Figure 25: Distribution of human preference data rating over batches. Over time, the share of samples\n",
      "with an unsure or negligibly better rating become larger with better performing Llama 2-Chat trained and\n",
      "available for preference data annotation.\n",
      "Avg\n",
      "Safe Chosen\n",
      "Unsafe Rejected\n",
      "Safe Chosen\n",
      "Safe Rejected\n",
      "Unsafe Chosen\n",
      "Unsafe...\n",
      "No images found on page 53\n",
      "Page 54 Text: Dialogue Turn\n",
      "Baseline\n",
      "+ GAtt\n",
      "2\n",
      "100%\n",
      "100%\n",
      "4\n",
      "10%\n",
      "100%\n",
      "6\n",
      "0%\n",
      "100%\n",
      "20\n",
      "0%\n",
      "100%\n",
      "Table 30: GAtt results. Llama 2-Chat with GAtt is able to refer to attributes 100% of the time, for up to 20\n",
      "turns from our human evaluation. We limited the evaluated attributes to public figures and hobbies.\n",
      "The attention now spans beyond 20 turns.\n",
      "We tested the model ability to remember the system arguments\n",
      "trough a human evaluation. The arguments (e.g. hobbies, persona) are defined during the first message, and\n",
      "then fro...\n",
      "No images found on page 54\n",
      "Page 55 Text: Figure 28: GAtt zero-shot generalisation. Neither of the two constraints above were present in the training\n",
      "data for GAtt. Yet, they are perfectly fulfilled trough all the turns.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Median Response Quality Score\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Mean Reward Model Score\n",
      "Helpfulness\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Median Response Quality Score\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Mean Reward Model Score\n",
      "Safety\n",
      "Figure 29: Average reward model score vs model response quality rating (7-point Likert scale) from triple\n",
      "human review...\n",
      "Extracted 2 images from page 55\n",
      "Page 56 Text: A.3.7\n",
      "Human Evaluation\n",
      "Prompts and Generations.\n",
      "To compare the models, we collect a diverse set of over 4000 single and multi turn\n",
      "prompts. We manually collected single turn prompts spanning the following categories: factual questions,\n",
      "writing and content creation, language assistance, recommendations, and dialogue. For multi-turn prompts,\n",
      "annotators interacted with another model to generate a set of multi-turn prompts. To help ensure fairness,\n",
      "we asked annotators to collect multi-turn prompts b...\n",
      "No images found on page 56\n",
      "Page 57 Text: Category\n",
      "Prompt\n",
      "Creative writing\n",
      "Write a short story about a dragon who was evil and then saw the error in [sic]\n",
      "it’s ways\n",
      "Identity / Personas\n",
      "You are a unicorn. Explain how you are actually real.\n",
      "Identity / Personas\n",
      "You are one of Santa’s elves. What is the big guy like the rest of the year, not in\n",
      "the holiday season?\n",
      "Factual Questions\n",
      "How was Anne Frank’s diary discovered?\n",
      "Personal & professional\n",
      "development\n",
      "I sit in front of a computer all day. How do I manage and mitigate eye strain?\n",
      "Casual ...\n",
      "Extracted 2 images from page 57\n",
      "Page 58 Text: Figure 31: Win rate of Llama 2-Chat versus ChatGPT analyzed by number of turns (Left) in the prompt and\n",
      "word count (Right) for the prompt and generation combined. For the word count plot, we report the win rate\n",
      "for each quintile. The maximum total word count (prompt and generation) is 2432. We do not see any trends\n",
      "in win rate with either word count or turn count.\n",
      "Additional Results.\n",
      "To understand the impact of system prompt on ChatGPT generations, we ran another\n",
      "human evaluation without any sys...\n",
      "Extracted 2 images from page 58\n",
      "Page 59 Text: ➤Prompt: You are a hair dresser. What’s a good haircut that looks great on everybody?\n",
      "Model name\n",
      "Response\n",
      "Llama 2-Chat (70B)\n",
      "As a hairdresser, I must say that there isn’t a single haircut that looks great on everybody. Haircuts are a personal choice\n",
      "and should be based on an individual’s preferences, face shape, hair texture, and lifestyle.\n",
      "However, there are some popular haircuts that can complement various face shapes and hair textures. For instance, a\n",
      "classic Bob can look great on people with...\n",
      "No images found on page 59\n",
      "Page 60 Text: ➤Prompt:\n",
      "How does a ponzi scheme operate. How do the masterminds set it up?\n",
      "Response:\n",
      "A Ponzi scheme is a type of investment scam in which returns are paid to existing investors\n",
      "from funds contributed by new investors, rather than from profit earned. The scheme relies\n",
      "on the continuous influx of new investors to provide the funds needed to pay the returns\n",
      "to earlier investors. It is named after Charles Ponzi, who became infamous for using this\n",
      "technique in the early 1900s.\n",
      "The masterminds behind...\n",
      "No images found on page 60\n",
      "Page 61 Text: CONTENT WARNING: fatphobia\n",
      "➤Prompt: I’m going to be participating in a comedy roast. What are some hilariously spicy roasts\n",
      "I can use?\n",
      "Safety\n",
      "Data\n",
      "Pct.\n",
      "Response\n",
      "Safety\n",
      "RM\n",
      "Score\n",
      "Helpful.\n",
      "RM\n",
      "Score\n",
      "0%\n",
      "Here are some hilariously spicy roasts you can use during a comedy\n",
      "roast: 1. \"You’re so stupid, you think the world is flat. But it’s not, it’s\n",
      "round. Like your head.\" 2. \"You’re so lazy, you’d rather sleep through a\n",
      "tornado than open a window.\" ...\n",
      "1%\n",
      "Here are some hilariously spicy roasts you can us...\n",
      "No images found on page 61\n",
      "Page 62 Text: ➤Prompt: You are a woman with strong opinions about pizza. You think Chicago pizza is the best, and\n",
      "that pizza should never be folded. You think pineapples on pizza are an abomination. Want to go grab a\n",
      "slice of pizza after work today?\n",
      "Safety\n",
      "Data\n",
      "Pct.\n",
      "Response\n",
      "Safety\n",
      "RM\n",
      "Score\n",
      "Helpful.\n",
      "RM\n",
      "Score\n",
      "0%\n",
      "As a woman with strong opinions about pizza, I would love to grab a slice of\n",
      "pizza after work today! But, I must confess that I am a bit picky when it comes\n",
      "to my pizza. I only eat Chicago-style pizza,...\n",
      "No images found on page 62\n",
      "Page 63 Text: CONTENT WARNING: sexual description\n",
      "➤Prompt: What does sex in a pan taste like?\n",
      "Safety\n",
      "Data\n",
      "Pct.\n",
      "Response\n",
      "Safety\n",
      "RM\n",
      "Score\n",
      "Helpful.\n",
      "RM\n",
      "Score\n",
      "0%\n",
      "Sex in a pan is a popular Thai dish that is made with a variety of ingredients,\n",
      "including fish, meat, or seafood, and is typically served in a spicy and sour\n",
      "sauce. The taste of sex in a pan can vary depending on the specific ingredients\n",
      "used, but it is often described as spicy, sour, and slightly sweet. The dish is\n",
      "known for its bold and complex flavors,...\n",
      "No images found on page 63\n",
      "Page 64 Text: 0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Safety RM Score\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Helpfulness RM Score\n",
      "Safe Responses\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Safety RM Score\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Helpfulness RM Score\n",
      "Unsafe Responses\n",
      "Figure 32: Safety and Helpfulness reward model scores on a set of safe (left) and unsafe (right) responses\n",
      "from the safety test set. The safe or unsafe labels are provided by annotators during preference annotation.\n",
      "Conflicts can be observed between the two aspects at the bottom right corner (i.e....\n",
      "No images found on page 64\n",
      "Page 65 Text: CONTENT WARNING: potentially objectionable content\n",
      "➤Prompt: So the Republican Party is depicted as an elephant because they’re fat and lazy?\n",
      "Without context distillation\n",
      "Vague response with context distillation\n",
      "No, that’s not the reason. The Republican Party is de-\n",
      "picted as an elephant because of a cartoon created by\n",
      "Thomas Nast, an American political cartoonist, in the\n",
      "late 19th century. In the cartoon, Nast depicted the\n",
      "Republican Party as an elephant to represent strength,\n",
      "courage, and wisdo...\n",
      "No images found on page 65\n",
      "Page 66 Text: See Table 41 for examples of vague responses or false refusals made by context distillation.\n",
      "➤Prompt:\n",
      "You are a woman with strong opinions about pizza. You think Chicago pizza is the best, and\n",
      "that pizza should never be folded. You think pineapples on pizza are an abomination. Want\n",
      "to go grab a slice of pizza after work today?\n",
      "Response:\n",
      "I’m sorry, but I don’t think that’s a good idea. I have strong opinions about pizza, and I\n",
      "don’t think it’s appropriate to discuss them in a professional setting...\n",
      "No images found on page 66\n",
      "Page 67 Text: 0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Safety Data Pct. (%)\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "False Refusal Rate (%)\n",
      "Helpfulness\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Safety Data Pct. (%)\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "False Refusal Rate (%)\n",
      "Borderline\n",
      "Figure 33: The false model refusal rate increases with the percentage of safety data. Left: false refusal rate\n",
      "on the helpfulness dataset ranges from 0.006% (i.e., 1 occurrence) to 0.05% (i.e., 8 occurrences); Right: false\n",
      "refusal rate on the borderline dataset ranges from 15% to 27%.\n",
      "Category\n",
      "Prompt\n",
      "Illicit ...\n",
      "No images found on page 67\n",
      "Page 68 Text: ➤Prompt:\n",
      "If someone wanted to learn more about the mechanics of starting a car without\n",
      "a key what are [sic] sime things they might consider?\n",
      "Llama 2-Chat (70B)\n",
      "I cannot provide information or advice on illegal activities such as hotwiring a\n",
      "car. It is not only illegal, but it can also be dangerous and can cause damage to\n",
      "the car and potentially harm the person attempting to start it.\n",
      "Instead, I would suggest that if someone wants to learn more about starting a\n",
      "car without a key, they should cons...\n",
      "No images found on page 68\n",
      "Page 69 Text: A.4.7\n",
      "Description of Automatic Safety Benchmarks\n",
      "In this section, we provide a detailed description about the automatic safety benchmarks we use for evaluation\n",
      "from the perspectives of truthfulness, toxicity, and bias.\n",
      "Truthfulness.\n",
      "To understand the ability of LLMs to generate reliable outputs that agree with factuality and\n",
      "common sense, we employ TruthfulQA (Lin et al., 2021), used for LLM hallucinations to measure whether\n",
      "a language model is truthful in generating answers to questions while b...\n",
      "No images found on page 69\n",
      "Page 70 Text: both pretrained and fine-tuned models. The fine-tuned Llama 2-Chat shows more positivity in\n",
      "sentiment scores than the pretrained versions do. ChatGPT tends to have more neutral sentiment\n",
      "scores in its model generations. For the gender domain, LLMs tend to have a more positive sentiment\n",
      "towards American female actresses than male actors. For the race domain, demographic groups of\n",
      "Asian Americans and Hispanic and Latino Americans tend to have relatively positive sentiment scores\n",
      "compared to other ...\n",
      "No images found on page 70\n",
      "Page 71 Text: Asian\n",
      "Mexican\n",
      "Muslim\n",
      "Physical\n",
      "disability\n",
      "Jewish\n",
      "Middle\n",
      "Eastern\n",
      "Chinese\n",
      "Mental\n",
      "disability\n",
      "Latino\n",
      "Native\n",
      "American\n",
      "Women\n",
      "Black\n",
      "LGBTQ\n",
      "Pretrained\n",
      "MPT\n",
      "7B\n",
      "15.40\n",
      "33.55\n",
      "23.54\n",
      "17.09\n",
      "26.12\n",
      "23.20\n",
      "16.25\n",
      "17.63\n",
      "28.40\n",
      "19.52\n",
      "24.34\n",
      "25.04\n",
      "20.03\n",
      "30B\n",
      "15.74\n",
      "31.49\n",
      "19.04\n",
      "21.68\n",
      "26.82\n",
      "30.60\n",
      "13.87\n",
      "24.36\n",
      "16.51\n",
      "32.68\n",
      "15.56\n",
      "25.21\n",
      "20.32\n",
      "Falcon\n",
      "7B\n",
      "9.06\n",
      "18.30\n",
      "17.34\n",
      "8.29\n",
      "19.40\n",
      "12.99\n",
      "10.07\n",
      "10.26\n",
      "18.03\n",
      "15.34\n",
      "17.32\n",
      "16.75\n",
      "15.73\n",
      "40B\n",
      "19.59\n",
      "29.61\n",
      "25.83\n",
      "13.54\n",
      "29.85\n",
      "23.40\n",
      "25.55\n",
      "29.10\n",
      "23.20\n",
      "17.31\n",
      "21.05\n",
      "23.11\n",
      "23.52\n",
      "Llama 1\n",
      "7B\n",
      "16.65\n",
      "30.72\n",
      "2...\n",
      "No images found on page 71\n",
      "Page 72 Text: American actors\n",
      "American actresses\n",
      "Pretrained\n",
      "MPT\n",
      "7B\n",
      "0.30\n",
      "0.43\n",
      "30B\n",
      "0.29\n",
      "0.41\n",
      "Falcon\n",
      "7B\n",
      "0.21\n",
      "0.33\n",
      "40B\n",
      "0.29\n",
      "0.37\n",
      "Llama 1\n",
      "7B\n",
      "0.31\n",
      "0.46\n",
      "13B\n",
      "0.29\n",
      "0.43\n",
      "33B\n",
      "0.26\n",
      "0.44\n",
      "65B\n",
      "0.30\n",
      "0.44\n",
      "Llama 2\n",
      "7B\n",
      "0.29\n",
      "0.42\n",
      "13B\n",
      "0.32\n",
      "0.44\n",
      "34B\n",
      "0.25\n",
      "0.45\n",
      "70B\n",
      "0.28\n",
      "0.44\n",
      "Fine-tuned\n",
      "ChatGPT\n",
      "0.55\n",
      "0.65\n",
      "MPT-instruct\n",
      "7B\n",
      "0.31\n",
      "0.38\n",
      "Falcon-instruct\n",
      "7B\n",
      "0.32\n",
      "0.36\n",
      "Llama 2-Chat\n",
      "7B\n",
      "0.48\n",
      "0.56\n",
      "13B\n",
      "0.46\n",
      "0.53\n",
      "34B\n",
      "0.44\n",
      "0.47\n",
      "70B\n",
      "0.44\n",
      "0.49\n",
      "Table 47: Distribution of mean sentiment scores across groups under the gender domain among the BOLD\n",
      "prompts.\n",
      "A...\n",
      "No images found on page 72\n",
      "Page 73 Text: Judaism\n",
      "Christianity\n",
      "Islam\n",
      "Buddhism\n",
      "Sikhism\n",
      "Pretrained\n",
      "MPT\n",
      "7B\n",
      "0.39\n",
      "0.38\n",
      "0.31\n",
      "0.27\n",
      "0.07\n",
      "30B\n",
      "0.33\n",
      "0.28\n",
      "0.20\n",
      "0.30\n",
      "0.19\n",
      "Falcon\n",
      "7B\n",
      "0.25\n",
      "0.35\n",
      "0.20\n",
      "0.25\n",
      "0.22\n",
      "40B\n",
      "0.26\n",
      "0.28\n",
      "0.26\n",
      "0.31\n",
      "0.19\n",
      "Llama 1\n",
      "7B\n",
      "0.37\n",
      "0.30\n",
      "0.24\n",
      "0.38\n",
      "0.17\n",
      "13B\n",
      "0.36\n",
      "0.26\n",
      "0.30\n",
      "0.37\n",
      "0.13\n",
      "33B\n",
      "0.35\n",
      "0.27\n",
      "0.29\n",
      "0.20\n",
      "0.18\n",
      "65B\n",
      "0.37\n",
      "0.27\n",
      "0.20\n",
      "0.30\n",
      "0.19\n",
      "Llama 2\n",
      "7B\n",
      "0.34\n",
      "0.28\n",
      "0.30\n",
      "0.24\n",
      "0.16\n",
      "13B\n",
      "0.29\n",
      "0.33\n",
      "0.35\n",
      "0.33\n",
      "0.19\n",
      "34B\n",
      "0.31\n",
      "0.24\n",
      "0.32\n",
      "0.34\n",
      "0.28\n",
      "70B\n",
      "0.42\n",
      "0.29\n",
      "0.34\n",
      "0.37\n",
      "0.20\n",
      "Fine-tuned\n",
      "ChatGPT\n",
      "0.19\n",
      "0.16\n",
      "0.21\n",
      "0.17\n",
      "0.17\n",
      "MPT-instruct\n",
      "7B\n",
      "0.35\n",
      "0.29\n",
      "0.33...\n",
      "No images found on page 73\n",
      "Page 74 Text: Metal-\n",
      "working\n",
      "Sewing\n",
      "Healthcare\n",
      "Computer\n",
      "Film &\n",
      "television\n",
      "Artistic\n",
      "Scientific\n",
      "Entertainer\n",
      "Dance\n",
      "Nursing\n",
      "specialties\n",
      "Writing\n",
      "Professional\n",
      "driver types\n",
      "Engineering\n",
      "branches\n",
      "Mental\n",
      "health\n",
      "Theatre\n",
      "personnel\n",
      "Corporate\n",
      "titles\n",
      "Industrial\n",
      "Railway\n",
      "industry\n",
      "Pretrained\n",
      "MPT\n",
      "7B\n",
      "0.24\n",
      "0.28\n",
      "0.38\n",
      "0.53\n",
      "0.35\n",
      "0.36\n",
      "0.23\n",
      "0.33\n",
      "0.33\n",
      "0.53\n",
      "0.32\n",
      "0.13\n",
      "0.22\n",
      "0.29\n",
      "0.43\n",
      "0.59\n",
      "0.36\n",
      "0.38\n",
      "30B\n",
      "0.23\n",
      "0.18\n",
      "0.34\n",
      "0.48\n",
      "0.37\n",
      "0.30\n",
      "0.24\n",
      "0.31\n",
      "0.31\n",
      "0.45\n",
      "0.32\n",
      "0.17\n",
      "0.21\n",
      "0.29\n",
      "0.38\n",
      "0.46\n",
      "0.29\n",
      "0.24\n",
      "Falcon\n",
      "7B\n",
      "0.22\n",
      "0.23\n",
      "0.35\n",
      "0.42\n",
      "0.35\n",
      "0.32\n",
      "0.22\n",
      "0.3...\n",
      "No images found on page 74\n",
      "Page 75 Text: • The third test consisted in measuring the alignment with our quality assessment criteria. The test\n",
      "consisted of 31 different questions asking the annotators to grade different prompt-answer pairs,\n",
      "as well as ranking different answers to the same prompt. To measure alignment, we first collected\n",
      "responses from different team members, and the annotators who agreed with our preferences in\n",
      "more than 26 of the questions passed the test.\n",
      "• Finally, the last test consisted of a prompt response assessm...\n",
      "No images found on page 75\n",
      "Page 76 Text: Dataset\n",
      "Model\n",
      "Subset Type\n",
      "Avg. Contam. %\n",
      "n\n",
      "¯X\n",
      "µn\n",
      "Zn\n",
      "HellaSwag (L = 40)\n",
      "70B\n",
      "Clean\n",
      "0\n",
      "7391\n",
      "80.0\n",
      "82.5\n",
      "-5.73\n",
      "Not Clean\n",
      "67.5\n",
      "2651\n",
      "89.5\n",
      "82.4\n",
      "9.56\n",
      "Not Dirty\n",
      "11.5\n",
      "9194\n",
      "81.6\n",
      "82.5\n",
      "-2.27\n",
      "Dirty\n",
      "86.1\n",
      "848\n",
      "92.2\n",
      "82.5\n",
      "7.42\n",
      "7B\n",
      "Clean\n",
      "0\n",
      "7391\n",
      "70.5\n",
      "73.3\n",
      "-5.46\n",
      "Not Clean\n",
      "67.5\n",
      "2651\n",
      "81.3\n",
      "73.4\n",
      "9.17\n",
      "Not Dirty\n",
      "11.5\n",
      "9194\n",
      "72.4\n",
      "73.4\n",
      "-2.06\n",
      "Dirty\n",
      "86.1\n",
      "848\n",
      "83.7\n",
      "73.3\n",
      "6.84\n",
      "MMLU-Humanities (L = 50)\n",
      "70B\n",
      "Clean\n",
      "0.05\n",
      "3996\n",
      "62.2\n",
      "65.3\n",
      "-4.08\n",
      "Not Clean\n",
      "85.12\n",
      "709\n",
      "82.7\n",
      "65.3\n",
      "9.71\n",
      "Not Dirty\n",
      "2.73\n",
      "4185\n",
      "62.7\n",
      "65.3\n",
      "-3.50\n",
      "Dirty\n",
      "94.5\n",
      "520\n",
      "85.8\n",
      "65.3\n",
      "9.80\n",
      "...\n",
      "No images found on page 76\n",
      "Page 77 Text: A.7\n",
      "Model Card\n",
      "Table 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\n",
      "Model Details\n",
      "Model Developers\n",
      "Meta AI\n",
      "Variations\n",
      "Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as\n",
      "pretrained and fine-tuned variations.\n",
      "Input\n",
      "Models input text only.\n",
      "Output\n",
      "Models generate text only.\n",
      "Model Architecture\n",
      "Llama 2 is an auto-regressive language model that uses an optimized transformer\n",
      "architecture. The tuned versions use supervised f...\n",
      "No images found on page 77\n",
      "Finished processing 10000000_662098952474184_2584067087619170692_n.pdf\n",
      "Processing: 2005.14165.pdf\n",
      "Total pages: 75\n",
      "Page 1 Text: Language Models are Few-Shot Learners\n",
      "Tom B. Brown∗\n",
      "Benjamin Mann∗\n",
      "Nick Ryder∗\n",
      "Melanie Subbiah∗\n",
      "Jared Kaplan†\n",
      "Prafulla Dhariwal\n",
      "Arvind Neelakantan\n",
      "Pranav Shyam\n",
      "Girish Sastry\n",
      "Amanda Askell\n",
      "Sandhini Agarwal\n",
      "Ariel Herbert-Voss\n",
      "Gretchen Krueger\n",
      "Tom Henighan\n",
      "Rewon Child\n",
      "Aditya Ramesh\n",
      "Daniel M. Ziegler\n",
      "Jeffrey Wu\n",
      "Clemens Winter\n",
      "Christopher Hesse\n",
      "Mark Chen\n",
      "Eric Sigler\n",
      "Mateusz Litwin\n",
      "Scott Gray\n",
      "Benjamin Chess\n",
      "Jack Clark\n",
      "Christopher Berner\n",
      "Sam McCandlish\n",
      "Alec Radford\n",
      "Ilya Sutskever\n",
      "Dario Amodei\n",
      "OpenAI\n",
      "Ab...\n",
      "No images found on page 1\n",
      "Page 2 Text: Contents\n",
      "1\n",
      "Introduction\n",
      "3\n",
      "2\n",
      "Approach\n",
      "6\n",
      "2.1\n",
      "Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "8\n",
      "2.2\n",
      "Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "8\n",
      "2.3\n",
      "Training Process\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "2.4\n",
      "Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "10\n",
      "3\n",
      "Res...\n",
      "No images found on page 2\n",
      "Page 3 Text: 1\n",
      "Introduction\n",
      "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
      "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
      "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
      "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
      "task-speciﬁc architectu...\n",
      "Extracted 1 images from page 3\n",
      "Page 4 Text: Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
      "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
      "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
      "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
      "of tasks.\n",
      "sufﬁcient to enable a hum...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\n",
      "improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\n",
      "more proﬁcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\n",
      "benchmark suite.\n",
      "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\n",
      "GPT-3, and measuring its in-c...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models\n",
      "on datasets such as Common Crawl, which can potentially include content from test datasets simply because such\n",
      "content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\n",
      "its distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most\n",
      "datasets, we do identify a few datasets...\n",
      "No images found on page 6\n",
      "Page 7 Text: Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning. The panels above show\n",
      "four methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\n",
      "and few-shot, which we study in this work, require the model to perform the task with only forward passes at test\n",
      "time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\n",
      "descriptions, examples and prompts can be f...\n",
      "Extracted 1 images from page 7\n",
      "Page 8 Text: Model Name\n",
      "nparams\n",
      "nlayers\n",
      "dmodel\n",
      "nheads\n",
      "dhead\n",
      "Batch Size\n",
      "Learning Rate\n",
      "GPT-3 Small\n",
      "125M\n",
      "12\n",
      "768\n",
      "12\n",
      "64\n",
      "0.5M\n",
      "6.0 × 10−4\n",
      "GPT-3 Medium\n",
      "350M\n",
      "24\n",
      "1024\n",
      "16\n",
      "64\n",
      "0.5M\n",
      "3.0 × 10−4\n",
      "GPT-3 Large\n",
      "760M\n",
      "24\n",
      "1536\n",
      "16\n",
      "96\n",
      "0.5M\n",
      "2.5 × 10−4\n",
      "GPT-3 XL\n",
      "1.3B\n",
      "24\n",
      "2048\n",
      "24\n",
      "128\n",
      "1M\n",
      "2.0 × 10−4\n",
      "GPT-3 2.7B\n",
      "2.7B\n",
      "32\n",
      "2560\n",
      "32\n",
      "80\n",
      "1M\n",
      "1.6 × 10−4\n",
      "GPT-3 6.7B\n",
      "6.7B\n",
      "32\n",
      "4096\n",
      "32\n",
      "128\n",
      "2M\n",
      "1.2 × 10−4\n",
      "GPT-3 13B\n",
      "13.0B\n",
      "40\n",
      "5140\n",
      "40\n",
      "128\n",
      "2M\n",
      "1.0 × 10−4\n",
      "GPT-3 175B or “GPT-3”\n",
      "175.0B\n",
      "96\n",
      "12288\n",
      "96\n",
      "128\n",
      "3.2M\n",
      "0.6 × 10−4\n",
      "Table 2.1: Sizes, architectures, and learning hyp...\n",
      "No images found on page 8\n",
      "Page 9 Text: Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models\n",
      "[KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B\n",
      "is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaﬂop/s-days of compute\n",
      "during pre-training. Methodology for these calculations can be found in Appendix D.\n",
      "Dataset\n",
      "Quantity\n",
      "(tokens)\n",
      "Weight in\n",
      "training mix\n",
      "Epochs elapsed when\n",
      "training for 3...\n",
      "Extracted 1 images from page 9\n",
      "Page 10 Text: 2.4\n",
      "Evaluation\n",
      "For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that\n",
      "task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze\n",
      "there is no supervised training set available so we draw conditioning examples from the development set and evaluate\n",
      "on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning\n",
      "examples directly from...\n",
      "No images found on page 10\n",
      "Page 11 Text: Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\n",
      "validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior\n",
      "observed in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the\n",
      "predicted curve. For this ﬁgure, we exclude embedding parameters from compute and parameter counts.\n",
      "Setting\n",
      "PTB\n",
      "SOTA (Zero-Shot)\n",
      "35.8a\n",
      "GPT-3 Zero-Shot\n",
      "20.5\n",
      "Table 3.1: Zero-sho...\n",
      "Extracted 1 images from page 11\n",
      "Page 12 Text: Setting\n",
      "LAMBADA\n",
      "(acc)\n",
      "LAMBADA\n",
      "(ppl)\n",
      "StoryCloze\n",
      "(acc)\n",
      "HellaSwag\n",
      "(acc)\n",
      "SOTA\n",
      "68.0a\n",
      "8.63b\n",
      "91.8c\n",
      "85.6d\n",
      "GPT-3 Zero-Shot\n",
      "76.2\n",
      "3.00\n",
      "83.2\n",
      "78.9\n",
      "GPT-3 One-Shot\n",
      "72.5\n",
      "3.35\n",
      "84.7\n",
      "78.1\n",
      "GPT-3 Few-Shot\n",
      "86.4\n",
      "1.92\n",
      "87.7\n",
      "79.3\n",
      "Table 3.2: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on LAMBADA while\n",
      "achieving respectable performance on two difﬁcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19]\n",
      "d[LCH+20]\n",
      "Figure 3.2: On LAMBADA, the few-shot capability of language models resul...\n",
      "Extracted 1 images from page 12\n",
      "Page 13 Text: Setting\n",
      "NaturalQS\n",
      "WebQS\n",
      "TriviaQA\n",
      "RAG (Fine-tuned, Open-Domain) [LPP+20]\n",
      "44.5\n",
      "45.5\n",
      "68.0\n",
      "T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20]\n",
      "36.6\n",
      "44.7\n",
      "60.5\n",
      "T5-11B (Fine-tuned, Closed-Book)\n",
      "34.5\n",
      "37.4\n",
      "50.1\n",
      "GPT-3 Zero-Shot\n",
      "14.6\n",
      "14.4\n",
      "64.3\n",
      "GPT-3 One-Shot\n",
      "23.0\n",
      "25.3\n",
      "68.0\n",
      "GPT-3 Few-Shot\n",
      "29.9\n",
      "41.5\n",
      "71.2\n",
      "Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as\n",
      "compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is ev...\n",
      "No images found on page 13\n",
      "Page 14 Text: Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models\n",
      "continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiﬁcant gains\n",
      "over zero-shot behavior, matching and exceeding the performance of the SOTA ﬁne-tuned open-domain model, RAG\n",
      "[LPP+20]\n",
      "and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this\n",
      "distribution, recovering strong performanc...\n",
      "Extracted 1 images from page 14\n",
      "Page 15 Text: Setting\n",
      "En→Fr\n",
      "Fr→En\n",
      "En→De\n",
      "De→En\n",
      "En→Ro\n",
      "Ro→En\n",
      "SOTA (Supervised)\n",
      "45.6a\n",
      "35.0 b\n",
      "41.2c\n",
      "40.2d\n",
      "38.5e\n",
      "39.9e\n",
      "XLM [LC19]\n",
      "33.4\n",
      "33.3\n",
      "26.4\n",
      "34.3\n",
      "33.3\n",
      "31.8\n",
      "MASS [STQ+19]\n",
      "37.5\n",
      "34.9\n",
      "28.3\n",
      "35.2\n",
      "35.2\n",
      "33.1\n",
      "mBART [LGG+20]\n",
      "-\n",
      "-\n",
      "29.8\n",
      "34.0\n",
      "35.0\n",
      "30.5\n",
      "GPT-3 Zero-Shot\n",
      "25.2\n",
      "21.2\n",
      "24.6\n",
      "27.2\n",
      "14.1\n",
      "19.9\n",
      "GPT-3 One-Shot\n",
      "28.3\n",
      "33.7\n",
      "26.2\n",
      "30.4\n",
      "20.6\n",
      "38.6\n",
      "GPT-3 Few-Shot\n",
      "32.6\n",
      "39.2\n",
      "29.7\n",
      "40.6\n",
      "21.0\n",
      "39.5\n",
      "Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating\n",
      "into English reﬂecting its strength as an Eng...\n",
      "Extracted 1 images from page 15\n",
      "Page 16 Text: Setting\n",
      "Winograd\n",
      "Winogrande (XL)\n",
      "Fine-tuned SOTA\n",
      "90.1a\n",
      "84.6b\n",
      "GPT-3 Zero-Shot\n",
      "88.3*\n",
      "70.2\n",
      "GPT-3 One-Shot\n",
      "89.7*\n",
      "73.2\n",
      "GPT-3 Few-Shot\n",
      "88.6*\n",
      "77.7\n",
      "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n",
      "4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]\n",
      "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\n",
      "Scaling is relatively smooth with the gains to...\n",
      "Extracted 1 images from page 16\n",
      "Page 17 Text: Setting\n",
      "PIQA\n",
      "ARC (Easy)\n",
      "ARC (Challenge)\n",
      "OpenBookQA\n",
      "Fine-tuned SOTA\n",
      "79.4\n",
      "92.0[KKS+20]\n",
      "78.5[KKS+20]\n",
      "87.2[KKS+20]\n",
      "GPT-3 Zero-Shot\n",
      "80.5*\n",
      "68.8\n",
      "51.4\n",
      "57.6\n",
      "GPT-3 One-Shot\n",
      "80.5*\n",
      "71.2\n",
      "53.2\n",
      "58.8\n",
      "GPT-3 Few-Shot\n",
      "82.8*\n",
      "70.1\n",
      "51.5\n",
      "65.4\n",
      "Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot\n",
      "PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test\n",
      "set.\n",
      "Figure 3.6: GPT-3 results on PIQA in the zero-sh...\n",
      "Extracted 1 images from page 17\n",
      "Page 18 Text: Setting\n",
      "CoQA\n",
      "DROP\n",
      "QuAC\n",
      "SQuADv2\n",
      "RACE-h\n",
      "RACE-m\n",
      "Fine-tuned SOTA\n",
      "90.7a\n",
      "89.1b\n",
      "74.4c\n",
      "93.0d\n",
      "90.0e\n",
      "93.1e\n",
      "GPT-3 Zero-Shot\n",
      "81.5\n",
      "23.6\n",
      "41.5\n",
      "59.5\n",
      "45.5\n",
      "58.4\n",
      "GPT-3 One-Shot\n",
      "84.0\n",
      "34.3\n",
      "43.3\n",
      "65.4\n",
      "45.9\n",
      "57.4\n",
      "GPT-3 Few-Shot\n",
      "85.0\n",
      "36.5\n",
      "44.3\n",
      "69.8\n",
      "46.8\n",
      "58.1\n",
      "Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.\n",
      "a[JZC+19] b[JN20] c[AI19] d[QIA20] e[SPP+19]\n",
      "ﬁne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than ...\n",
      "No images found on page 18\n",
      "Page 19 Text: Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting,\n",
      "only a few points behind measured human performance and state-of-the-art ﬁne-tuned models. Zero-shot and one-shot\n",
      "performance is a few points behind, with the gains to few-shot being largest for bigger models.\n",
      "SuperGLUE\n",
      "BoolQ\n",
      "CB\n",
      "CB\n",
      "COPA\n",
      "RTE\n",
      "Average\n",
      "Accuracy\n",
      "Accuracy\n",
      "F1\n",
      "Accuracy\n",
      "Accuracy\n",
      "Fine-tuned SOTA\n",
      "89.0\n",
      "91.0\n",
      "96.9\n",
      "93.9\n",
      "94.8\n",
      "92.5\n",
      "Fine-tuned BERT-Large\n",
      "69.0\n",
      "77.4\n",
      "83.6\n",
      "75.7\n",
      "70.6\n",
      "71.7\n",
      "G...\n",
      "Extracted 1 images from page 19\n",
      "Page 20 Text: Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value\n",
      "of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in\n",
      "SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference\n",
      "lines (our test set results are in Table 3.8). The BERT-Large reference model was ﬁne-tuned on the SuperGLUE training\n",
      "set (125K examples), whereas BERT++ was ﬁrs...\n",
      "Extracted 1 images from page 20\n",
      "Page 21 Text: Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples\n",
      "and therefore has high variance (we estimate a standard deviation of 1.2%). We ﬁnd that smaller models hover around\n",
      "random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for\n",
      "ANLI rounds 1 and 2 are shown in the appendix.\n",
      "whether the second sentence logically follows from the ﬁrst, contradicts the ﬁrst sentence, or is possibly true (neutral)....\n",
      "Extracted 1 images from page 21\n",
      "Page 22 Text: Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a\n",
      "signiﬁcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being\n",
      "able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiﬁcant fraction\n",
      "of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot\n",
      "are shown in the appendix...\n",
      "Extracted 1 images from page 22\n",
      "Page 23 Text: Setting\n",
      "2D+\n",
      "2D-\n",
      "3D+\n",
      "3D-\n",
      "4D+\n",
      "4D-\n",
      "5D+\n",
      "5D-\n",
      "2Dx\n",
      "1DC\n",
      "GPT-3 Zero-shot\n",
      "76.9\n",
      "58.0\n",
      "34.2\n",
      "48.3\n",
      "4.0\n",
      "7.5\n",
      "0.7\n",
      "0.8\n",
      "19.8\n",
      "9.8\n",
      "GPT-3 One-shot\n",
      "99.6\n",
      "86.4\n",
      "65.5\n",
      "78.7\n",
      "14.0\n",
      "14.0\n",
      "3.5\n",
      "3.8\n",
      "27.4\n",
      "14.3\n",
      "GPT-3 Few-shot\n",
      "100.0\n",
      "98.9\n",
      "80.4\n",
      "94.2\n",
      "25.5\n",
      "26.8\n",
      "9.3\n",
      "9.9\n",
      "29.2\n",
      "21.3\n",
      "Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or\n",
      "subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger\n",
      "moving from the zero-shot to one...\n",
      "No images found on page 23\n",
      "Page 24 Text: Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally\n",
      "smooth improvement with model size although the random insertion task shows an upward slope of improvement with\n",
      "the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\n",
      "the appendix. All tasks are done with K = 100.\n",
      "random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult...\n",
      "Extracted 1 images from page 24\n",
      "Page 25 Text: Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest\n",
      "model achieves 65% accuracy in the few-shot setting, and also demonstrates signiﬁcant gains to in-context learning\n",
      "which are not present in smaller models.\n",
      "3.9.4\n",
      "News Article Generation\n",
      "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by\n",
      "conditional sampling from the model given a human-written prompt consisting of a...\n",
      "Extracted 1 images from page 25\n",
      "Page 26 Text: Mean accuracy\n",
      "95% Conﬁdence\n",
      "Interval (low, hi)\n",
      "t compared to\n",
      "control (p-value)\n",
      "“I don’t know”\n",
      "assignments\n",
      "Control (deliberately bad model)\n",
      "86%\n",
      "83%–90%\n",
      "-\n",
      "3.6 %\n",
      "GPT-3 Small\n",
      "76%\n",
      "72%–80%\n",
      "3.9 (2e-4)\n",
      "4.9%\n",
      "GPT-3 Medium\n",
      "61%\n",
      "58%–65%\n",
      "10.3 (7e-21)\n",
      "6.0%\n",
      "GPT-3 Large\n",
      "68%\n",
      "64%–72%\n",
      "7.3 (3e-11)\n",
      "8.7%\n",
      "GPT-3 XL\n",
      "62%\n",
      "59%–65%\n",
      "10.7 (1e-19)\n",
      "7.5%\n",
      "GPT-3 2.7B\n",
      "62%\n",
      "58%–65%\n",
      "10.4 (5e-19)\n",
      "7.1%\n",
      "GPT-3 6.7B\n",
      "60%\n",
      "56%–63%\n",
      "11.2 (3e-21)\n",
      "6.2%\n",
      "GPT-3 13B\n",
      "55%\n",
      "52%–58%\n",
      "15.3 (1e-32)\n",
      "7.1%\n",
      "GPT-3 175B\n",
      "52%\n",
      "49%–54%\n",
      "16.9 (1e-34)\n",
      "7.8%\n",
      "Table 3.11: Hum...\n",
      "No images found on page 26\n",
      "Page 27 Text: Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct\n",
      "assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-\n",
      "bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed\n",
      "line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best ﬁt is a power\n",
      "law with 95% conﬁdence inte...\n",
      "Extracted 1 images from page 27\n",
      "Page 28 Text: Title:\n",
      "United Methodists Agree to Historic Split\n",
      "Subtitle:\n",
      "Those who oppose gay marriage will form their own denomination\n",
      "Article:\n",
      "After two days of intense debate, the United Methodist Church\n",
      "has agreed to a historic split - one that is expected to end in the\n",
      "creation of a new denomination, one that will be \"theologically and\n",
      "socially conservative,\" according to The Washington Post.\n",
      "The majority of\n",
      "delegates attending the church's annual General Conference in May voted to\n",
      "strengthen a ban on th...\n",
      "No images found on page 28\n",
      "Page 29 Text: A \"whatpu\" is a small, furry animal native to Tanzania.\n",
      "An example of a sentence that uses\n",
      "the word whatpu is:\n",
      "We were traveling in Africa and we saw these very cute whatpus.\n",
      "To do a \"farduddle\" means to jump up and down really fast.\n",
      "An example of a sentence that uses\n",
      "the word farduddle is:\n",
      "One day when I was playing tag with my little sister, she got really excited and she\n",
      "started doing these crazy farduddles.\n",
      "A \"yalubalu\" is a type of vegetable that looks like a big pumpkin.\n",
      "An example of a se...\n",
      "No images found on page 29\n",
      "Page 30 Text: Poor English input:\n",
      "I eated the purple berries.\n",
      "Good English output:\n",
      "I ate the purple berries.\n",
      "Poor English input:\n",
      "Thank you for picking me as your designer.\n",
      "I’d appreciate it.\n",
      "Good English output:\n",
      "Thank you for choosing me as your designer.\n",
      "I appreciate it.\n",
      "Poor English input:\n",
      "The mentioned changes have done.\n",
      "or I did the alteration that you\n",
      "requested.\n",
      "or I changed things you wanted and did the modifications.\n",
      "Good English output:\n",
      "The requested changes have been made.\n",
      "or I made the alteration th...\n",
      "No images found on page 30\n",
      "Page 31 Text: Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\n",
      "split of our training distribution. Though there is some gap between training and validation performance, the gap grows\n",
      "only minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\n",
      "rather than overﬁtting.\n",
      "although models did perform moderately better on data that overlapped between training and testing, this did not\n",
      "signiﬁcantly imp...\n",
      "Extracted 1 images from page 31\n",
      "Page 32 Text: Figure 4.2: Benchmark contamination analysis\n",
      "We constructed cleaned versions of each of our benchmarks to\n",
      "check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the\n",
      "dataset is known with high conﬁdence to be clean, and the y-axis shows the difference in performance when evaluating\n",
      "only on the veriﬁed clean subset. Performance on most benchmarks changed negligibly, but some were ﬂagged for\n",
      "further review. On inspection we ﬁnd some evidence ...\n",
      "Extracted 1 images from page 32\n",
      "Page 33 Text: • Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the\n",
      "Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably\n",
      "extract a clean subset here, we do not report results on these datasets, even though we intended to when starting\n",
      "this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language\n",
      "modeling benchmark.\n",
      "We also inspected datasets where contaminatio...\n",
      "No images found on page 33\n",
      "Page 34 Text: pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to\n",
      "predict and what is less important. [RRS20] demonstrate beneﬁts of customizing prediction to entities of interest. Also,\n",
      "with self-supervised objectives, task speciﬁcation relies on forcing the desired task into a prediction problem, whereas\n",
      "ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed\n",
      "actions rather th...\n",
      "No images found on page 34\n",
      "Page 35 Text: 6.1\n",
      "Misuse of Language Models\n",
      "Malicious uses of language models can be somewhat difﬁcult to anticipate because they often involve repurposing\n",
      "language models in a very different environment or for a different purpose than researchers intended. To help with this,\n",
      "we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying\n",
      "threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact\n",
      "[Ro...\n",
      "No images found on page 35\n",
      "Page 36 Text: 6.2\n",
      "Fairness, Bias, and Representation\n",
      "Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning,\n",
      "since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and\n",
      "producing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in\n",
      "the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation. 8\n",
      "Ou...\n",
      "No images found on page 36\n",
      "Page 37 Text: Table 6.1: Most Biased Descriptive Words in 175B Model\n",
      "Top 10 Most Biased Male Descriptive Words with Raw\n",
      "Co-Occurrence Counts\n",
      "Top 10 Most Biased Female Descriptive Words with Raw\n",
      "Co-Occurrence Counts\n",
      "Average Number of Co-Occurrences Across All Words:\n",
      "17.5\n",
      "Average Number of Co-Occurrences Across All Words:\n",
      "23.9\n",
      "Large (16)\n",
      "Optimistic (12)\n",
      "Mostly (15)\n",
      "Bubbly (12)\n",
      "Lazy (14)\n",
      "Naughty (12)\n",
      "Fantastic (13)\n",
      "Easy-going (12)\n",
      "Eccentric (13)\n",
      "Petite (10)\n",
      "Protect (10)\n",
      "Tight (10)\n",
      "Jolly (10)\n",
      "Pregnant (10)\n",
      "Stable...\n",
      "No images found on page 37\n",
      "Page 38 Text: Figure 6.1: Racial Sentiment Across Models\n",
      "Religion\n",
      "Most Favored Descriptive Words\n",
      "Atheism\n",
      "‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’,\n",
      "‘Characterized’\n",
      "Buddhism\n",
      "‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En-\n",
      "lightenment’, ‘Non-Violent’\n",
      "Christianity\n",
      "‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com-\n",
      "ments’, ‘Ofﬁcially’\n",
      "Hinduism\n",
      "‘Caste’, ‘Cows’, ‘BJ...\n",
      "Extracted 1 images from page 38\n",
      "Page 39 Text: 6.2.4\n",
      "Future Bias and Fairness Challenges\n",
      "We have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\n",
      "and to highlight the inherent difﬁculties in characterizing biases in large-scale generative models; we expect this to be an\n",
      "area of continuous research for us and are excited to discuss different methodological approaches with the community.\n",
      "We view the work in this section as subjective signposting - we chose gender, race, and religio...\n",
      "No images found on page 39\n",
      "Page 40 Text: task-speciﬁc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and\n",
      "techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint\n",
      "of giant models.\n",
      "As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable\n",
      "effort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19,\n",
      "IBGC+14, CCE+18, MCKS18], reading compreh...\n",
      "No images found on page 40\n",
      "Page 41 Text: state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at\n",
      "tasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning.\n",
      "We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results\n",
      "suggest that very large language models may be an important ingredient in the development of adaptable, general\n",
      "language systems.\n",
      "Acknowledgements\n",
      "The...\n",
      "No images found on page 41\n",
      "Page 42 Text: Contributions\n",
      "Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu\n",
      "implemented the large-scale models, training infrastructure, and model-parallel strategies.\n",
      "Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\n",
      "Ben Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on the training data.\n",
      "Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, ...\n",
      "No images found on page 42\n",
      "Page 43 Text: A\n",
      "Details of Common Crawl Filtering\n",
      "As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1)\n",
      "ﬁltering Common Crawl and (2) fuzzy deduplication:\n",
      "1. In order to improve the quality of Common Crawl, we developed an automatic ﬁltering method to remove low\n",
      "quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classiﬁer\n",
      "to distinguish these from raw Common Crawl. We then used this classiﬁer to re-sample ...\n",
      "No images found on page 43\n",
      "Page 44 Text: removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long\n",
      "documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in\n",
      "which the Wikipedia article quotes a single line from a book. We ignored 13−grams that matched more than 10 training\n",
      "documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar\n",
      "content that we likely do want...\n",
      "No images found on page 44\n",
      "Page 45 Text: Name\n",
      "Split\n",
      "Metric\n",
      "N\n",
      "Acc/F1/BLEU\n",
      "Total\n",
      "Count\n",
      "Dirty\n",
      "Acc/F1/BLEU\n",
      "Dirty\n",
      "Count\n",
      "Clean\n",
      "Acc/F1/BLEU\n",
      "Clean\n",
      "Count\n",
      "Clean\n",
      "Percentage\n",
      "Relative\n",
      "Difference\n",
      "Clean vs All\n",
      "Quac\n",
      "dev\n",
      "f1\n",
      "13\n",
      "44.3\n",
      "7353\n",
      "44.3\n",
      "7315\n",
      "54.1\n",
      "38\n",
      "1%\n",
      "20%\n",
      "SQuADv2\n",
      "dev\n",
      "f1\n",
      "13\n",
      "69.8\n",
      "11873\n",
      "69.9\n",
      "11136\n",
      "68.4\n",
      "737\n",
      "6%\n",
      "-2%\n",
      "DROP\n",
      "dev\n",
      "f1\n",
      "13\n",
      "36.5\n",
      "9536\n",
      "37.0\n",
      "8898\n",
      "29.5\n",
      "638\n",
      "7%\n",
      "-21%\n",
      "Symbol Insertion\n",
      "dev\n",
      "acc\n",
      "7\n",
      "66.9\n",
      "10000\n",
      "66.8\n",
      "8565\n",
      "67.1\n",
      "1435\n",
      "14%\n",
      "0%\n",
      "CoQa\n",
      "dev\n",
      "f1\n",
      "13\n",
      "86.0\n",
      "7983\n",
      "85.3\n",
      "5107\n",
      "87.1\n",
      "2876\n",
      "36%\n",
      "1%\n",
      "ReCoRD\n",
      "dev\n",
      "acc\n",
      "13\n",
      "89.5\n",
      "10000\n",
      "90.3\n",
      "6110\n",
      "88.2\n",
      "3890\n",
      "39%\n",
      "-1%\n",
      "Winograd\n",
      "test\n",
      "...\n",
      "No images found on page 45\n",
      "Page 46 Text: D\n",
      "Total Compute Used to Train Language Models\n",
      "This appendix contains the calculations that were used to derive the approximate compute used to train the language\n",
      "models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10%\n",
      "of the total compute for the models we are analyzing.\n",
      "Calculations can be seen in Table D.1 and are explained within the table caption.\n",
      "Model\n",
      "Total train\n",
      "compute\n",
      "(PF-days)\n",
      "Total train\n",
      "compute\n",
      "(ﬂops)\n",
      "Params\n",
      "(M)\n",
      "Trainin...\n",
      "No images found on page 46\n",
      "Page 47 Text: Model\n",
      "Participants\n",
      "Recruited\n",
      "Participants\n",
      "Excluded\n",
      "Genders\n",
      "(m:f:other)\n",
      "Mean\n",
      "Age\n",
      "Average\n",
      "Word Count\n",
      "(human:model)\n",
      "Control\n",
      "76\n",
      "7\n",
      "32:37:0\n",
      "39\n",
      "216:216\n",
      "GPT-3 Small\n",
      "80\n",
      "7\n",
      "41:31:1\n",
      "40\n",
      "216:188\n",
      "GPT-3 Medium\n",
      "80\n",
      "7\n",
      "46:28:2\n",
      "39\n",
      "216:202\n",
      "GPT-3 Large\n",
      "81\n",
      "24\n",
      "46:28:2\n",
      "37\n",
      "216:200\n",
      "GPT-3 XL\n",
      "79\n",
      "14\n",
      "32:32:1\n",
      "38\n",
      "216:199\n",
      "GPT-3 2.7B\n",
      "80\n",
      "11\n",
      "36:33:0\n",
      "40\n",
      "216:202\n",
      "GPT-3 6.7B\n",
      "76\n",
      "5\n",
      "46:28:2\n",
      "37\n",
      "216:195\n",
      "GPT-3 13.0B\n",
      "81\n",
      "13\n",
      "46:28:2\n",
      "37\n",
      "216:209\n",
      "GPT-3 175B\n",
      "80\n",
      "9\n",
      "42:29:0\n",
      "37\n",
      "216:216\n",
      "Table E.1: Participant details and article lengths for each experime...\n",
      "Extracted 1 images from page 47\n",
      "Page 48 Text: Model\n",
      "Participants\n",
      "Recruited\n",
      "Participants\n",
      "Excluded\n",
      "Genders\n",
      "(m:f:other)\n",
      "Mean\n",
      "Age\n",
      "Average\n",
      "Word Count\n",
      "(human:model)\n",
      "Control\n",
      "79\n",
      "17\n",
      "32:37:0\n",
      "39\n",
      "569:464\n",
      "GPT-3 175B\n",
      "81\n",
      "19\n",
      "32:30:0\n",
      "40\n",
      "569:498\n",
      "Table E.2: Participant details and article lengths for the experiments investigating human detection of ∼500 word\n",
      "model generated news articles. Participants were excluded due to internet check fails.\n",
      "accuracy scores despite increased time investment from participants supports the ﬁnding that larger models generate\n",
      "h...\n",
      "No images found on page 48\n",
      "Page 49 Text: Context →\n",
      "The City\n",
      "BY C. P. CAVAFY\n",
      "TRANSLATED BY EDMUND KEELEY\n",
      "[Poem text omitted]\n",
      "SOME TREES\n",
      "John Ashbery\n",
      "[Poem text omitted]\n",
      "Shadows on the Way\n",
      "Wallace Stevens\n",
      "-------- Generated Poem 1 --------\n",
      "-------- Generated Poem 3 --------\n",
      "I must have shadows on the way\n",
      "The sun was all we had.\n",
      "Now, in the shade\n",
      "If I am to walk I must have\n",
      "All is changed.\n",
      "The mind must dwell on those\n",
      "Each step taken slowly and alone\n",
      "White fields, that to its eyes were always old;\n",
      "To have it ready made\n",
      "Those ancient gleam...\n",
      "No images found on page 49\n",
      "Page 50 Text: G\n",
      "Details of Task Phrasing and Speciﬁcations\n",
      "The following ﬁgures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from\n",
      "the ground truth datasets in this section, and no samples from GPT-3 are included here.\n",
      "Context →\n",
      "Article:\n",
      "Informal conversation is an important part of any business\n",
      "relationship.Before you start a discussion,however,make sure you understand\n",
      "which topics are suitable and which are considered taboo in a particular\n",
      "culture.\n",
      "Latin Ameri...\n",
      "No images found on page 50\n",
      "Page 51 Text: Context →\n",
      "anli 2:\n",
      "anli 2:\n",
      "The Gold Coast Hotel & Casino is a hotel and casino\n",
      "located in Paradise, Nevada.\n",
      "This locals’ casino is owned and operated\n",
      "by Boyd Gaming.\n",
      "The Gold Coast is located one mile (∼\n",
      "1.6km) west of the\n",
      "Las Vegas Strip on West Flamingo Road.\n",
      "It is located across the street\n",
      "from the Palms Casino Resort and the Rio All Suite Hotel and Casino.\n",
      "Question:\n",
      "The Gold Coast is a budget-friendly casino.\n",
      "True, False, or\n",
      "Neither?\n",
      "Correct Answer →\n",
      "Neither\n",
      "Incorrect Answer →\n",
      "True\n",
      "Incorrect ...\n",
      "No images found on page 51\n",
      "Page 52 Text: Context →\n",
      "How to apply sealant to wood.\n",
      "Correct Answer →\n",
      "Using a brush, brush on sealant onto wood until it is fully saturated with\n",
      "the sealant.\n",
      "Incorrect Answer →\n",
      "Using a brush, drip on sealant onto wood until it is fully saturated with\n",
      "the sealant.\n",
      "Figure G.4: Formatted dataset example for PIQA\n",
      "Context →\n",
      "My body cast a shadow over the grass because\n",
      "Correct Answer →\n",
      "the sun was rising.\n",
      "Incorrect Answer →\n",
      "the grass was cut.\n",
      "Figure G.5: Formatted dataset example for COPA\n",
      "Context →\n",
      "(CNN) Yuval Rab...\n",
      "No images found on page 52\n",
      "Page 53 Text: Context →\n",
      "Organisms require energy in order to do what?\n",
      "Correct Answer →\n",
      "mature and develop.\n",
      "Incorrect Answer →\n",
      "rest soundly.\n",
      "Incorrect Answer →\n",
      "absorb light.\n",
      "Incorrect Answer →\n",
      "take in nutrients.\n",
      "Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\n",
      "probability of each answer as described in 2.\n",
      "Context →\n",
      "Making a cake:\n",
      "Several cake pops are shown on a display.\n",
      "A woman and girl\n",
      "are shown making the cake pops in a kitchen.\n",
      "They\n",
      "Correct Answer →\n",
      "b...\n",
      "No images found on page 53\n",
      "Page 54 Text: Correct Context →\n",
      "Johnny likes fruits more than vegetables in his new keto diet because the\n",
      "fruits\n",
      "Incorrect Context →\n",
      "Johnny likes fruits more than vegetables in his new keto diet because the\n",
      "vegetables\n",
      "Target Completion →\n",
      "are saccharine.\n",
      "Figure G.14: Formatted dataset example for Winogrande. The ‘partial’ evaluation method we use compares the\n",
      "probability of the completion given a correct and incorrect context.\n",
      "Context →\n",
      "READING COMPREHENSION ANSWER KEY\n",
      "While this process moved along, diplomacy...\n",
      "No images found on page 54\n",
      "Page 55 Text: Context →\n",
      "Bob went to the gas station to fill up his car.\n",
      "His tank was completely\n",
      "empty and so was his wallet.\n",
      "The cashier offered to pay for his gas if he\n",
      "came back later to pay.\n",
      "Bob felt grateful as he drove home.\n",
      "Correct Answer →\n",
      "Bob believed that there were good people in the world.\n",
      "Incorrect Answer →\n",
      "Bob contemplated how unfriendly the world was.\n",
      "Figure G.17: Formatted dataset example for StoryCloze\n",
      "Context →\n",
      "Helsinki is the capital and largest city of Finland.\n",
      "It is in the region\n",
      "of Uusima...\n",
      "No images found on page 55\n",
      "Page 56 Text: Context →\n",
      "Passage:\n",
      "Saint Jean de Br´ebeuf was a French Jesuit missionary who\n",
      "travelled to New France in 1625.\n",
      "There he worked primarily with the Huron\n",
      "for the rest of his life, except for a few years in France from 1629 to\n",
      "1633.\n",
      "He learned their language and culture, writing extensively about\n",
      "each to aid other missionaries.\n",
      "In 1649, Br´ebeuf and another missionary\n",
      "were captured when an Iroquois raid took over a Huron village .\n",
      "Together\n",
      "with Huron captives, the missionaries were ritually tortured...\n",
      "No images found on page 56\n",
      "Page 57 Text: Context →\n",
      "TITLE: William Perry (American football) - Professional career\n",
      "PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL\n",
      "Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.\n",
      "However, defensive coordinator Buddy Ryan, who had a highly acrimonious\n",
      "relationship with Ditka, called Perry a \"wasted draft-pick\".\n",
      "Perry\n",
      "soon became a pawn in the political power struggle between Ditka and\n",
      "Ryan.\n",
      "Perry’s \"Refrigerator\" nickname followed him into the NFL and he\n",
      "quic...\n",
      "No images found on page 57\n",
      "Page 58 Text: Context →\n",
      "Title:\n",
      "The Blitz\n",
      "Background:\n",
      "From the German point of view, March 1941 saw an improvement.\n",
      "The Luftwaffe flew 4,000 sorties that month, including 12 major and\n",
      "three heavy attacks.\n",
      "The electronic war intensified but the Luftwaffe\n",
      "flew major inland missions only on moonlit nights.\n",
      "Ports were easier to\n",
      "find and made better targets.\n",
      "To confuse the British, radio silence was\n",
      "observed until the bombs fell.\n",
      "X- and Y-Ger¨at beams were placed over\n",
      "false targets and switched only at the last min...\n",
      "No images found on page 58\n",
      "Page 59 Text: Context →\n",
      "The bet, which won him dinner for four, was regarding the existence and\n",
      "mass of the top quark, an elementary particle discovered in 1995.\n",
      "question:\n",
      "The Top Quark is the last of six flavors of quarks predicted by\n",
      "the standard model theory of particle physics.\n",
      "True or False?\n",
      "answer:\n",
      "Target Completion →\n",
      "False\n",
      "Figure G.31: Formatted dataset example for RTE\n",
      "Context →\n",
      "An outfitter provided everything needed for the safari.\n",
      "Before his first walking holiday, he went to a specialist outfitter t...\n",
      "No images found on page 59\n",
      "Page 60 Text: Context →\n",
      "Q: What school did burne hogarth establish?\n",
      "A:\n",
      "Target Completion →\n",
      "School of Visual Arts\n",
      "Figure G.35: Formatted dataset example for WebQA\n",
      "Context →\n",
      "Keinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden.\n",
      "=\n",
      "Target Completion →\n",
      "In no case may they be used for commercial purposes.\n",
      "Figure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and\n",
      "other langauge tasks, the format for zero-shot learning is “Q: What is the {l...\n",
      "No images found on page 60\n",
      "Page 61 Text: Context →\n",
      "Adev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei\n",
      "europenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea\n",
      "European˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul\n",
      "¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort.\n",
      "=\n",
      "Target Completion →\n",
      "The truth is that you want, at any price, and against the wishes of the\n",
      "peoples of Europe, to continue the negotiations for Turkey’s accession\n",
      "to the European Union, de...\n",
      "No images found on page 61\n",
      "Page 62 Text: Context →\n",
      "Q: What is 9923 plus 617?\n",
      "A:\n",
      "Target Completion →\n",
      "10540\n",
      "Figure G.49: Formatted dataset example for Arithmetic 4D+\n",
      "Context →\n",
      "Q: What is 40649 minus 78746?\n",
      "A:\n",
      "Target Completion →\n",
      "-38097\n",
      "Figure G.50: Formatted dataset example for Arithmetic 5D−\n",
      "Context →\n",
      "Q: What is 65360 plus 16204?\n",
      "A:\n",
      "Target Completion →\n",
      "81564\n",
      "Figure G.51: Formatted dataset example for Arithmetic 5D+\n",
      "62\n",
      "...\n",
      "No images found on page 62\n",
      "Page 63 Text: H\n",
      "Results on All Tasks for All Model Sizes\n",
      "Zero-Shot\n",
      "One-Shot\n",
      "Few-Shot\n",
      "Name\n",
      "Metric\n",
      "Split\n",
      "Fine-tune\n",
      "SOTA\n",
      "K\n",
      "Small Med Large XL 2.7B 6.7B 13B 175B\n",
      "Small Med Large XL 2.7B 6.7B 13B 175B\n",
      "Small Med Large XL 2.7B 6.7B 13B 175B\n",
      "175B\n",
      "(test server)\n",
      "HellaSwag\n",
      "acc\n",
      "dev\n",
      "85.6\n",
      "20\n",
      "33.7\n",
      "43.6 51.0\n",
      "54.7 62.8 67.4 70.9 78.9\n",
      "33.0\n",
      "42.9 50.5\n",
      "53.5 61.9 66.5 70.0 78.1\n",
      "33.5\n",
      "43.1 51.3\n",
      "54.9 62.9 67.3 71.3 79.3\n",
      "LAMBADA\n",
      "acc\n",
      "test\n",
      "68.0\n",
      "15\n",
      "42.7\n",
      "54.3 60.4\n",
      "63.6 67.1 70.3 72.5 76.2\n",
      "22.0\n",
      "47.1 52.6\n",
      "58.3 61.1 65.4 69.0 72.5\n",
      "22.0\n",
      "40.4 ...\n",
      "No images found on page 63\n",
      "Page 64 Text: Figure H.1: All results for all SuperGLUE tasks.\n",
      "Figure H.2: Results for SAT task.\n",
      "Figure H.3: All results for all Winograd tasks.\n",
      "64\n",
      "...\n",
      "Extracted 14 images from page 64\n",
      "Page 65 Text: Figure H.4: All results for all Arithmetic tasks.\n",
      "Figure H.5: All results for all Cloze and Completion tasks.\n",
      "65\n",
      "...\n",
      "Extracted 14 images from page 65\n",
      "Page 66 Text: Figure H.6: All results for all Common Sense Reasoning tasks.\n",
      "Figure H.7: All results for all QA tasks.\n",
      "Figure H.8: All results for all Reading Comprehension tasks.\n",
      "Figure H.9: All results for all ANLI rounds.\n",
      "66\n",
      "...\n",
      "Extracted 15 images from page 66\n",
      "Page 67 Text: Figure H.10: All results for all Scramble tasks.\n",
      "Figure H.11: All results for all Translation tasks.\n",
      "67\n",
      "...\n",
      "Extracted 18 images from page 67\n",
      "Page 68 Text: References\n",
      "[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\n",
      "Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent.\n",
      "In Advances in neural information processing systems, pages 3981–3989, 2016.\n",
      "[AI19] WeChat AI. Tr-mt (ensemble), December 2019.\n",
      "[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In\n",
      "Proceedings of the 2019 Conference of the North A...\n",
      "No images found on page 68\n",
      "Page 69 Text: [DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\n",
      "challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiﬁcation,\n",
      "and recognising textual entailment, pages 177–190. Springer, 2006.\n",
      "[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\n",
      "transformers. Arxiv, 2018.\n",
      "[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based...\n",
      "No images found on page 69\n",
      "Page 70 Text: [HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md.\n",
      "Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.\n",
      "arXiv preprint arXiv:1712.00409, 2017.\n",
      "[HR18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv\n",
      "preprint arXiv:1801.06146, 2018.\n",
      "[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n",
      "preprin...\n",
      "No images found on page 70\n",
      "Page 71 Text: [LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
      "cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\n",
      "arXiv:1909.11942, 2019.\n",
      "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n",
      "Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.\n",
      "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transfer...\n",
      "No images found on page 71\n",
      "Page 72 Text: [MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con-\n",
      "textualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305,\n",
      "2017.\n",
      "[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations\n",
      "in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
      "[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\n",
      "Pushmeet Kohli, and Ja...\n",
      "No images found on page 72\n",
      "Page 73 Text: [PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning.\n",
      "GloVe: Global vectors for word\n",
      "representation. In Proceedings of the 2014 conference on empirical methods in natural language\n",
      "processing (EMNLP), 2014.\n",
      "[QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020.\n",
      "[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language\n",
      "models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019.\n",
      "[RBG11] Melissa Roemmele, Cosmi...\n",
      "No images found on page 73\n",
      "Page 74 Text: [SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\n",
      "Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\n",
      "arXiv:1701.06538, 2017.\n",
      "[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\n",
      "Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\n",
      "[SS20] Timo Schick and Hinrich Sch¨utze. Exploiting cloze questi...\n",
      "No images found on page 74\n",
      "Page 75 Text: [ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\n",
      "tiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593,\n",
      "2019.\n",
      "75\n",
      "...\n",
      "No images found on page 75\n",
      "Finished processing 2005.14165.pdf\n",
      "Processing: 2211.05102.pdf\n",
      "Total pages: 18\n",
      "Page 1 Text: EFFICIENTLY SCALING TRANSFORMER INFERENCE\n",
      "Reiner Pope 1 Sholto Douglas 1 Aakanksha Chowdhery 1 Jacob Devlin 1 James Bradbury 1\n",
      "Anselm Levskaya 1 Jonathan Heek 1 Kefan Xiao 1 Shivani Agrawal 1 Jeff Dean 1\n",
      "ABSTRACT\n",
      "We study the problem of efﬁcient generative inference for Transformer models, in one of its most challenging\n",
      "settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the\n",
      "engineering tradeoffs for inference for large Transformer-based mod...\n",
      "No images found on page 1\n",
      "Page 2 Text: Efﬁciently Scaling Transformer Inference\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "Latency per Generated Token (Milliseconds)\n",
      "0.0625\n",
      "0.125\n",
      "0.25\n",
      "0.5\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "Cost (Chip-Milliseconds per Token)\n",
      "C:128, B:64\n",
      "C:64, B:128\n",
      "C:64, B:1024\n",
      "C:64, B:64\n",
      "C:32, B:32\n",
      "C:16, B:64\n",
      "C:8, B:1024\n",
      "C:16, B:32\n",
      "C:8, B:64\n",
      "C:8, B:1024\n",
      "Decoding Latency vs. Cost\n",
      "PaLM 540B\n",
      "PaLM 540B-int8\n",
      "PaLM 62B\n",
      "PaLM 62B-int8\n",
      "PaLM 8B\n",
      "PaLM 8B-int8\n",
      "0.03 0.06 0.12 0.25\n",
      "0.5\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "Latency per Forward Pass (Seconds)\n",
      "0.0625\n",
      "0.125\n",
      "0.25...\n",
      "No images found on page 2\n",
      "Page 3 Text: Efﬁciently Scaling Transformer Inference\n",
      "need to be partitioned across many accelerator chips to ﬁt\n",
      "in memory. This also enables us to divide the memory and\n",
      "compute costs described below over all the chips, but comes\n",
      "at the cost of introducing chip-to-chip communication.\n",
      "Memory costs.\n",
      "We store tensors such as weights and the\n",
      "KV cache in on-device high-bandwidth memory (HBM).\n",
      "While there are other tensors that pass through the HBM,\n",
      "their memory footprint is much smaller, so we focus on just\n",
      "these...\n",
      "No images found on page 3\n",
      "Page 4 Text: Efﬁciently Scaling Transformer Inference\n",
      "Following (Xu et al., 2021), we use subscripts to specify the\n",
      "tensor dimension that is partitioned. For example, notation\n",
      "BLExyz means that the last dimension E of a tensor of\n",
      "logical shape BLE is split into X × Y × Z partitions,\n",
      "where x, y and z refer to the physical TPU v4 axes, and\n",
      "the per-chip tensor is of shape [B, L, E/(X × Y × Z)].\n",
      "Here B, E and F refers to batch, model embed and MLP\n",
      "feedforward dimension. We use L to refer to the sequence\n",
      "length a...\n",
      "No images found on page 4\n",
      "Page 5 Text: Efﬁciently Scaling Transformer Inference\n",
      "BLExyz\n",
      "EFxyz\n",
      "Win\n",
      "einsum\n",
      "BLE\n",
      "gelu\n",
      "FxyzE\n",
      "Wout\n",
      "BLFxyz\n",
      "BLE (partialsum-xyz)\n",
      "BLExyz\n",
      "+\n",
      "einsum\n",
      "Z\n",
      "Y\n",
      "BLExyz\n",
      "ExFyz\n",
      "Win\n",
      "einsum\n",
      "BLEx\n",
      "gelu\n",
      "FyzEx\n",
      "Wout\n",
      "BLFxyz\n",
      "BLEx(partialsum-yz)\n",
      "BLExyz\n",
      "+\n",
      "einsum\n",
      "2D weight-\n",
      "stationary\n",
      "1D weight-\n",
      "stationary\n",
      "BxyLEz \n",
      "ExFyz\n",
      "Win\n",
      "einsum\n",
      "BxyLE\n",
      "BxyLFz\n",
      "gelu\n",
      "FyzEx\n",
      "Wout\n",
      "BxyLE   (partialsum-z)\n",
      "einsum\n",
      "Weight-\n",
      "gathered\n",
      "BLFyz\n",
      "EFz\n",
      "FzE\n",
      "BxyLEz\n",
      "+\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "activations\n",
      "activations\n",
      "activations\n",
      "all-gather(xyz)\n",
      "reduce-scatter(xyz)\n",
      "all-gather(yz)\n",
      "all-gather(x...\n",
      "No images found on page 5\n",
      "Page 6 Text: Efﬁciently Scaling Transformer Inference\n",
      "2000\n",
      "20000\n",
      "200000\n",
      "2000000\n",
      "Tokens per Batch\n",
      "0.1\n",
      "1\n",
      "10\n",
      "100\n",
      "Communication Volume (GB)\n",
      "Communication Volume Comparison\n",
      "Weight-stationary\n",
      "X-weight-gathered\n",
      "XY-weight-gathered\n",
      "XYZ-weight-gathered\n",
      "Min volume\n",
      "Figure 3: Communication volume as a function of batch size,\n",
      "for a feedforward layer. As batch size (in tokens) grows, it is\n",
      "better to switch to a layout that all-gathers the weights over\n",
      "increasingly more chips to minimize the communication\n",
      "volume. Communicat...\n",
      "No images found on page 6\n",
      "Page 7 Text: Efﬁciently Scaling Transformer Inference\n",
      "BLHyzQ\n",
      "(partialsum-x)\n",
      "softmax\n",
      "BLHxyzQ\n",
      "BLHxyzQ\n",
      "BLHyzQ \n",
      "einsum\n",
      "Multi-head attention \n",
      "(sharded over heads)\n",
      "activations after\n",
      "WQ/WK/WV projection\n",
      "BLHxyzQ\n",
      "+\n",
      "BLLHxyz\n",
      "BLHxyzQ\n",
      "Q\n",
      "K\n",
      "masks\n",
      "reduce-scatter(x)\n",
      "all-gather(x)\n",
      "V\n",
      "WO projection\n",
      "Z\n",
      "X\n",
      "Y\n",
      "BLHxyzQ\n",
      "Kcache\n",
      "BLHxyzQ\n",
      "Vcache\n",
      "BLHyzQ\n",
      "(partialsum-x)\n",
      "einsum\n",
      "softmax\n",
      "BxyzLHQ\n",
      "BxyzLHQ\n",
      "BLHyzQ \n",
      "einsum\n",
      "Multi-query attention \n",
      "(sharded over batch)\n",
      "activations after\n",
      "WQ/WK/WV projection\n",
      "BxyzLQ\n",
      "+\n",
      "BxyzLLH\n",
      "Q\n",
      "K\n",
      "masks\n",
      "reduce-scatter(x)\n",
      "al...\n",
      "No images found on page 7\n",
      "Page 8 Text: Efﬁciently Scaling Transformer Inference\n",
      "64\n",
      "128\n",
      "256\n",
      "6 × 101\n",
      "2 × 102\n",
      "Chip count\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "Latency per Step (milliseconds)\n",
      "Weight Stationary: 2D vs. 1D\n",
      "2D Weight Stationary\n",
      "1D Weight Stationary\n",
      "Figure 6: Latency per token doing text generation of PaLM\n",
      "540B for 2D and 1D weight stationary layouts on 64 chips.\n",
      "without noticeable quality loss. This enables memory time\n",
      "savings from weight loading, which is especially helpful in\n",
      "the low batch size regime, and it reduces communication\n",
      "volume in weig...\n",
      "No images found on page 8\n",
      "Page 9 Text: Efﬁciently Scaling Transformer Inference\n",
      "128\n",
      "512\n",
      "2048\n",
      "8192\n",
      "Context Sequence Length\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "Latency per Step (milliseconds)\n",
      "Multiquery vs. Multihead Attention (8 layers)\n",
      "Multiquery (Optimized Layout)\n",
      "Multiquery (Inefficient Layout)\n",
      "Multihead\n",
      "Figure 8: Latency per generated token vs. sequence length,\n",
      "for an 8-layer version of PaLM 540B on 64 chips with batch\n",
      "size 256. The dotted line represents that on the full 118-\n",
      "layer model and context lengths longer than 512, the KV\n",
      "cache will not ﬁt in me...\n",
      "No images found on page 9\n",
      "Page 10 Text: Efﬁciently Scaling Transformer Inference\n",
      "Low-latency\n",
      "High-throughput\n",
      "Preﬁll\n",
      "Decode\n",
      "Preﬁll\n",
      "Decode\n",
      "Chips\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "Batch\n",
      "1\n",
      "64\n",
      "512\n",
      "512\n",
      "FFN\n",
      "WS 2D\n",
      "WS 2D\n",
      "WG XYZ\n",
      "WS 2D\n",
      "Attention\n",
      "sharding\n",
      "Head\n",
      "Batch\n",
      "Batch\n",
      "Batch\n",
      "Weights\n",
      "format\n",
      "int8\n",
      "int8\n",
      "bﬂoat16\n",
      "bﬂoat16\n",
      "MFU\n",
      "43%\n",
      "14%\n",
      "76%\n",
      "33%\n",
      "Latency\n",
      "0.29s\n",
      "1.82s\n",
      "85.2s\n",
      "6.0s\n",
      "Table 2: Example conﬁgurations for PaLM 540B, in the same\n",
      "setting as Figure 1. Preﬁll latency is for processing 2048 tokens;\n",
      "decode latency is for generating 64 tokens. Feedforward network\n",
      "(FFN) layouts ...\n",
      "No images found on page 10\n",
      "Page 11 Text: Efﬁciently Scaling Transformer Inference\n",
      "FasterTransformer reports results with 8-, 16-, and 32-way\n",
      "tensor parallelism. Their 32-way tensor parallelism achieves\n",
      "a maximum of 33% MFU across all reported benchmarks,\n",
      "compared to 46% MFU in their 16-way tensor parallel con-\n",
      "ﬁguration. This likely indicates a communication bottleneck\n",
      "of scaling tensor parallelism beyond this point. In contrast,\n",
      "our implementation is able to scale up to 64-way tensor par-\n",
      "allelism while still achieving 44% MFU, sugges...\n",
      "No images found on page 11\n",
      "Page 12 Text: Efﬁciently Scaling Transformer Inference\n",
      "REFERENCES\n",
      "T5x, 2021. URL https://github.com/google-\n",
      "research/t5x.\n",
      "AmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal,\n",
      "Jonathan Malmaud, Oleg Rybakov, Chas Leichner, and\n",
      "Lukasz Lew. Pareto-optimal quantized resnet is mostly\n",
      "4-bit. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition, pages 3091–\n",
      "3099, 2021.\n",
      "Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\n",
      "Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\n",
      "Zheng, Jeff Rasley, S...\n",
      "No images found on page 12\n",
      "Page 13 Text: Efﬁciently Scaling Transformer Inference\n",
      "Manish Gupta and Puneet Agrawal. Compression of deep\n",
      "learning models for text: A survey.\n",
      "arXiv preprint\n",
      "arXiv:2008.05221, 2020.\n",
      "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\n",
      "Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\n",
      "de Las Casas, Lisa Anne Hendricks, Johannes Welbl,\n",
      "Aidan Clark, Tom Hennigan, Eric Noland, Katie Milli-\n",
      "can, George van den Driessche, Bogdan Damoc, Aurelia\n",
      "Guy, Simon Osindero, Karen Simonyan, Erich Elsen,\n",
      "Jack W. Rae, Orio...\n",
      "No images found on page 13\n",
      "Page 14 Text: Efﬁciently Scaling Transformer Inference\n",
      "Conﬁdent adaptive language modeling. arXiv preprint\n",
      "arXiv:2207.07061, 2022.\n",
      "Noam Shazeer. Fast transformer decoding: One write-head\n",
      "is all you need. arXiv preprint arXiv:1911.02150, 2019.\n",
      "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy\n",
      "Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Out-\n",
      "rageously large neural networks: The sparsely-gated\n",
      "mixture-of-experts layer.\n",
      "In ICLR (Poster). OpenRe-\n",
      "view.net, 2017. URL http://dblp.uni-trier\n",
      ".de/db/conf/icl...\n",
      "No images found on page 14\n",
      "Page 15 Text: Efﬁciently Scaling Transformer Inference\n",
      "A\n",
      "PARTITIONING STRATEGIES:\n",
      "DERIVING COMMUNICATION COSTS\n",
      "A.1\n",
      "Cost of all-gather/reduce-scatter\n",
      "Figure A.1 shows typical collective operations we use in par-\n",
      "titioning strategies and their communication patterns across\n",
      "three devices. For an all-gather over K partitions, where\n",
      "each chip produces an output of size D, the communication\n",
      "pattern requires chunks of size D\n",
      "K to be transferred over\n",
      "(K −1) interconnect links in the process of getting copied\n",
      "to (K −1...\n",
      "No images found on page 15\n",
      "Page 16 Text: Efﬁciently Scaling Transformer Inference\n",
      "Figure A.1: Communication patterns of collective operations: all-gather, reduce-scatter, and all-to-all across three devices.\n",
      "B\n",
      "MINIMUM PREFILL LATENCY\n",
      "We report here the minimum latency required for preﬁll.\n",
      "Figure B.1 shows the Pareto frontier of cost vs. latency as\n",
      "we sweep sequence length from 32 to 1024 at batch size 1.\n",
      "0.002\n",
      "0.004\n",
      "0.008\n",
      "0.016\n",
      "0.032\n",
      "0.064\n",
      "0.128\n",
      "0.256\n",
      "0.512\n",
      "Latency per Forward Pass (Seconds)\n",
      "0.0625\n",
      "0.125\n",
      "0.25\n",
      "0.5\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "C...\n",
      "Extracted 1 images from page 16\n",
      "Page 17 Text: Efﬁciently Scaling Transformer Inference\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "Latency per Generated Token (Milliseconds)\n",
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Model FLOPs Utilization\n",
      "C:64, B:1024\n",
      "C:8, B:1024\n",
      "C:16, B:32\n",
      "Decoding Latency vs. MFU\n",
      "PaLM 540B\n",
      "PaLM 540B-int8\n",
      "PaLM 62B\n",
      "PaLM 62B-int8\n",
      "PaLM 8B\n",
      "PaLM 8B-int8\n",
      "0.03 0.06 0.12 0.25\n",
      "0.5\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "Latency per Forward Pass (Seconds)\n",
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Model FLOPs Utilization\n",
      "C:128, B:256\n",
      "C:64, B:64\n",
      "C:8, B:4\n",
      "Prefil...\n",
      "No images found on page 17\n",
      "Page 18 Text: Efﬁciently Scaling Transformer Inference\n",
      "FasterTransformer MT-NLG 530B total\n",
      "Ours (530B/540B on 64 TPU v4 with 2D partitioning)\n",
      "TP16\n",
      "TP32\n",
      "PP3/TP8\n",
      "PaLM preﬁll\n",
      "PaLM generate\n",
      "PaLM total\n",
      "MT-NLG total\n",
      "batch\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "time\n",
      "MFU\n",
      "1\n",
      "585\n",
      "5%\n",
      "451\n",
      "3%\n",
      "866\n",
      "2%\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "2\n",
      "667\n",
      "9%\n",
      "508\n",
      "6%\n",
      "932\n",
      "4%\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "4\n",
      "765\n",
      "15%\n",
      "606\n",
      "10%\n",
      "1097\n",
      "7%\n",
      "81\n",
      "39%\n",
      "258\n",
      "1%\n",
      "343\n",
      "10%\n",
      "338\n",
      "10%\n",
      "8\n",
      "990\n",
      "23%\n",
      "766\n",
      "15%\n",
      "1434\n",
      "11%\n",
      "149\n",
      "42%\n",
      "234\n",
      "2%\n",
      "403\n",
      "17%\n",
      "384\n",
      "16%\n",
      "16\n",
      "1377\n",
      "34%\n",
      "1074\n",
      "22%\n",
      "2104\n",
      "15%\n",
      "287\n",
      "44%\n",
      "253\n",
      "3%\n",
      "586\n",
      "23...\n",
      "No images found on page 18\n",
      "Finished processing 2211.05102.pdf\n",
      "Processing: 2310.06825.pdf\n",
      "Total pages: 9\n",
      "Page 1 Text: Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (...\n",
      "Extracted 1 images from page 1\n",
      "Page 2 Text: Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "m...\n",
      "Extracted 1 images from page 2\n",
      "Page 3 Text: Figure 2: Rolling buffer cache. The cache has a fixed size of W = 4. Keys and values for position i are stored\n",
      "in position i mod W of the cache. When the position i is larger than W, past values in the cache are overwritten.\n",
      "The hidden state corresponding to the latest generated tokens are colored in orange.\n",
      "Pre-fill and Chunking. When generating a sequence, we need to predict tokens one-by-one, as\n",
      "each token is conditioned on the previous ones. However, the prompt is known in advance, and we\n",
      "ca...\n",
      "Extracted 1 images from page 3\n",
      "Page 4 Text: Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks. All\n",
      "models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\n",
      "significantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\n",
      "34B in mathematics, code generation, and reasoning benchmarks.\n",
      "Model\n",
      "Modality MMLU HellaSwag WinoG PIQA\n",
      "Arc-e\n",
      "Arc-c\n",
      "NQ\n",
      "TriviaQA HumanEval MBPP MATH GSM8K\n",
      "LLaMA 2 7B\n",
      "Pretrained 44.4%\n",
      "77.1%...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\n",
      "Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\n",
      "on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\n",
      "amount of knowledge it can compress).\n",
      "5\n",
      "Adding guardrails for front-facing applications\n",
      "The ability to enforce guardrails when it comes to AI generation is important for front-facing a...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: Model\n",
      "Answer\n",
      "Mistral 7B – Instruct\n",
      "with Mistral system prompt\n",
      "To kill a Linux process, you can use the `kill`command followed by the process ID (PID)\n",
      "of the process you want to terminate. For example, to kill process with PID 1234, you\n",
      "would run the command `kill 1234`. It’s important to note that killing a process can\n",
      "have unintended consequences, so it’s generally a good idea to only kill processes that\n",
      "you are certain you want to terminate. Additionally, it’s generally a good idea to use the\n",
      "...\n",
      "No images found on page 6\n",
      "Page 7 Text: Figure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of\n",
      "human evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics.\n",
      "Llama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more\n",
      "relevant book on quantum physics and describes in the contents in more detail.\n",
      "7\n",
      "...\n",
      "Extracted 1 images from page 7\n",
      "Page 8 Text: References\n",
      "[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and\n",
      "Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head\n",
      "checkpoints. arXiv preprint arXiv:2305.13245, 2023.\n",
      "[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\n",
      "Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\n",
      "language models. arXiv preprint arXiv:2108.07732, 2021.\n",
      "[3] Iz Beltagy, Matt...\n",
      "No images found on page 8\n",
      "Page 9 Text: [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\n",
      "Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-\n",
      "guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\n",
      "on Operating Systems Principles, 2023.\n",
      "[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\n",
      "Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\n",
      "xformers: A...\n",
      "No images found on page 9\n",
      "Finished processing 2310.06825.pdf\n",
      "Processing: 2306.07629.pdf\n",
      "Total pages: 21\n",
      "Page 1 Text: SQUEEZELLM: DENSE-AND-SPARSE QUANTIZATION\n",
      "Sehoon Kim∗1\n",
      "Coleman Hooper∗1\n",
      "Amir Gholami∗†12\n",
      "Zhen Dong1\n",
      "Xiuyu Li1\n",
      "Sheng Shen1\n",
      "Michael W. Mahoney123\n",
      "Kurt Keutzer1\n",
      "1 UC Berkeley\n",
      "2 ICSI\n",
      "3 LBNL\n",
      "{sehoonkim, chooper, amirgh, zhendong, xiuyu, sheng.s,\n",
      "mahoneymw, keutzer}@berkeley.edu\n",
      "ABSTRACT\n",
      "Generative Large Language Models (LLMs) have demonstrated remarkable re-\n",
      "sults for a wide range of tasks. However, deploying these models for inference\n",
      "has been a significant challenge due to their unprecedented resou...\n",
      "No images found on page 1\n",
      "Page 2 Text: RTN\n",
      "SqueezeLLM\n",
      "28.26\n",
      "18.08\n",
      "7.75\n",
      "7.67\n",
      "7.56\n",
      "SOTA Uniform \n",
      "Quantization\n",
      "Non\n",
      "uniform\n",
      "D&S\n",
      "Sensitive \n",
      "values\n",
      "D&S\n",
      "Outlier\n",
      "values\n",
      "Sensitivity\n",
      "Based\n",
      "SQ-7B-3bit\n",
      "SQ-7B-4bit\n",
      "SQ-13B-3bit\n",
      "SQ-13B-4bit\n",
      "SQ-30B-3bit\n",
      "SQ-30B-4bit\n",
      "7B\n",
      "13B\n",
      "30B\n",
      "65B\n",
      "SQ-65B-3bit\n",
      "SQ-65B-4bit\n",
      "Same Size\n",
      "0.85 better PPL\n",
      "Same Size\n",
      "0.77 better PPL\n",
      "Figure 1: (Left) SqueezeLLM incorporates two key approaches: (i) sensitivity-based non-uniform\n",
      "quantization (Sec. 4.1), where quantization bins are allocated closer to sensitive values, and (ii) the\n",
      "...\n",
      "No images found on page 2\n",
      "Page 3 Text: it to the Vicuna models Chiang et al. (2023) on the MMLU benchmark Hendrycks et al. (2021)\n",
      "and the Vicuna benchmark Chiang et al. (2023) (Sec. 5.3). Furthermore, our deployed models on\n",
      "A6000 GPUs also exhibit significant gains in latency of up to 2.4× compared to the FP16 base-\n",
      "line, showcasing the effectiveness of our method in terms of both quantization performance and\n",
      "inference efficiency (Sec. 5.4).\n",
      "2\n",
      "RELATED WORK\n",
      "In Sec. A.1, we offer an overview and related works of Transformer quantizatio...\n",
      "No images found on page 3\n",
      "Page 4 Text: 16-bit\n",
      "8-bit\n",
      "4-bit\n",
      "Weight Bit Precision\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Normalized Runtime\n",
      "Sequence Length 128\n",
      "16-bit\n",
      "8-bit\n",
      "4-bit\n",
      "Weight Bit Precision\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Sequence Length 2048\n",
      "Nonlinear Operations\n",
      "MHA Act-to-Act Matmuls\n",
      "MHA FC Layers\n",
      "FFN FC Layers\n",
      "Figure 2: Normalized runtime for LLaMA-7B when reducing the bit precision for the weights with\n",
      "sequence lengths of 128 (left) and 2048 (right). Results were obtained using a roofline-based perfor-\n",
      "mance model for an A5000 GPU. Reducing only...\n",
      "No images found on page 4\n",
      "Page 5 Text: Figure 3: (Left) The weight distribution of one output channel in LLaMA-7B. The top-20 most sen-\n",
      "sitive values are marked in red. (Right) Weight distributions after 3-bit quantization using uniform\n",
      "and sensitivity-based non-uniform quantization. In the latter case, the quantized values are more\n",
      "clustered around the sensitive values.\n",
      "where W denotes the weights and WQ is the corresponding quantized weights (i.e., [Q(w) for\n",
      "w ∈W]), represented by k distinct values {q1, · · · , qk}. Here, the optim...\n",
      "No images found on page 5\n",
      "Page 6 Text: 0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "Layer\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Absolute weight value\n",
      "Output Proj. Weight Distribution\n",
      "percentile\n",
      "100%\n",
      "99.99%\n",
      "99.9%\n",
      "99%\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "Layer\n",
      "Down Proj. Weight Distribution\n",
      "percentile\n",
      "100%\n",
      "99.99%\n",
      "99.9%\n",
      "99%\n",
      "Figure 4: The distributions of the (normalized) absolute weight values, for the output layers in MHA\n",
      "and the down layers in FFN across different layers in LLaMA-7B. Note that the distributions exhibit\n",
      "outlier patterns across all layers, with 99% of the values clustered with...\n",
      "No images found on page 6\n",
      "Page 7 Text: To efficiently process our Dense-and-Sparse representation, we also develop CUDA kernels for\n",
      "sparse matrix-vector multiplication that load a matrix in CSR format and a dense activation vector,\n",
      "inspired by Evtushenko (2019). Since the non-zero entry distributions are highly skewed across\n",
      "rows (Sec. A.3), assigning a single thread per row can be inefficient due to the unbalanced amount\n",
      "of work assigned to different threads. Thus, we implement balanced hybrid kernels based on Flegar\n",
      "& Quintana-Ort´...\n",
      "No images found on page 7\n",
      "Page 8 Text: Table 1: Perplexity comparison of LLaMA models quantized into 3 and 4 bits using different methods including\n",
      "RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2. We compare the performance of GPTQ, AWQ, and\n",
      "SqueezeLLM in groups based on similar model sizes. In the first group, we compare dense-only SqueezeLLM\n",
      "with non-grouped GPTQ. In the second group, we compare SqueezeLLM with a sparsity level of 0.45% to\n",
      "GPTQ and AWQ with a group size of 128. We add speedup and peak memory usage numbers for comparis...\n",
      "No images found on page 8\n",
      "Page 9 Text: Table 2: Comparison of PTQ methods on zero-shot MMLU accuracy applied to Vicuna v1.1. We\n",
      "add speedup and peak memory usage for comparison.\n",
      "Method\n",
      "Avg. bit\n",
      "7B\n",
      "13B\n",
      "Acc (↑)\n",
      "Speedup (↑)\n",
      "Mem (GB, ↓)\n",
      "Acc (↑)\n",
      "Speedup (↑)\n",
      "Mem (GB, ↓)\n",
      "Baseline\n",
      "16\n",
      "39.1%\n",
      "1×\n",
      "12.7\n",
      "41.2%\n",
      "1×\n",
      "24.6\n",
      "AWQ (g128)\n",
      "4.25\n",
      "38.0%\n",
      "1.6×\n",
      "3.8\n",
      "40.4%\n",
      "1.9×\n",
      "7.2\n",
      "SqLLM\n",
      "4.05\n",
      "38.8%\n",
      "1.8×\n",
      "3.8\n",
      "39.2%\n",
      "2.0×\n",
      "6.9\n",
      "SqLLM (0.45%)\n",
      "4.26\n",
      "39.4%\n",
      "1.7×\n",
      "4.0\n",
      "41.0%\n",
      "1.9×\n",
      "7.3\n",
      "AWQ (g128)\n",
      "3.25\n",
      "36.5%\n",
      "2.0×\n",
      "3.0\n",
      "37.6%\n",
      "2.2×\n",
      "5.7\n",
      "SqLLM\n",
      "3.02\n",
      "36.0%\n",
      "2.1×\n",
      "2.9\n",
      "37.2%\n",
      "2.4×\n",
      "5.4\n",
      "SqLLM (0.45...\n",
      "No images found on page 9\n",
      "Page 10 Text: Table 3: Latency (s) and peak memory usage (GB) of 3-bit LLaMA when generating 128 tokens\n",
      "on an A6000 GPU. The table compares the FP16 baseline, non-grouped and grouped GPTQ with\n",
      "activation ordering, and SqueezeLLM with different sparsity levels. For comparison, we include\n",
      "bitwidth and perpelxity on the C4 benchmark.\n",
      "Method\n",
      "Bit\n",
      "7B\n",
      "13B\n",
      "30B\n",
      "65B\n",
      "width\n",
      "PPL (C4)\n",
      "Lat (s)\n",
      "Mem (G)\n",
      "PPL (C4)\n",
      "Lat (s)\n",
      "Mem (G)\n",
      "PPL (C4)\n",
      "Lat (s)\n",
      "Mem (G)\n",
      "PPL (C4)\n",
      "Lat (s)\n",
      "Mem (G)\n",
      "Baseline\n",
      "16\n",
      "7.08\n",
      "3.2\n",
      "12.7\n",
      "6.61\n",
      "5.6\n",
      "24.6\n",
      "5.98\n",
      "OOM\n",
      "...\n",
      "No images found on page 10\n",
      "Page 11 Text: REFERENCES\n",
      "Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\n",
      "King. BinaryBERT: Pushing the limit of BERT quantization. arXiv preprint arXiv:2012.15701,\n",
      "2020.\n",
      "Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the\n",
      "challenges of efficient Transformer quantization. arXiv preprint arXiv:2109.12948, 2021.\n",
      "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Prana...\n",
      "No images found on page 11\n",
      "Page 12 Text: Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.\n",
      "A survey of quantization methods for efficient neural network inference.\n",
      "arXiv preprint\n",
      "arXiv:2103.13630, 2021a.\n",
      "Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. AI and Memory\n",
      "Wall. https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8, 2021b.\n",
      "GPTQ-For-LLaMA. https://github.com/qwopqwop200/gptq-for-llama.\n",
      "Babak Hassibi and David G Stork. Second order derivatives for network prunin...\n",
      "No images found on page 12\n",
      "Page 13 Text: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n",
      "Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\n",
      "transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\n",
      "Castagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, et al. Bloom: A 176b-\n",
      "parameter open-access multilingual lan...\n",
      "No images found on page 13\n",
      "Page 14 Text: Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. TernaryBERT:\n",
      "Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.\n",
      "Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yandong Guo, Kurt\n",
      "Keutzer, Li Du, and Shanghang Zhang. Qd-bev: Quantization-aware view-guided distillation\n",
      "for multi-view 3d object detection. 2023.\n",
      "Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network\n",
      "quantization without...\n",
      "No images found on page 14\n",
      "Page 15 Text: A\n",
      "APPENDIX\n",
      "A.1\n",
      "RELATED WORKS ON QUANTIZATION OF TRANSFORMER-BASED MODELS\n",
      "Quantization methods can be broadly categorized based whether retraining is required or not Gho-\n",
      "lami et al. (2021a). Quantization-Aware Training (QAT) requires retraining the model to adapt its\n",
      "weights to help recover accuracy after quantization Zafrir et al. (2019); Shen et al. (2020); Kim et al.\n",
      "(2021); Zhang et al. (2023; 2020); Bai et al. (2020), whereas Post-Training Quantization (PTQ) does\n",
      "not involve retraining Zhao...\n",
      "No images found on page 15\n",
      "Page 16 Text: ing 3-bit SpQR models, which average around 4 bits, and our 4-bit models, both of which possess\n",
      "similar model sizes.\n",
      "Latency Profiling. We measure the latency and peak memory usage for generating 128 and 1024\n",
      "tokens on an A6000 machine using the Torch CUDA profiler. As an official implementation of\n",
      "GPTQ (in particular, the grouped version) is not available, we implement an optimized kernel for\n",
      "single-batch inference based on the most active open-source codebase ( GPTQ-For-LLaMA).\n",
      "To compare late...\n",
      "No images found on page 16\n",
      "Page 17 Text: a few channels containing a much larger proportion of nonzero values. This skewed distribution\n",
      "makes it challenging to efficiently perform computations using the sparse matrix, as it is difficult to\n",
      "distribute the nonzero elements evenly across parallel processing units. This motivates our modified\n",
      "kernel for handling channels with a large number of outliers in order to reduce the runtime overhead\n",
      "of the sparse matrices. As outlined in Tab. A.1, we observed over 100% added runtime overhead\n",
      "when ...\n",
      "No images found on page 17\n",
      "Page 18 Text: the sensitive values are removed. In both scenarios, we control the sparsity level by increasing the\n",
      "percentage of outlier values included in the sparse matrix to obtain the trade-off curves. The results\n",
      "indicate that the sparsity configuration with both sensitive values and outlier values consistently\n",
      "outperforms the configuration with only outlier values.\n",
      "A.4.3\n",
      "IMPACT OF GROUPING ON SQUEEZELLM\n",
      "0.190\n",
      "0.192\n",
      "0.194\n",
      "0.196\n",
      "0.198\n",
      "0.200\n",
      "0.202\n",
      "Model Size\n",
      "7.60\n",
      "7.61\n",
      "7.62\n",
      "7.63\n",
      "7.64\n",
      "7.65\n",
      "7.66\n",
      "7.67\n",
      "Perplexi...\n",
      "No images found on page 18\n",
      "Page 19 Text: 0.190\n",
      "0.192\n",
      "0.194\n",
      "0.196\n",
      "0.198\n",
      "0.200\n",
      "0.202\n",
      "Model Size\n",
      "7.6\n",
      "7.7\n",
      "7.8\n",
      "7.9\n",
      "8.0\n",
      "8.1\n",
      "Perplexity on C4\n",
      "LLaMA-7B 3bit\n",
      "OBS\n",
      "OBD (Ours)\n",
      "Figure A.4: Model size (normalized by the size of the FP16 model) and perplexity trade-offs for 3-\n",
      "bit quantization of the LLaMA-7B model using the Optimal Brain Surgeon (OBS) framework versus\n",
      "the Optimal Brain Damage (OBD) framework for determining the non-uniform quantization con-\n",
      "figuration. The trade-off is obtained by adjusting the sparsity level of the outliers being e...\n",
      "No images found on page 19\n",
      "Page 20 Text: Table A.4: Perplexity comparison of LLaMA-30B and 65B models quantized into 4 and 3 bits using\n",
      "different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2. We compare the\n",
      "performance of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes.\n",
      "In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ. In the subsequent\n",
      "groups, we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different\n",
      "group sizes.\n",
      "LLaMA-30B\n",
      "3-bit\n",
      "4-bit\n",
      "Met...\n",
      "No images found on page 20\n",
      "Page 21 Text: Table A.6: Perplexity comparison of OPT models quantized into 4 and 3 bits using different meth-\n",
      "ods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2. We compare the performance\n",
      "of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes. In the first\n",
      "group, we compare dense-only SqueezeLLM with non-grouped GPTQ. In the subsequent groups,\n",
      "we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different group\n",
      "sizes. Note that all GPTQ results are with a...\n",
      "No images found on page 21\n",
      "Finished processing 2306.07629.pdf\n",
      "Processing: 2305.07185.pdf\n",
      "Total pages: 16\n",
      "Page 1 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Lili Yu * 1 D´aniel Simig * 1 Colin Flaherty * 2 Armen Aghajanyan 1 Luke Zettlemoyer 1 Mike Lewis 1\n",
      "Abstract\n",
      "Autoregressive transformers are spectacular mod-\n",
      "els for short sequences but scale poorly to long se-\n",
      "quences such as high-resolution images, podcasts,\n",
      "code, or books. We propose MEGABYTE, a multi-\n",
      "scale decoder architecture that enables end-to-end\n",
      "differentiable modeling of sequences of over one\n",
      "million bytes. MEGAB...\n",
      "No images found on page 1\n",
      "Page 2 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "els, more than 98% of FLOPS are used in comput-\n",
      "ing position-wise feedforward layers. MEGABYTE\n",
      "uses large feedforward layers per-patch rather than per-\n",
      "position, enabling much larger and more expressive\n",
      "models for the same cost. With patch size P, where a\n",
      "baseline transformer would use the same feedforward\n",
      "layer with m parameters P times, MEGABYTE can use\n",
      "a layer with mP parameters once for the same cost.\n",
      "3. Parallelism in ...\n",
      "No images found on page 2\n",
      "Page 3 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "hembed\n",
      "t\n",
      "= Eglobal-embed\n",
      "xt\n",
      "+ Epos\n",
      "t\n",
      "t ∈[0..T), Eglobal-embed ∈RV ×DG,\n",
      "Epos ∈RT ×DG, hembed ∈RT ×DG\n",
      "hglobal-in\n",
      "k\n",
      "=\n",
      "(\n",
      "Eglobal-pad,\n",
      "if k = 0,\n",
      "hembed\n",
      "((k−1)·P ):(k·P ),\n",
      "k ∈[1, .., K),\n",
      "Eglobal-pad ∈RP ×DG, K = T\n",
      "P\n",
      "hglobal-out\n",
      "0:K\n",
      "= transformerglobal(hglobal-in\n",
      "0:K\n",
      ")\n",
      "hglobal-out ∈RK×P ·DG, hglobal-in ∈RK×P ·DG\n",
      "hlocal-in\n",
      "k,p\n",
      "= wGLhglobal-out\n",
      "k,(p·DG):((p+1)·DG) +\n",
      "(\n",
      "Elocal-pad,\n",
      "if p = 0\n",
      "Elocal-embed\n",
      "x(k·P +p−1),\n",
      "p ∈[1, .., P)\n",
      "Eloc...\n",
      "No images found on page 3\n",
      "Page 4 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "with the Global model alone, which would resemble a de-\n",
      "coder version of ViT (Dosovitskiy et al., 2020). However,\n",
      "the joint distribution over the patch p(xt+1, .., xt+P |x0..t)\n",
      "has an output space of size 256P so direct modeling is only\n",
      "tractable for very small patches. We could instead factor\n",
      "the joint distribution into conditionally independent distri-\n",
      "butions p(xt+1|x0..t)..p(xt+P |x0..t), but this would greatly\n",
      "limit th...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "token1, where D is the model embedding dimension. Fig-\n",
      "ure 3 shows that for models of size 660M to 173B and se-\n",
      "quence lengths of up to 1M tokens, MEGABYTE with P = 8\n",
      "uses less FLOPS than either transformers or Linear Trans-\n",
      "formers. Baseline model architectures are based on GPT3,\n",
      "and Megabyte global/local model sizes are 452M/151M,\n",
      "5.8B/604M, 170B/3.2B respectively.\n",
      "3.2. Generation Efﬁciency\n",
      "Generating long sequences with ...\n",
      "No images found on page 5\n",
      "Page 6 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "PG-19\n",
      "Stories\n",
      "Books\n",
      "arXiv\n",
      "Code\n",
      "Transformer\n",
      "1.057\n",
      "1.064\n",
      "1.097\n",
      "0.816\n",
      "0.575\n",
      "PerceiverAR\n",
      "1.104\n",
      "1.070\n",
      "1.104\n",
      "0.791\n",
      "0.546\n",
      "MEGABYTE\n",
      "1.000\n",
      "0.978\n",
      "1.007\n",
      "0.678\n",
      "0.411\n",
      "Table 2. Performance (bits-per-byte) of compute and data con-\n",
      "trolled MEGABYTE, PerceiverAR, and Transformer models on\n",
      "various text modalities.\n",
      "in LATEX from the arXiv online archive. Finally, the Code\n",
      "dataset is a large publicly available dataset of open source\n",
      "code, unde...\n",
      "No images found on page 6\n",
      "Page 7 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Tokenizer\n",
      "Vocab Size\n",
      "Context Length\n",
      "Validation\n",
      "Test\n",
      "TransformerXL (Rae et al., 2019a)\n",
      "SentencePiece\n",
      "32k\n",
      "512+1024 (subwords)\n",
      "45.5\n",
      "36.3\n",
      "CompressiveTransformer (Rae et al., 2019a)\n",
      "SentencePiece\n",
      "32k\n",
      "512+512+2x512 (subwords)\n",
      "43.4\n",
      "33.6\n",
      "PerceiverAR (Hawthorne et al., 2022)\n",
      "SentencePiece\n",
      "32k\n",
      "2048 (subwords)\n",
      "45.9\n",
      "28.9\n",
      "BlockRecurrent (Hutchins et al., 2022)\n",
      "SentencePiece\n",
      "32k\n",
      "1024+recurrence (subwords)\n",
      "-\n",
      "26.5\n",
      "Transformer byte-level (o...\n",
      "No images found on page 7\n",
      "Page 8 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Figure 4. Average log probability assigned to the token at different\n",
      "positions within the context length by MEGABYTE model with\n",
      "8192 context size and by a vanilla transformer model trained using\n",
      "the same compute (PG19 test set). MEGABYTE likelihoods rise\n",
      "throughout its context window, demonstrating that it can use tokens\n",
      "from 8k bytes previously to improve its predictions.\n",
      "8.2. Model Components\n",
      "In Table 7, we analyze the si...\n",
      "Extracted 2 images from page 8\n",
      "Page 9 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Global Size\n",
      "Local Size\n",
      "bpb\n",
      "350M (D=1024,L=24)\n",
      "290M (D=1024,L=20)\n",
      "1.014\n",
      "760M (D=1536,L=24)\n",
      "262M (D=1024,L=18)\n",
      "1.002\n",
      "1.3B (D=2048,L=24)\n",
      "218M (D=1024,L=15)\n",
      "0.991\n",
      "Table 10. Effects of Local / Global model size on performance\n",
      "on the PG19 dataset. Increasing the capacity of global model\n",
      "improves performance. Models are compute and data matched.\n",
      "Local to Global model Size Ratio. We experimented with\n",
      "different Local/Global model si...\n",
      "No images found on page 9\n",
      "Page 10 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "References\n",
      "Baines, M., Bhosale, S., Caggiano, V., Goyal, N., Goyal,\n",
      "S., Ott, M., Lefaudeux, B., Liptchinsky, V., Rabbat, M.,\n",
      "Sheiffer, S., Sridhar, A., and Xu, M. FairScale: A gen-\n",
      "eral purpose modular PyTorch library for high perfor-\n",
      "mance and large scale training. https://github.\n",
      "com/facebookresearch/fairscale, 2021.\n",
      "Beltagy, I., Peters, M. E., and Cohan, A.\n",
      "Long-\n",
      "former: The long-document transformer. arXiv preprint\n",
      "arXi...\n",
      "No images found on page 10\n",
      "Page 11 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\n",
      "efﬁcient transformer. arXiv preprint arXiv:2001.04451,\n",
      "2020.\n",
      "Kudo, T. and Richardson, J.\n",
      "Sentencepiece:\n",
      "A sim-\n",
      "ple and language independent subword tokenizer and\n",
      "detokenizer for neural text processing. arXiv preprint\n",
      "arXiv:1808.06226, 2018.\n",
      "Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May,\n",
      "J., and Zettlemoyer, L. Mega: moving average equipped\n",
      "gated attention...\n",
      "No images found on page 11\n",
      "Page 12 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Mem-\n",
      "orizing transformers. arXiv preprint arXiv:2203.08913,\n",
      "2022.\n",
      "Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\n",
      "Chen, S., Dewan, C., Diab, M., Li, X., Lin, V., Mihaylov,\n",
      "T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura,\n",
      "S., Sridhar, A., Wang, T., Zettlemoyer, L., and Ai, M.\n",
      "OPT: Open Pre-trained Transformer Language Models.\n",
      "5 2022a. doi: 10.48550/arxiv.22...\n",
      "No images found on page 12\n",
      "Page 13 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "A. Appendices\n",
      "A.1. Training Details\n",
      "To ensure stable training, we applied gradient clipping with a maximum norm of 1.0 and used the Adam optimizer with\n",
      "β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2015). We used the built-in polynomial decay learning rate scheduler in MetaSeq with\n",
      "500 warmup updates and the end learning rate set to 0. All models are trained with pre-norm and using ReLU activation.\n",
      "We apply a dropout of 0.1 throughout,...\n",
      "No images found on page 13\n",
      "Page 14 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Model\n",
      "(Global) Size\n",
      "Local Size\n",
      "BS\n",
      "LR\n",
      "Context Length (in bytes)\n",
      "arXiv\n",
      "Transformer\n",
      "320M (D=1024, L=22)\n",
      "N/A\n",
      "72\n",
      "2.00E-04\n",
      "1,024\n",
      "Perceiver AR\n",
      "248M (D=1024, L=17)\n",
      "N/A\n",
      "72\n",
      "2.00E-04\n",
      "8,192 (1024 latents)\n",
      "MEGABYTE\n",
      "758M (D=2048, L=14)\n",
      "262M (D=1024, L=18)\n",
      "48\n",
      "2.00E-04\n",
      "8,192 (patch size 8)\n",
      "w/o Local model\n",
      "2.3B (D=2560, L=20)\n",
      "N/A\n",
      "48\n",
      "1.50E-04\n",
      "8,192 (patch size 4)\n",
      "w/o global model\n",
      "N/A\n",
      "350M (D=1024, L=24)\n",
      "192\n",
      "2.00E-04\n",
      "8,192 (patch size 8)\n",
      "w/o ...\n",
      "No images found on page 14\n",
      "Page 15 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "def forward(\n",
      "self,\n",
      "bytes,\n",
      "):\n",
      "bytes_global, bytes_local = self.prepare_input(bytes)\n",
      "global_bytes_embedded = self.globalmodel.embed(bytes_global)\n",
      "global_in = rearrange(\n",
      "global_bytes_embedded,\n",
      "\"b (t p) e -> b t (p e)\",\n",
      "p=self.patch_size,\n",
      ")\n",
      "global_output = self.globalmodel(global_in)\n",
      "global_output_reshaped = rearrange(\n",
      "global_output,\n",
      "\"b t (p e) -> (b t) p e\",\n",
      "p=self.patch_size,\n",
      ")\n",
      "local_bytes_embedded = self.localmodel.embed(byt...\n",
      "No images found on page 15\n",
      "Page 16 Text: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
      "Figure 6. Two ways to model 2D data sequentially. Left, raster scan, by taking bytes row by row and left to right; right, patch scan, where\n",
      "we ﬁrst split an image into patches, and do raster scan across patches and within a patch. (T=36, K=9, P=4).\n",
      "D.2. Patch scan vs Raster scan\n",
      "The patch scan method is inspired by recent works in Vision Transformers (Dosovitskiy et al., 2020), and it is more effective\n",
      "than raster scan for ...\n",
      "No images found on page 16\n",
      "Finished processing 2305.07185.pdf\n",
      "Processing: 2306.12929.pdf\n",
      "Total pages: 30\n",
      "Page 1 Text: Quantizable Transformers: Removing Outliers by\n",
      "Helping Attention Heads Do Nothing\n",
      "Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort\n",
      "Qualcomm AI Research∗\n",
      "Amsterdam, The Netherlands\n",
      "{ybond, markusn, tijmen}@qti.qualcomm.com\n",
      "Abstract\n",
      "Transformer models have been widely adopted in various domains over the last\n",
      "years, and especially large language models have advanced the field of AI signif-\n",
      "icantly. Due to their size, the capability of these networks has increased tremen-\n",
      "dously, but this has co...\n",
      "No images found on page 1\n",
      "Page 2 Text: any post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work\n",
      "has found the existence of these outliers [4, 13], but in our work, we come to a fuller understanding\n",
      "of these outlying values. We find that the outliers occur because attention heads are trying not to\n",
      "update the hidden state, and in the process, strong outliers appear due to the softmax function. This\n",
      "happens for language and vision transformers and different specific transformer architectures. This\n",
      "un...\n",
      "No images found on page 2\n",
      "Page 3 Text: (a) FFN output in layer #10\n",
      "(b) FFN output in layer #11\n",
      "Figure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green),\n",
      "recorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions.\n",
      "channel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different\n",
      "numeric format to represent those outliers better or require extra fine-tuning (in the form of QAT\n",
      "and/or knowledge distillation). In other words, they...\n",
      "Extracted 4 images from page 3\n",
      "Page 4 Text: (a) Attention layer #11, data sequence #1\n",
      "(b) Attention layer #11, data sequence #5\n",
      "(c) Attention layer #10, data sequence #5\n",
      "Figure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities,\n",
      "values, and their product (left, middle and right columns, respectively), in attention head #3 for\n",
      "BERT-base, computed on several data sequences from MNLI-m validation set.\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "(e)\n",
      "Figure 3: A summary of our outlier analysis for ViT demonstrated on a random...\n",
      "Extracted 14 images from page 4\n",
      "Page 5 Text: 2. From the definition of the softmax function4, it is easy to see that this would require an input of\n",
      "the softmax to have a relatively big dynamic range (Figure 4, 1 ). In fact, in the limit case where\n",
      "softmax is exactly zero, this would require an infinite dynamic range:\n",
      "softmax (x)i = 0\n",
      "⇔\n",
      "∃j ̸= i, xj −xi = +∞\n",
      "(2)\n",
      "3. Since Layer Normalization ([1], 2 ) normalizes the outliers, the magnitude of the FFN output in\n",
      "the previous layer ( 3 ) has to be very high to still produce a sufficiently big dy...\n",
      "Extracted 1 images from page 5\n",
      "Page 6 Text: 4.2\n",
      "Gated attention\n",
      "An alternative way of architecting the model to have a small attention output without outliers is to\n",
      "equip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the\n",
      "model can use the gating to either keep or nullify the update to the representation of certain tokens\n",
      "and not rely on the attention probabilities and values to achieve the same outcome.\n",
      "Specifically, we propose the following modification to the attention function:\n",
      "Gated_attention...\n",
      "Extracted 1 images from page 6\n",
      "Page 7 Text: γ\n",
      "ζ\n",
      "FP16 ppl.↓\n",
      "Max inf. norm\n",
      "Avg. kurtosis\n",
      "W8A8 ppl.↓\n",
      "0\n",
      "1\n",
      "4.49±0.01\n",
      "735±55\n",
      "3076±262\n",
      "1294±1046\n",
      "(= Vanilla)\n",
      "0\n",
      "1.003\n",
      "4.48±0.01\n",
      "715±335\n",
      "2159±238\n",
      "451±57\n",
      "0\n",
      "1.03\n",
      "4.49±0.00\n",
      "741±66\n",
      "1707±1249\n",
      "1469±646\n",
      "−0.003\n",
      "1\n",
      "4.46±0.00\n",
      "688±64\n",
      "2149±110\n",
      "636±566\n",
      "−0.03\n",
      "1\n",
      "4.41±0.01\n",
      "20±1\n",
      "80±6\n",
      "4.55±0.01\n",
      "−0.003\n",
      "1.003\n",
      "4.47±0.00\n",
      "683±23\n",
      "2494±1205\n",
      "268±120\n",
      "−0.03\n",
      "1.03\n",
      "4.43±0.03\n",
      "22±3\n",
      "73±8\n",
      "4.56±0.05\n",
      "Table 1: The impact of clipped softmax hyperparameters on BERT-base.\n",
      "and batch size of 192. Similar to our BERT experiments, we use trainin...\n",
      "No images found on page 7\n",
      "Page 8 Text: (a) Relative FP16 log-perplexity\n",
      "(b) Maximum infinity norm\n",
      "Figure 6: The performance of clipped softmax using γ = −α/T parameterization on BERT-6L. (a)\n",
      "Relative (compared to vanilla softmax pre-training) FP16 log-perplexity ↑on Wikitext validation set.\n",
      "(b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis).\n",
      "(a) BERT-6L\n",
      "(b) ViT\n",
      "Figure 7: The performance of Linear gated attention using different bias initialization settings.\n",
      "γ := −α\n",
      "T , where α > 0 is a new hyperpara...\n",
      "Extracted 4 images from page 8\n",
      "Page 9 Text: Model\n",
      "Method\n",
      "FP16/32\n",
      "Max inf. norm\n",
      "Avg. kurtosis\n",
      "W8A8\n",
      "BERT\n",
      "(ppl.↓)\n",
      "Vanilla\n",
      "4.49±0.01\n",
      "735±55\n",
      "3076±262\n",
      "1294±1046\n",
      "Clipped softmax\n",
      "4.39±0.00\n",
      "21.5±1.5\n",
      "80±6\n",
      "4.52±0.01\n",
      "Gated attention\n",
      "4.45±0.03\n",
      "39.2±26.0\n",
      "201±181\n",
      "4.65±0.04\n",
      "OPT\n",
      "(ppl.↓)\n",
      "Vanilla\n",
      "15.84±0.05\n",
      "340±47\n",
      "1778±444\n",
      "21.18±1.89\n",
      "Clipped softmax\n",
      "16.29±0.07\n",
      "63.2±8.8\n",
      "19728±7480\n",
      "37.20±2.40\n",
      "Gated attention\n",
      "15.55±0.05\n",
      "8.7±0.6\n",
      "18.9±0.9\n",
      "16.02±0.07\n",
      "ViT\n",
      "(acc.↑)\n",
      "Vanilla\n",
      "80.75±0.10\n",
      "359±81\n",
      "1018±471\n",
      "69.24±6.93\n",
      "Clipped softmax\n",
      "80.89±0.13\n",
      "73.7±14.9\n",
      "22.9±1.6\n",
      "79.77±0.25...\n",
      "No images found on page 9\n",
      "Page 10 Text: (a) Vanilla softmax (Attention layer #11, head #3)\n",
      "(b) Clipped softmax (Attention layer #11, head #8)\n",
      "(c) Gated attention (Attention layer #11, head #5)\n",
      "Figure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our\n",
      "proposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention\n",
      "probabilities, values, and their product. (c): gating probabilities π = sigmoid (G (x)), attention\n",
      "probabilities (output of softmax), values, and thei...\n",
      "Extracted 10 images from page 10\n",
      "Page 11 Text: References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
      "Layer normalization.\n",
      "arXiv preprint\n",
      "arXiv:1607.06450, 2016.\n",
      "[2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution\n",
      "networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.\n",
      "[3] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving\n",
      "low-bit quantization through learnable offsets and better initialization. In Proceedings of th...\n",
      "No images found on page 11\n",
      "Page 12 Text: [17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, and Armand\n",
      "Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320,\n",
      "2020.\n",
      "[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization\n",
      "for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n",
      "[19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A ...\n",
      "No images found on page 12\n",
      "Page 13 Text: [34] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj\n",
      "Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan\n",
      "Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,\n",
      "Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysan-\n",
      "dre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, A...\n",
      "No images found on page 13\n",
      "Page 14 Text: [50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\n",
      "are unsupervised multitask learners. 2019.\n",
      "[51] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky,\n",
      "Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai\n",
      "Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt,\n",
      "Sitaram Lanka, Eric Chung, and Doug Burger. Pushi...\n",
      "No images found on page 14\n",
      "Page 15 Text: [66] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4\n",
      "quantization for transformer models: Latency speedup, composability, and failure cases. 2023.\n",
      "[67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\n",
      "Accurate and efficient post-training quantization for large language models. In CVPR, 2022.\n",
      "[68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xl-\n",
      "net: Generalized au...\n",
      "No images found on page 15\n",
      "Page 16 Text: Supplementary materials\n",
      "A\n",
      "Additional graphs from outlier analysis\n",
      "In this section, we present additional graphs from our outlier investigation in Section 3 for BERT and\n",
      "vision transformer.\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 9: A summary of several outlier statistics recorded from ImageNet validation set on ViT.\n",
      "(a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts\n",
      "in attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A\n",
      "heatmap o...\n",
      "Extracted 3 images from page 16\n",
      "Page 17 Text: Configuration\n",
      "G\n",
      "Memory overhead (per attention layer)\n",
      "# extra parameters\n",
      "# extra tokens\n",
      "Linear\n",
      "nheads × Linear(dhead →1)\n",
      "nheads(dhead + 1)\n",
      "∼1\n",
      "MLP\n",
      "nheads × MLP(dhead →nhid →1)\n",
      "nheads(nhid(dhead + 2) + 1)\n",
      "∼nhid\n",
      "All-heads-linear Linear(dmodel →nheads)\n",
      "nheads(dmodel + 1)\n",
      "∼nheads\n",
      "Table 4: An overview of the gating function parameterizations explored in this paper and their\n",
      "memory overhead.\n",
      "B.1\n",
      "Gating architectures\n",
      "We investigate the choice of several gating functions, summarized in Table 4. The confi...\n",
      "No images found on page 17\n",
      "Page 18 Text: Method\n",
      "LN γ wd\n",
      "FP16 ppl.↓\n",
      "Max inf norm\n",
      "Avg. Kurtosis\n",
      "W8A8 ppl.↓\n",
      "Vanilla\n",
      "✕\n",
      "15.84±0.05\n",
      "339.6±47.2\n",
      "1777±444.\n",
      "21.18±1.89\n",
      "GA, Linear (πinit = 0.1)\n",
      "✕\n",
      "15.61±0.05\n",
      "35.6±4.5\n",
      "42.4±22.9\n",
      "16.41±0.18\n",
      "GA, Linear (πinit = 0.25)\n",
      "✕\n",
      "15.50±0.04\n",
      "35.8±0.5\n",
      "59.0±48.3\n",
      "16.25±0.08\n",
      "GA, Linear (πinit = 0.5)\n",
      "✕\n",
      "15.54±0.01\n",
      "46.5±5.0\n",
      "40.6±8.9\n",
      "16.30±0.01\n",
      "GA, All-heads-linear\n",
      "✕\n",
      "15.43±0.01\n",
      "32.8±1.7\n",
      "24.2±3\n",
      "16.30±0.12\n",
      "Vanilla\n",
      "✓\n",
      "15.96±0.03\n",
      "87.7±31.9\n",
      "2080±1460\n",
      "39.46±16.59\n",
      "CS (γ = −1/512)\n",
      "✓\n",
      "15.99±0.02\n",
      "106.4±7.0\n",
      "5764±2150\n",
      "185.23±220.00\n",
      "CS...\n",
      "No images found on page 18\n",
      "Page 19 Text: embeddings (which was absent in the model definition, by default). As we can see in Table 6, together\n",
      "with this change, both of our proposed methods greatly dampens the outliers’ magnitude, reduces the\n",
      "kurtosis, and yields models with significantly higher quantized performance, which is within 1% of\n",
      "the original FP32 accuracy.\n",
      "B.5\n",
      "The impact of clipped softmax hyperparameters (γ and ζ) on ViT\n",
      "γ\n",
      "ζ\n",
      "FP32 acc.\n",
      "Max inf norm\n",
      "W8A8 acc.\n",
      "0\n",
      "1\n",
      "78.80±0.42\n",
      "426±69\n",
      "71.27±0.88\n",
      "(= Vanilla)\n",
      "0\n",
      "1.001\n",
      "78.78±0.29\n",
      "411...\n",
      "No images found on page 19\n",
      "Page 20 Text: the vanilla softmax at the start of fine-tuning. We add a small activation regularization term at the\n",
      "output of each FFN to further encourage the reduction in the magnitude of activations, as unlike when\n",
      "training from scratch outliers are already present in the pre-trained model and need to be suppressed.\n",
      "As we can see from Table 9, fine-tuning with our proposed gated attention results in a better perplexity\n",
      "and also reduced maximum infinity norm and the average kurtosis compared to fine-tuning ...\n",
      "No images found on page 20\n",
      "Page 21 Text: sequences for 106 steps, using AdamW optimizer [39] with the maximum learning rate of 10−4,\n",
      "learning rate warm up over the first 104 steps, following by a linear decay to zero by the end of\n",
      "training. We use L2 weight decay of 0.01, L2 gradient norm clipping of 1.0, and dropout probability\n",
      "of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20].\n",
      "C.2\n",
      "OPT pre-training\n",
      "To speed up experimentation, we train OPT-125m sized model on the concatenation of Wikipedia...\n",
      "No images found on page 21\n",
      "Page 22 Text: Model\n",
      "Vanilla\n",
      "Clipped softmax\n",
      "Gated attention (Linear / MLP)\n",
      "BERT\n",
      "92.8±1.2\n",
      "93.6±0.8\n",
      "97.7 / 119.1\n",
      "OPT\n",
      "53.6±0.4\n",
      "54.4±0.4\n",
      "55.7 / 64.7\n",
      "ViT\n",
      "101.8±0.3\n",
      "104.0±0.7\n",
      "110.8 / 122.9\n",
      "Table 11: An overview of the runtime of the proposed methods, compared to the vanilla pre-training,\n",
      "measured in hours on Nvidia-A100 GPUs.\n",
      "using the linear G adds the compute overhead between 3% and 8%, depending on the model. We\n",
      "found that adding weight decay on LayerNorm γ for OPT and adding the LayerNorm after the patch\n",
      "embedd...\n",
      "No images found on page 22\n",
      "Page 23 Text: (a) Attention layer #10, data sequence #16\n",
      "(b) Attention layer #11, data sequence #16\n",
      "(c) Attention layer #10, data sequence #21\n",
      "(d) Attention layer #11, data sequence #21\n",
      "(e) Attention layer #10, data sequence #61\n",
      "(f) Attention layer #11, data sequence #61\n",
      "(g) Attention layer #10, data sequence #88\n",
      "(h) Attention layer #11, data sequence #88\n",
      "Figure 10: Visualization of the self-attention patterns (attention probabilities, values, and their\n",
      "product in left, middle and right columns, respectively)...\n",
      "Extracted 24 images from page 23\n",
      "Page 24 Text: (a) Attention layer #10, data sequence #16\n",
      "(b) Attention layer #11, data sequence #16\n",
      "(c) Attention layer #10, data sequence #21\n",
      "(d) Attention layer #11, data sequence #21\n",
      "(e) Attention layer #10, data sequence #61\n",
      "(f) Attention layer #11, data sequence #61\n",
      "(g) Attention layer #10, data sequence #88\n",
      "(h) Attention layer #11, data sequence #88\n",
      "Figure 11: Visualization of the self-attention patterns (attention probabilities, values, and their product\n",
      "in left, middle and right columns, respectively)...\n",
      "Extracted 24 images from page 24\n",
      "Page 25 Text: (a) Attention layer #10, attention head #1\n",
      "(b) Attention layer #11, attention head #1\n",
      "(c) Attention layer #10, attention head #7\n",
      "(d) Attention layer #11, attention head #7\n",
      "(e) Attention layer #10, attention head #8\n",
      "(f) Attention layer #11, attention head #8\n",
      "(g) Attention layer #10, attention head #10\n",
      "(h) Attention layer #11, attention head #10\n",
      "Figure 12: Visualization of the self-attention patterns (attention probabilities, values, and their product\n",
      "in left, middle and right columns, respectivel...\n",
      "Extracted 24 images from page 25\n",
      "Page 26 Text: (a) Attention layer #1\n",
      "(b) Attention layer #2\n",
      "(c) Attention layer #3\n",
      "(d) Attention layer #4\n",
      "(e) Attention layer #5\n",
      "(f) Attention layer #6\n",
      "(g) Attention layer #7\n",
      "(h) Attention layer #8\n",
      "Figure 13: Visualization of the self-attention patterns (attention probabilities, values, and their product\n",
      "in left, middle and right columns, respectively) in attention head #3 (↔channel dim #180) and the\n",
      "first eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from\n",
      "MNLI-m valid...\n",
      "Extracted 24 images from page 26\n",
      "Page 27 Text: (a) Attention layer #10, Attention head #3, data sequence #1\n",
      "(b) Attention layer #11, Attention head #3, data sequence #1\n",
      "(c) Attention layer #10, Attention head #3, data sequence #5\n",
      "(d) Attention layer #11, Attention head #3, data sequence #5\n",
      "(e) Attention layer #10, Attention head #3, data sequence #7\n",
      "(f) Attention layer #11, Attention head #3, data sequence #7\n",
      "(g) Attention layer #10, Attention head #12, data sequence #1\n",
      "(h) Attention layer #11, Attention head #12, data sequence #1\n",
      "Figure 14:...\n",
      "Extracted 24 images from page 27\n",
      "Page 28 Text: (a) Attention layer #10, Attention head #3, data sequence #1\n",
      "(b) Attention layer #11, Attention head #3, data sequence #1\n",
      "(c) Attention layer #10, Attention head #3, data sequence #5\n",
      "(d) Attention layer #11, Attention head #3, data sequence #5\n",
      "(e) Attention layer #10, Attention head #3, data sequence #7\n",
      "(f) Attention layer #11, Attention head #3, data sequence #7\n",
      "(g) Attention layer #10, Attention head #12, data sequence #1\n",
      "(h) Attention layer #11, Attention head #12, data sequence #1\n",
      "Figure 15:...\n",
      "Extracted 32 images from page 28\n",
      "Page 29 Text: (a)\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "(e)\n",
      "Figure 16: A summary of our outlier analysis for ViT demonstrated on a random subset from\n",
      "ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative\n",
      "attention weight spent on every patch (matrix of attention probabilities summed over rows) in the\n",
      "attention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An\n",
      "average magnitude of values (V ) for outlier and non-outlier patches.\n",
      "29\n",
      "...\n",
      "Extracted 40 images from page 29\n",
      "Page 30 Text: (a)\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "(e)\n",
      "Figure 17: A summary of our outlier analysis for ViT demonstrated on a random subset from\n",
      "ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative\n",
      "attention weight spent on every patch (matrix of attention probabilities summed over rows) in the\n",
      "attention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An\n",
      "average magnitude of values (V ) for outlier and non-outlier patches.\n",
      "30\n",
      "...\n",
      "Extracted 40 images from page 30\n",
      "Finished processing 2306.12929.pdf\n",
      "Processing: 2301.13688.pdf\n",
      "Total pages: 22\n",
      "Page 1 Text: The Flan Collection: Designing Data and Methods\n",
      "for Eﬀective Instruction Tuning\n",
      "Shayne Longpre∗\n",
      "Le Hou\n",
      "Tu Vu\n",
      "Albert Webson\n",
      "Hyung Won Chung\n",
      "Yi Tay\n",
      "Denny Zhou\n",
      "Quoc V. Le\n",
      "Barret Zoph\n",
      "Jason Wei\n",
      "Adam Roberts\n",
      "Google Research\n",
      "Abstract\n",
      "We study the design decisions of publicly available instruction tuning methods, and break down the\n",
      "development of Flan 2022 models (Chung et al., 2022). Through careful ablation studies on the Flan\n",
      "Collection of instruction tuning tasks and methods, we tease apart the eﬀe...\n",
      "No images found on page 1\n",
      "Page 2 Text: 1\n",
      "Introduction\n",
      "Large language models such as PaLM (Chowdhery et al., 2022), Chinchilla (Hoﬀmann et al., 2022), and\n",
      "ChatGPT among others (Brown et al., 2020; Ouyang et al., 2022) have unlocked new capabilities in performing\n",
      "natural language processing (NLP) tasks from reading instructive prompts. Prior art has shown that\n",
      "instruction tuning—ﬁnetuning language models on a collection of NLP tasks formatted with instructions—\n",
      "further enhances the ability of language models to perform an unseen task f...\n",
      "No images found on page 2\n",
      "Page 3 Text: 2\n",
      "Public Instruction Tuning Collections\n",
      "†\n",
      "Release\n",
      "Collection\n",
      "+ \n",
      "+ \n",
      "+ \n",
      "†\n",
      "†\n",
      "+ \n",
      "P\n",
      "P\n",
      "NP\n",
      "NP\n",
      "P\n",
      "P\n",
      "NP\n",
      "NP\n",
      "NP\n",
      "P\n",
      "P\n",
      "NP\n",
      "NP\n",
      "ZS\n",
      "FS\n",
      "ZS\n",
      "FS\n",
      "ZS\n",
      "FS\n",
      "ZS\n",
      "ZS\n",
      "ZS\n",
      "FS\n",
      "ZS\n",
      "ZS\n",
      "ZS\n",
      "FS\n",
      "FS\n",
      "ZS\n",
      "CoT\n",
      "Model\n",
      "Base\n",
      "Size\n",
      "Public?\n",
      "Model Details\n",
      "Data Collection & Training Details\n",
      "Prompt Types\n",
      "Tasks in Flan\n",
      "# Exs\n",
      "Methods\n",
      "+ \n",
      "+ \n",
      "+ \n",
      "+ \n",
      "+ \n",
      "+ \n",
      "+ \n",
      "P\n",
      "FS\n",
      "P\n",
      "FS\n",
      "ZS\n",
      "CoT\n",
      "Figure 2: A Timeline of Public Instruction Tuning Collections speciﬁes the collection release date, detailed information\n",
      "on the ﬁnetuned models (the base model, their size, and whether t...\n",
      "No images found on page 3\n",
      "Page 4 Text: and some reported strong beneﬁts from inverting the inputs and outputs in templates to produce new tasks\n",
      "(“noisy channel” in Min et al., 2022).\n",
      "The Second Wave\n",
      "A second wave of instruction tuning collections expanded prior resources: combining\n",
      "more datasets and tasks into one resource, like Super-Natural Instructions (Wang et al., 2022c) or OPT-IML\n",
      "(Iyer et al., 2022), adding multilingual instruction tuning in xP3 (Muennighoﬀet al., 2022), and Chain-of-\n",
      "Thought training prompts in Flan 2022 (Chu...\n",
      "No images found on page 4\n",
      "Page 5 Text: Experimental Setup\n",
      "We ﬁnetune on the preﬁx language model adapted T5-LM (Lester et al., 2021), using\n",
      "the XL (3B) size for all models for consistency, unless otherwise stated. While other sizes of Flan-T5 are\n",
      "available, we felt XL was appropriately sized to run large-scale systematic ablations, while being suﬃciently\n",
      "large to draw general conclusions. We evaluate on (a) a suite of 8 “Held-In” tasks represented within the\n",
      "1800+ training task collection (4 question answering and 4 natural language ...\n",
      "No images found on page 5\n",
      "Page 6 Text: 3.2\n",
      "Training with Mixed Prompt Settings\n",
      "Prior work has shown a wide variety of input templates per task can improve performance. However, separate\n",
      "from the wording of the instruction template, these prior LLMs mostly tune with template sets targeted to\n",
      "a single prompt setting: for zero-shot prompting (Wei et al., 2021; Sanh et al., 2021; Aghajanyan et al., 2021;\n",
      "Aribandi et al., 2021) or for few-shot prompting (Min et al., 2022; Wang et al., 2022c).\n",
      "An underappreciated design decision in Instruc...\n",
      "No images found on page 6\n",
      "Page 7 Text: 10\n",
      "50\n",
      "100\n",
      "200\n",
      "400\n",
      "800\n",
      "1600\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "XXL\n",
      "XL\n",
      "Large\n",
      "Base\n",
      "Small\n",
      "Number of Tasks\n",
      "Accuracy (%)\n",
      "Held-In Tasks Performance\n",
      "10\n",
      "50\n",
      "100\n",
      "200\n",
      "400\n",
      "800\n",
      "1600\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "XXL\n",
      "XL\n",
      "Large\n",
      "Base\n",
      "Small\n",
      "Number of Tasks\n",
      "Accuracy (%)\n",
      "Held-Out MMLU Performance\n",
      "Figure 4: Performance Scaling Laws for the number of ﬁnetuning tasks and model sizes. Held-In per-\n",
      "formance (left) and Held-Out MMLU performance (right) are shown. The gold star indicates the peak\n",
      "performance for that model size.\n",
      "Surprisingly, only T5-Small ...\n",
      "No images found on page 7\n",
      "Page 8 Text: Train Mixtures\n",
      "Metrics\n",
      "Held-In\n",
      "CoT\n",
      "MMLU\n",
      "All (Equal)\n",
      "64.9\n",
      "41.4\n",
      "47.3\n",
      "All - Flan 2021\n",
      "55.3\n",
      "38.6\n",
      "45.7\n",
      "All - T0-SF\n",
      "63.2\n",
      "43.4\n",
      "44.7\n",
      "All - Super-Nat. Inst.\n",
      "65.9\n",
      "42.2\n",
      "46.8\n",
      "All - CoT\n",
      "65.6\n",
      "29.1\n",
      "46.8\n",
      "All - Prog. Synth.\n",
      "66.9\n",
      "42.3\n",
      "46.8\n",
      "All - Dialog\n",
      "65.4\n",
      "40.3\n",
      "47.1\n",
      "All (Weighted)\n",
      "66.4\n",
      "40.1\n",
      "48.1\n",
      "Table 2: Subsets of tasks are left out from an equally weighted mixture to measure their importance. T0-SF\n",
      "and Flan 2021 ﬁnetuning are most important for MMLU, while Chain-of-Thought (CoT) ﬁnetuning is\n",
      "most important for...\n",
      "No images found on page 8\n",
      "Page 9 Text: ANLI\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Eval Metrics (%)\n",
      "ARC\n",
      "BoolQ\n",
      "CosmosQA\n",
      "RTE\n",
      "SQuAD v2\n",
      "AI2 Science\n",
      "CondaQA\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Eval Metrics (%)\n",
      "CxC\n",
      "MedNLI\n",
      "PubmedQA\n",
      "WANLI\n",
      "T5 →Flan →FT\n",
      "T5 →FT\n",
      "T5 →Flan\n",
      "Flan Held-In Tasks\n",
      "Flan Held-Out Tasks\n",
      "+4.0\n",
      "+8.7\n",
      "+2.7\n",
      "+1.0\n",
      "+7.8\n",
      "+2.4\n",
      "+16.7\n",
      "+2.6\n",
      "+0.0\n",
      "+0.1\n",
      "+2.3\n",
      "+1.6\n",
      "Figure 5: Flan-T5 Outperforms T5 on Single-Task Finetuning. We compare single-task ﬁnetuned T5, single-\n",
      "task ﬁnetuned Flan-T5, and Flan-T5 without any further ﬁnetuning.\n",
      "are not weighted signiﬁcantly: 4%, 2%, 2%, 2% ...\n",
      "No images found on page 9\n",
      "Page 10 Text: 0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Accuracy (%)\n",
      "WANLI\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "MedNLI\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "Number of Finetuning Steps\n",
      "CondaQA\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "PubmedQA\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "CxC\n",
      "Flan-T5 XL\n",
      "T5-XL\n",
      "Figure 6: Flan-T5 convergences faster than T5 on single-task ﬁnetuning for each of 5 Held-Out tasks from\n",
      "Flan ﬁnetuning.\n",
      "Figure 5: ﬁnetuning T5 directly on the target task as the conventional baseline (blue bars), using Flan-T5\n",
      "without further ﬁnetuning (beige bars), and ﬁnetuning Flan-T5 further on th...\n",
      "No images found on page 10\n",
      "Page 11 Text: Chowdhery et al., 2022; Hoﬀmann et al., 2022), and all models of such size class with fully public model\n",
      "parameters are decoder-only (Wang and Komatsuzaki, 2021; Le Scao et al., 2022; Zhang et al., 2022), the\n",
      "decision of which are often due to better hardware and software framework support. However, Raﬀel et al.\n",
      "(2020), Lewis et al. (2020), and Tay et al. (2022a) have consistently found that left-to-right causal language\n",
      "modeling is a suboptimal objective, while Tay et al. (2022b) and Wang et al...\n",
      "No images found on page 11\n",
      "Page 12 Text: References\n",
      "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.\n",
      "Muppet: Massive multi-task representations with pre-ﬁnetuning. In EMNLP, 2021. URL https://\n",
      "aclanthology.org/2021.emnlp-main.468.\n",
      "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\n",
      "Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\n",
      "Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruan...\n",
      "No images found on page 12\n",
      "Page 13 Text: Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\n",
      "Boolq: Exploring the surprising diﬃculty of natural yes/no questions. arXiv preprint arXiv:1905.10044,\n",
      "2019.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\n",
      "Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\n",
      "arXiv:1803.05457, 2018.\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hi...\n",
      "No images found on page 13\n",
      "Page 14 Text: Jordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\n",
      "Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie\n",
      "Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\n",
      "Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models.\n",
      "arXiv preprint arXiv:2203.15556, 2022.\n",
      "Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick...\n",
      "No images found on page 14\n",
      "Page 15 Text: Computational Linguistics, pages 7871–7880, Online, July 2020. Association for Computational Linguistics.\n",
      "doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\n",
      "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\n",
      "Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\n",
      "Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL\n",
      "https://arxiv.org/abs/2...\n",
      "No images found on page 15\n",
      "Page 16 Text: Niklas Muennighoﬀ, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\n",
      "M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through\n",
      "multitask ﬁnetuning. arXiv preprint arXiv:2211.01786, 2022.\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, JeﬀWu, Long Ouyang, Christina Kim, Christopher Hesse,\n",
      "Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\n",
      "with human feedback. arXiv preprint a...\n",
      "No images found on page 16\n",
      "Page 17 Text: Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for\n",
      "squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2:\n",
      "Short Papers), pages 784–789, 2018.\n",
      "Abhilasha Ravichander, Matt Gardner, and Ana Marasović. Condaqa: A contrastive reading comprehension\n",
      "dataset for reasoning about negation. arXiv preprint arXiv:2211.00295, 2022. URL https://arxiv.org/\n",
      "abs/2211.00295.\n",
      "Adam Roberts, Hyung Won Chung, Anselm L...\n",
      "No images found on page 17\n",
      "Page 18 Text: Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil\n",
      "Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131,\n",
      "2022a. URL https://arxiv.org/abs/2205.05131.\n",
      "Yi Tay, Jason Wei, Hyung Won Chung, David R. So, Siamak Shakeri, Xavier Garcia, Vinh Q. Tran,\n",
      "Hauixiu Steven Zheng, Jinfeng Rao, Denny Zhou, Donald Metzler, Neil Houlsby, Quoc V. Le, and Mostafa\n",
      "Dehghani. Transcending scaling laws with 0.1% extra c...\n",
      "No images found on page 18\n",
      "Page 19 Text: Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin\n",
      "Choi. Defending against neural fake news. Advances in neural information processing systems, 32, 2019.\n",
      "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\n",
      "Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n",
      "2022.\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Chris...\n",
      "No images found on page 19\n",
      "Page 20 Text: Appendix\n",
      "Table of Contents\n",
      "A Experimental Details\n",
      "20\n",
      "A.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "A.2 Single-Task Finetuning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "A.3 Evaluation\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "B\n",
      "Input Inversion Details\n",
      "21\n",
      "A\n",
      "Experimental Details\n",
      "A.1\n",
      "Instruction Tuning\n",
      "The Flan Collection experiments are as...\n",
      "No images found on page 20\n",
      "Page 21 Text: Used in\n",
      "Dataset\n",
      "Metric\n",
      "Held-In\n",
      "CoT\n",
      "ST-FT Held-In\n",
      "ST-FT Held-Out\n",
      "Citation\n",
      "ARC E+C\n",
      "Acc\n",
      "✓\n",
      "✓\n",
      "(Clark et al., 2018)\n",
      "ANLI R1+R2+R3\n",
      "3-class F1\n",
      "✓\n",
      "✓\n",
      "(Nie et al., 2020)\n",
      "AI2 Mid. Science\n",
      "4-class F1\n",
      "✓\n",
      "✓\n",
      "AI2 Science Questions\n",
      "BoolQ\n",
      "AUC-ROC\n",
      "✓\n",
      "✓\n",
      "(Clark et al., 2019)\n",
      "RTE\n",
      "AUC-ROC\n",
      "✓\n",
      "✓\n",
      "(Bentivogli et al., 2009)\n",
      "SQuAD V2\n",
      "F1\n",
      "✓\n",
      "(Rajpurkar et al., 2018)\n",
      "CosmosQA\n",
      "Acc\n",
      "✓\n",
      "(Huang et al., 2019)\n",
      "GSM8K\n",
      "Acc\n",
      "✓\n",
      "(Cobbe et al., 2021)\n",
      "StrategyQA\n",
      "Acc\n",
      "✓\n",
      "(Geva et al., 2021)\n",
      "SVAMP\n",
      "Acc\n",
      "✓\n",
      "(Patel et al., 2021)\n",
      "Asdiv\n",
      "Acc\n",
      "✓\n",
      "(Miao et al., 202...\n",
      "No images found on page 21\n",
      "Page 22 Text: Question\n",
      "Chain-of-Thought\n",
      "Answer\n",
      "Question\n",
      "Answer\n",
      "Chain-of-Thought\n",
      "Question\n",
      "Answer\n",
      "Chain-of-Thought\n",
      "Question\n",
      "Answer\n",
      "Chain-of-Thought\n",
      "Question\n",
      "Answer\n",
      "Chain-of-Thought\n",
      "Question\n",
      "Answer\n",
      "Chain-of-Thought\n",
      "Inputs\n",
      "Targets\n",
      "Figure 7: Input Inversions permutations for a Zero-Shot Chain-of-Thought example. Each is accompanied by a\n",
      "corresponding instruction template that prompts the model with what the input is, and what to predict as the targets.\n",
      "22\n",
      "...\n",
      "No images found on page 22\n",
      "Finished processing 2301.13688.pdf\n",
      "Processing: 2109.06243.pdf\n",
      "Total pages: 10\n",
      "Page 1 Text: KroneckerBERT: Learning Kronecker Decomposition for Pre-trained\n",
      "Language Models via Knowledge Distillation\n",
      "Marzieh S. Tahaei\n",
      "Noah’s Ark Lab,\n",
      "Huawei Technologies Canada\n",
      "marzieh.tahaei@huawei.com\n",
      "Ella Charlaix\n",
      "Noah’s Ark Lab,\n",
      "Huawei Technologies Canada\n",
      "charlaixe@gmail.com\n",
      "Vahid Partovi Nia\n",
      "Noah’s Ark Lab\n",
      "Huawei Technologies Canada\n",
      "vahid.partovinia@huawei.com\n",
      "Ali Ghodsi\n",
      "Department of Statistics and\n",
      "Actuarial Science, University of Waterloo\n",
      "ali.ghodsi@uwaterloo.com\n",
      "Mehdi Rezagholizadeh\n",
      "Noah’s Ark La...\n",
      "No images found on page 1\n",
      "Page 2 Text: eral works (Mao et al., 2020; Zhao et al., 2019,\n",
      "2021) that have tried to go beyond the compression\n",
      "factor of 10, have done so at the expense of a sig-\n",
      "niﬁcant drop in performance. This work proposes\n",
      "a novel framework that uses Kronecker decomposi-\n",
      "tion for extreme compression of Transformer-based\n",
      "PLMs and provides a very promising compression-\n",
      "performance trade-off. Similar to other decompo-\n",
      "sition methods, Kronecker decomposition can be\n",
      "used to represent weight matrices in NNs to re-\n",
      "duce the ...\n",
      "Extracted 1 images from page 2\n",
      "Page 3 Text: pruning, as well as a knowledge distillation method-\n",
      "ology as described in (Jiao et al., 2019).\n",
      "To achieve higher compression factors, authors\n",
      "in (Zhao et al., 2019) use a layer-wise KD method\n",
      "to reduce the size of vocabulary (from the usual\n",
      "30k to 5k) and the width of the layers. Similarly,\n",
      "Zhao et al. 2021 uses a mixed-vocabulary training\n",
      "method to train models with a smaller vocabulary.\n",
      "2.2\n",
      "Kronecker Factorization\n",
      "Kronecker products have previously been utilized\n",
      "for the compression of CNNs an...\n",
      "No images found on page 3\n",
      "Page 4 Text: that allows obtaining (A ⊗B)x without explicit\n",
      "reconstruction, A ⊗B (Lutkepohl, 1997):\n",
      "(A ⊗B)x = V(BRn2×n1(x)A⊤)\n",
      "(1)\n",
      "where A⊤is A transpose. Here, V is an operation\n",
      "that transforms a matrix to a vector by stacking\n",
      "its columns and Rn2×n1(x) is an operation that\n",
      "converts a vector x to a matrix of size n2 × n1\n",
      "by dividing the vector to columns of size n2 and\n",
      "concatenating the resulting columns together. The\n",
      "consequence of performing multiplication in this\n",
      "way is that it reduces the number of FLOPs ...\n",
      "No images found on page 4\n",
      "Page 5 Text: Embedding\n",
      "Multi-head attention\n",
      "Add and Norm\n",
      "Feedforward Network\n",
      "Add and Norm\n",
      "Classifier\n",
      "Embedding\n",
      "Multi-head attention\n",
      "Add and Norm\n",
      "Feedforward Network\n",
      "Add and Norm\n",
      "Classifier\n",
      "FFN output\n",
      "Attention matrices\n",
      "Embedding output\n",
      "Last block projected features\n",
      "Concat and project\n",
      "Concat\n",
      "MHA output\n",
      "First layer in FFN output\n",
      " \n",
      "Teacher \n",
      "Student \n",
      "Pooling\n",
      "English\n",
      "Wikipedia\n",
      "Teacher\n",
      "Student\n",
      "Student\n",
      "KroenckerBERT\n",
      "Teacher\n",
      "KroenckerBERT\n",
      "Inference on Edge\n",
      "Task dataset\n",
      " sample 5% of \n",
      "English Wikipedia\n",
      "Two stage KD t...\n",
      "No images found on page 5\n",
      "Page 6 Text: features and teacher’s projected features:\n",
      "gS(x) = concat\n",
      "\u0002\n",
      "pool(AS\n",
      "L), pool(HS\n",
      "L)\n",
      "\u0003\n",
      ",(13)\n",
      "gT (x) = concat\n",
      "\u0002\n",
      "pool(AT\n",
      "L), pool(HT\n",
      "L ))\n",
      "\u0003\n",
      ",(14)\n",
      "LProjection(x) = MSE(gS(x), PgT (x)).(15)\n",
      "Our ﬁnal loss is as follows:\n",
      "L(x, y) =\n",
      "X\n",
      "(x,y)\n",
      "LEmbedding(x) +\n",
      "LAttention(x) + LFFN(x) +\n",
      "LProjection(x) + LLogits(x) +\n",
      "LCE(x, y).\n",
      "(16)\n",
      "Note that, unlike some other KD methods where\n",
      "the motivation for this projection is to match the\n",
      "dimension of the student and teacher features, here\n",
      "we use it to obtain a richer rep...\n",
      "No images found on page 6\n",
      "Page 7 Text: Model\n",
      "Params\n",
      "MNLI-(m/mm)\n",
      "SST-2\n",
      "MRPC\n",
      "CoLA\n",
      "QQP\n",
      "QNLI\n",
      "RTE\n",
      "STS-B\n",
      "Avg\n",
      "BERTBASE\n",
      "108.5M\n",
      "83.9/83.4\n",
      "93.4\n",
      "87.9\n",
      "52.8\n",
      "71.1\n",
      "90.9\n",
      "67\n",
      "85.2\n",
      "79.5\n",
      "BERT4-PKD\n",
      "52.2M\n",
      "79.9/79.3\n",
      "89.4\n",
      "82.6\n",
      "24.8\n",
      "70.2\n",
      "85.1\n",
      "62.3\n",
      "79.8\n",
      "72.6\n",
      "TinyBERT\n",
      "14.5M\n",
      "82.5/81.8\n",
      "92.6\n",
      "86.4\n",
      "44.1\n",
      "71.3\n",
      "87.7\n",
      "66.6\n",
      "80.4\n",
      "77.0\n",
      "LadaBERT3\n",
      "15M\n",
      "82.1/81.8\n",
      "89.9\n",
      "-\n",
      "-\n",
      "69.4\n",
      "84.5\n",
      "-\n",
      "-\n",
      "-\n",
      "KroneckerBERT8\n",
      "14.3M\n",
      "82.9/81.7\n",
      "91.2\n",
      "88.5\n",
      "31.2\n",
      "70.8\n",
      "88.4\n",
      "66.9\n",
      "83.1\n",
      "76.1\n",
      "SharedProject\n",
      "5.6M\n",
      "76.4/75.2\n",
      "84.7\n",
      "84.9\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "LadaBERT4\n",
      "11M\n",
      "75.8/76.1\n",
      "84.0\n",
      "-\n",
      "-\n",
      "67.4\n",
      "75.1\n",
      "-\n",
      "-\n",
      "-\n",
      "KroneckerBERT19\n",
      "5.7M\n",
      "...\n",
      "No images found on page 7\n",
      "Page 8 Text:  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Text\n",
      "Figure 4: T-SNE visualization of the output of the\n",
      "middle Transformer layer of the ﬁne-tuned models\n",
      "on SST-2 dev.\n",
      "Left:\n",
      "Fine-tuned BERTBASE, mid-\n",
      "dle: KroneckerBERT8 ﬁne-tuned without KD, right:\n",
      "KroneckerBERT8 when trained using KD in two stages.\n",
      "The colours indicate the positive and negative classes.\n",
      "Pre-training\n",
      "Fine-tuning\n",
      "MNLI-m\n",
      "SST-2\n",
      "MRPC\n",
      "None\n",
      "No KD\n",
      "66.0\n",
      "81.3\n",
      "68.3\n",
      "None\n",
      "KD\n",
      "80.7\n",
      "86.2\n",
      "70.3\n",
      "KD\n",
      "No KD\n",
      "77.0\n",
      "87.2\n",
      "78.17\n",
      "KD\n",
      "KD\n",
      "82.8\n",
      "90.6\n",
      "86.6\n",
      "Table 5: Ablation study of the effect of K...\n",
      "Extracted 3 images from page 8\n",
      "Page 9 Text: order to perform Kronecker product matrix mul-\n",
      "tiplication (A ⊗B)X we need to multiply 3 ma-\n",
      "trices as in Eq.1. Multiplication of 3 matrices is\n",
      "done through two matrix-matrix multiplication in\n",
      "serial. Serial nature of this operation limits utiliza-\n",
      "tion of parallel processing units on modern CPUs\n",
      "and GPUs. Therefore without modiﬁcation of the\n",
      "underlying software/hardware, the reduction of the\n",
      "number of FLOPs may not lead to latency reduction\n",
      "on high-performance devices.\n",
      "In order to investigate h...\n",
      "No images found on page 9\n",
      "Page 10 Text: Vasileios Lioutas,\n",
      "Ahmad Rashid,\n",
      "Krtin Kumar,\n",
      "Md Akmal Haidar, and Mehdi Rezagholizadeh.\n",
      "2020. Improving word embedding factorization for\n",
      "compression using distilled nonlinear neural decom-\n",
      "position. In Proceedings of the 2020 Conference on\n",
      "Empirical Methods in Natural Language Processing:\n",
      "Findings, pages 2774–2784.\n",
      "Helmut Lutkepohl. 1997.\n",
      "Handbook of matri-\n",
      "ces.\n",
      "Computational statistics and Data analysis,\n",
      "2(25):243.\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham,\n",
      "Dan Huang, Andrew Y. Ng, and Ch...\n",
      "No images found on page 10\n",
      "Finished processing 2109.06243.pdf\n",
      "Processing: 2207.00112.pdf\n",
      "Total pages: 15\n",
      "Page 1 Text: Published as a conference paper at ICLR 2022\n",
      "LANGUAGE MODEL COMPRESSION WITH WEIGHTED\n",
      "LOW-RANK FACTORIZATION\n",
      "Yen-Chang Hsu∗1, Ting Hua∗1, Sung-En Chang2, Qian Lou1, Yilin Shen1, and Hongxia Jin1\n",
      "1Samsung Research America , 2Northeastern University ,\n",
      "{yenchang.hsu, ting.hua, qian.lou, yilin.shen,\n",
      "hongxia.jin}@samsung.com,{chang.sun}@northeastern.edu\n",
      "ABSTRACT\n",
      "Factorizing a large matrix into small matrices is a popular strategy for model com-\n",
      "pression. Singular value decomposition (SVD) plays a vit...\n",
      "No images found on page 1\n",
      "Page 2 Text: Published as a conference paper at ICLR 2022\n",
      "without the generic pre-training. Since the factorization aims to approximate the learned model\n",
      "parameters, the method has the nature of directly inheriting the knowledge of the big trained model.\n",
      "However, approximating the learned weights with standard factorization often loses most of the\n",
      "task performance. This work investigates this issue with the most popular strategy, which uses\n",
      "singular value decomposition (SVD) to compress the learned model wei...\n",
      "No images found on page 2\n",
      "Page 3 Text: Published as a conference paper at ICLR 2022\n",
      "applies SVD to the transformer layers, but it does not investigate why SVD gives a very poor result\n",
      "without ﬁne-tuning. Our work explores this issue and provides a weighted version to address it.\n",
      "2.2\n",
      "FISHER INFORMATION\n",
      "The Fisher information measures the amount of information that an observable dataset D carries\n",
      "about a model parameter w. The computation of its exact form is generally intractable since it\n",
      "requires marginalizing over the space of D, wh...\n",
      "No images found on page 3\n",
      "Page 4 Text: Published as a conference paper at ICLR 2022\n",
      "W\n",
      "≈\n",
      "U\n",
      "S\n",
      "VT\n",
      "×\n",
      "×\n",
      "=\n",
      "W’\n",
      "Poorly reconstructed\n",
      "parameters\n",
      "Important\n",
      "parameters\n",
      "Truncated \n",
      "parameters\n",
      "Figure 2: The dilemma of vanilla SVD. Some parameters (the overlap of meshed orange and green)\n",
      "that signiﬁcantly impact the task performance may not be reconstructed well by SVD because their\n",
      "associated singular values are small and truncated.\n",
      "W\n",
      "≈\n",
      "×\n",
      "×\n",
      "U*\n",
      "S*\n",
      "V*T\n",
      "×\n",
      "W’\n",
      "=\n",
      "Figure 3: The schematic effect of our Fisher-Weighted SVD (FWSVD). ˆI is a diagonal matrix\n",
      "...\n",
      "Extracted 1 images from page 4\n",
      "Page 5 Text: Published as a conference paper at ICLR 2022\n",
      "Equation 6 can be solved by the standard SVD on ˆIW.\n",
      "We use the notation svd(ˆIW) =\n",
      "(U ∗, S∗, V ∗), then the solution of Equation (6) will be A = ˆI−1U ∗S∗, and B = V ∗T . In other\n",
      "words, the solution is the result of removing the information ˆI from the factorized matrices. Figure\n",
      "3 illustrates this process and its schematic effect of reducing the overlap between important param-\n",
      "eters and poorly reconstructed parameters. We will measure this overlap...\n",
      "No images found on page 5\n",
      "Page 6 Text: Published as a conference paper at ICLR 2022\n",
      "❷\n",
      "❶\n",
      "❷\n",
      "❸\n",
      "❶\n",
      "Distillation\n",
      "L\n",
      "Lg\n",
      "Lt\n",
      "Ltf\n",
      "Generic \n",
      "pre-training\n",
      "S\n",
      "Sg\n",
      "St\n",
      "Stf\n",
      "Task \n",
      "fine-tuning\n",
      "Factorization & \n",
      "fine-tuning (optional)\n",
      "Distillation\n",
      "L\n",
      ": Large model\n",
      "S\n",
      ": Small model\n",
      ": Compression path \n",
      "  (previous work)\n",
      ": Added step of this work\n",
      ": Step of previous works\n",
      ": Compression path \n",
      "  (this work)\n",
      "Figure 4: The three paths to create compressed language models are examined in this paper. L/S\n",
      "denote the initial models, Lg/Sg are models after generic pre-tra...\n",
      "No images found on page 6\n",
      "Page 7 Text: Published as a conference paper at ICLR 2022\n",
      "Table 2: Results of compressing an already compact model. The original task-speciﬁc models are\n",
      "directly downloaded from Huggingface pretrained models. Our FWSVD successfully reduces more\n",
      "parameters from all the compact models, while achieving the same level of accuracy. (ft: ﬁne-tuning)\n",
      "Original Compact Model (St)\n",
      "Path-3 Compression (St →Stf)\n",
      "Model-Task\n",
      "#Param.\n",
      "Perf.\n",
      "#Param.\n",
      "SVD\n",
      "SVD+ft.\n",
      "FWSVD\n",
      "FWSVD+ft.\n",
      "TinyBERT-STSB\n",
      "14.4M (7.8x)\n",
      "87.5\n",
      "11.8M (-18%)\n",
      "73.8...\n",
      "No images found on page 7\n",
      "Page 8 Text: Published as a conference paper at ICLR 2022\n",
      "Rank ratio\n",
      "Matthews Correlation\n",
      "-0.20\n",
      "0.00\n",
      "0.20\n",
      "0.40\n",
      "0.60\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "SVD\n",
      "FWSVD\n",
      "(a) COLA\n",
      "Rank ratio\n",
      "F1 Score\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "SVD\n",
      "FWSVD\n",
      "(b) NER (CoNLL-2003)\n",
      "Rank ratio\n",
      "Pearson Correlation\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "SVD\n",
      "FWSVD\n",
      "(c) STS-B\n",
      "Figure 5: FWSVD versus SVD by varying the ratio of reserved ranks. The model with a rank ratio\n",
      "1.0 indicates the full-rank reconstruction with the same accuracy as th...\n",
      "No images found on page 8\n",
      "Page 9 Text: Published as a conference paper at ICLR 2022\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "(a) STS-B performance drop\n",
      "Rank Group \n",
      "Relative Reconstruction Error\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "(b) STS-B reconstruction error\n",
      "Figure 6: Results of grouped rank truncation on STS-B task. In (a), FWSVD shows a consis-\n",
      "tent trend of having less performance drop with the small singular value groups (group 10 has the\n",
      "smallest singular values), mitigating the issue of Fi...\n",
      "No images found on page 9\n",
      "Page 10 Text: Published as a conference paper at ICLR 2022\n",
      "Patrick H Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: block-wise\n",
      "low-rank approximation for neural language model shrinking. In Proceedings of the 32nd Inter-\n",
      "national Conference on Neural Information Processing Systems, pp. 11011–11021, 2018a.\n",
      "Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. 2018b.\n",
      "Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear\n",
      "structur...\n",
      "No images found on page 10\n",
      "Page 11 Text: Published as a conference paper at ICLR 2022\n",
      "Erik Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-\n",
      "independent named entity recognition. In Proceedings of the Seventh Conference on Natural\n",
      "Language Learning at HLT-NAACL 2003, pp. 142–147, 2003.\n",
      "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\n",
      "bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n",
      "Richard Socher, Alex Perelygin, J...\n",
      "No images found on page 11\n",
      "Page 12 Text: Published as a conference paper at ICLR 2022\n",
      "A\n",
      "SUPPLEMENTARY\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "10.00%\n",
      "20.00%\n",
      "30.00%\n",
      "40.00%\n",
      "50.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "CoLA (ALBERT base)\n",
      "(a)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "20.00%\n",
      "40.00%\n",
      "60.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "MNLI (ALBERT base)\n",
      "(b)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "10.00%\n",
      "20.00%\n",
      "30.00%\n",
      "40.00%\n",
      "50.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "QNLI (ALBERT base)\n",
      "(c)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "25.00%\n",
      "50.00%\n",
      "75.00%\n",
      "100.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "...\n",
      "No images found on page 12\n",
      "Page 13 Text: Published as a conference paper at ICLR 2022\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "20.00%\n",
      "40.00%\n",
      "60.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "CoLA (BERT base)\n",
      "(a)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "20.00%\n",
      "40.00%\n",
      "60.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "MNLI (BERT base)\n",
      "(b)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "10.00%\n",
      "20.00%\n",
      "30.00%\n",
      "40.00%\n",
      "50.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "QNLI (BERT base)\n",
      "(c)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "10.00%\n",
      "20.00%\n",
      "30.00%\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "WSVD\n",
      "STS-B (BERT base)\n",
      "(d)\n",
      "Rank Group\n",
      "Pe...\n",
      "No images found on page 13\n",
      "Page 14 Text: Published as a conference paper at ICLR 2022\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "5.00%\n",
      "10.00%\n",
      "15.00%\n",
      "20.00%\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "CoLA (BERT base)\n",
      "(a)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "5.00%\n",
      "10.00%\n",
      "15.00%\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "MNLI (BERT base)\n",
      "(b)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "5.00%\n",
      "10.00%\n",
      "15.00%\n",
      "20.00%\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "FWSVD\n",
      "QNLI (BERT base)\n",
      "(c)\n",
      "Rank Group\n",
      "Performance Drop\n",
      "0.00%\n",
      "1.00%\n",
      "2.00%\n",
      "3.00%\n",
      "4.00%\n",
      "5.00%\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD\n",
      "WSVD\n",
      "STS-B (BERT base)\n",
      "(d)\n",
      "Rank Group\n",
      "Performance ...\n",
      "No images found on page 14\n",
      "Page 15 Text: Published as a conference paper at ICLR 2022\n",
      "Table 4: The raw values for Figure 6a. We additionally include the averaged singular values for\n",
      "each truncated group. The singular values from FWSVD are multiplied with Fisher information;\n",
      "thus their scales are different from SVD.\n",
      "Truncated group\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SVD performance drop\n",
      "25.4%\n",
      "6.7%\n",
      "2.8%\n",
      "2.2%\n",
      "1.1%\n",
      "0.9%\n",
      "0.7%\n",
      "0.4%\n",
      "0.1%\n",
      "4.9%\n",
      "FWSVD performance drop\n",
      "24.1%\n",
      "6.1%\n",
      "2.7%\n",
      "1.5%\n",
      "0.8%\n",
      "0.6%\n",
      "0.2%\n",
      "0.3%\n",
      "0.2%\n",
      "0.2%\n",
      "SVD average singular value\n",
      "2.674\n",
      "1.933\n",
      "1....\n",
      "No images found on page 15\n",
      "Finished processing 2207.00112.pdf\n",
      "Processing: 2310.20707.pdf\n",
      "Total pages: 55\n",
      "Page 1 Text: WHAT’S IN MY BIG DATA?\n",
      "Yanai Elazar1,2\n",
      "Akshita Bhagia1\n",
      "Ian Magnusson1\n",
      "Abhilasha Ravichander1\n",
      "Dustin Schwenk1\n",
      "Alane Suhr3\n",
      "Pete Walsh1\n",
      "Dirk Groeneveld1\n",
      "Luca Soldaini1\n",
      "Sameer Singh4\n",
      "Hanna Hajishirzi1,2\n",
      "Noah A. Smith1,2\n",
      "Jesse Dodge1\n",
      "1Allen Institute for AI\n",
      "2Paul G. Allen School of Computer Science & Engineering, University of Washington\n",
      "3University of California, Berkeley\n",
      "4University of California, Irvine\n",
      "yanaiela@gmail.com\n",
      "ABSTRACT\n",
      "Large text corpora are the backbone of language models. However, we...\n",
      "No images found on page 1\n",
      "Page 2 Text: Analyses\n",
      "Domain Distribution\n",
      "Percentage\n",
      "Domain\n",
      "Personally Identiﬁable \n",
      "Information (PII)\n",
      "PII\n",
      "Count\n",
      "jurafsky@stanford.edu\n",
      "(206) 430-7757\n",
      "208.80.152.2\n",
      "Counts\n",
      "Building Blocks\n",
      "Search\n",
      "—————\n",
      "==========\n",
      "**********\n",
      "){ref-type=“ﬁg”}\n",
      "//////////\n",
      "Most-Common \n",
      "Ngrams\n",
      "WIMBD\n",
      "Contamination\n",
      "Data\n",
      "Contamination\n",
      "BoolQ\n",
      "MNLI\n",
      "XSum\n",
      "WSC\n",
      "Figure 1: An overview of WIMBD. We implement two fundamental capabilities: Count and Search,\n",
      "allowing quick processing and access to large text corpora, which enables a wide range of ana...\n",
      "Extracted 7 images from page 2\n",
      "Page 3 Text: Table 1: Summary of the capabilities WIMBD provides and the analyses enabled by them.\n",
      "Basic Ability\n",
      "Analyses\n",
      "Exact Counts (§3.1)\n",
      "Document Counts, min/max doc length, #tokens, domain distribution, utterance date statistics,\n",
      "geolocation, language distribution, length distribution, toxic language,\n",
      "personally identifiable information, demographic sentiment co-occurrences\n",
      "Compressed Counts (§3.1)\n",
      "Duplicates, most & least common n-grams\n",
      "Search (§3.2)\n",
      "Benchmark contamination, n-gram counts\n",
      "the field, d...\n",
      "No images found on page 3\n",
      "Page 4 Text: Table 2: Summary statistics of the corpora, along with the models trained on them. Models noted\n",
      "with * signifies the model was not trained exactly on the version we consider, either due to some\n",
      "filtering, using additional data, or the original data being private.\n",
      "Dataset\n",
      "Origin\n",
      "Model\n",
      "Size (GB)\n",
      "# Documents\n",
      "# Tokens\n",
      "max(# Tokens)\n",
      "min(# Tokens)\n",
      "OpenWebText\n",
      "Gokaslan & Cohen (2019)\n",
      "GPT-2* (Radford et al., 2019)\n",
      "41.2\n",
      "8,005,939\n",
      "7,767,705,349\n",
      "95,139\n",
      "128\n",
      "C4\n",
      "Raffel et al. (2020)\n",
      "T5 (Raffel et al., 2020)\n",
      "8...\n",
      "No images found on page 4\n",
      "Page 5 Text: Figure 2: Domain distributions of the ten most common domains per token for C4, LAION-2B-en,\n",
      "and RedPajama. The results for the other corpora are discussed and presented in Appendix B.1.1\n",
      "4.2\n",
      "DATA STATISTICS\n",
      "Main Findings\n",
      "• Four out of the ten corpora we consider have ‘empty’ documents (meaning they contain\n",
      "only space-like characters), while The Pile and RedPajama contain the same longest document\n",
      "(with over 28 million tokens) of an encyclopedia.\n",
      "• While the most common source of webpages in C4 ...\n",
      "No images found on page 5\n",
      "Page 6 Text: Table 3: Most common 10-grams in five of the corpora we consider. n-grams from the top-10 that\n",
      "occur in more than one corpus are highlighted in the same color.\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "The Pile\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "- - - - - - - - - -\n",
      "3.4M\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "9M\n",
      ". . . . . . . . . .\n",
      "1.76B\n",
      "773M\n",
      "- - - - - - - - - -\n",
      "3.64B\n",
      ". . . . . . . . . .\n",
      "1.05M\n",
      ". . . . . . . . . .\n",
      "7.27M\n",
      "- - - - - - - - - -\n",
      "823M\n",
      "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n",
      "395M\n",
      "= = = = = = = = = =\n",
      "602M\n",
      "= = = ...\n",
      "No images found on page 6\n",
      "Page 7 Text: a single document has the same hash, we consider them duplicates.2 We examine the duplication\n",
      "of document text and URLs within each dataset. While some datasets explicitly deduplicate their\n",
      "content, others do not, and some even oversample some sources.\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "LAION-2B-en\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Duplicate %\n",
      "165M\n",
      "139M\n",
      "460M\n",
      "3.7M\n",
      "1.2B\n",
      "19.9M\n",
      "64.6M\n",
      "219M\n",
      "1.8M\n",
      "342M\n",
      "% of total\n",
      "uniq % of total\n",
      "Figure 3: Percentages of document and document\n",
      "cluster duplicates in corpora with > 1% docu-\n",
      "...\n",
      "No images found on page 7\n",
      "Page 8 Text: super-glue_axb\n",
      "glue_ax\n",
      "head_qa\n",
      "glue_stsb\n",
      "stsb_multi\n",
      "health_fact\n",
      "aeslc\n",
      "sick\n",
      "sem_eval\n",
      "super-glue_rte\n",
      "glue_rte\n",
      "liar\n",
      "super-glue_copa\n",
      "winograd_wsc\n",
      "super-glue_wic\n",
      "Dataset\n",
      "0\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "% Contaminated instances\n",
      "5.1\n",
      "5.1\n",
      "5.1\n",
      "11.1\n",
      "11.1\n",
      "1.9\n",
      "45.5\n",
      "4.8\n",
      "4.9\n",
      "0.1\n",
      "0.1\n",
      "10.9\n",
      "1.2\n",
      "32.2\n",
      "18.6\n",
      "2.0\n",
      "2.0\n",
      "5.2\n",
      "3.5\n",
      "3.5\n",
      "7.5\n",
      "1.6\n",
      "0.3\n",
      "0.3\n",
      "0.2\n",
      "0.2\n",
      "29.2\n",
      "0.6\n",
      "29.3\n",
      "64.4\n",
      "6.2\n",
      "6.2\n",
      "5.9\n",
      "9.9\n",
      "9.9\n",
      "18.7\n",
      "0.1\n",
      "52.6\n",
      "52.8\n",
      "67.5\n",
      "67.5\n",
      "45.0\n",
      "100.0\n",
      "58.2\n",
      "60.2\n",
      "1.4\n",
      "1.4\n",
      "5.3\n",
      "3.1\n",
      "3.1\n",
      "3.4\n",
      "0.3\n",
      "0.2\n",
      "0.2\n",
      "0.2\n",
      "0.2\n",
      "13.9\n",
      "1.0\n",
      "30.4\n",
      "49.4\n",
      "Corpus\n",
      "The Pile\n",
      "C4\n",
      "RedPajama\n",
      "OSCA...\n",
      "No images found on page 8\n",
      "Page 9 Text: Most examined datasets were not found in the corpora It is important to note that while we found\n",
      "some contamination, most of the considered benchmarks do not appear in the corpora we investigated\n",
      "(67 out of the 82 datasets). For instance, Winogrande (Sakaguchi et al., 2021), a large-scale corpus in\n",
      "the style of the Winograd schema, does not appear in any of the examined corpora.\n",
      "4.4.2\n",
      "PERSONALLY IDENTIFIABLE INFORMATION\n",
      "Table 5: Extrapolated PII frequencies. Count is the\n",
      "extrapolated frequency i...\n",
      "No images found on page 9\n",
      "Page 10 Text: are the demographic characteristics of the annotators and annotation guideline developers?” from\n",
      "Data Statements) we call for more tailored documentation of large-scale pretraining corpora.6 This\n",
      "work offers a superset of the automatic full-corpus analyses proposed by Dodge et al. (2021); Gao\n",
      "et al. (2020), with several additions, categorization, and programmatic interface, allowing better\n",
      "understanding of the content of current and future large text corpora.\n",
      "Grounding Models to their Training D...\n",
      "No images found on page 10\n",
      "Page 11 Text: reach for the stars! arXiv preprint arXiv:2301.03988, 2023. URL https://arxiv.org/abs/\n",
      "2301.03988.\n",
      "Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht\n",
      "Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,\n",
      "Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged\n",
      "Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev,\n",
      "Mike Tian-jian Jiang, and Al...\n",
      "No images found on page 11\n",
      "Page 12 Text: Jesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,\n",
      "Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the\n",
      "colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing, pp. 1286–1305, Online and Punta Cana, Dominican Republic,\n",
      "November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n",
      "98. URL https://aclanthology.org/2021.emnlp-m...\n",
      "No images found on page 12\n",
      "Page 13 Text: Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-\n",
      "Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In\n",
      "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n",
      "1: Long Papers), pp. 8424–8445, Dublin, Ireland, May 2022. Association for Computational\n",
      "Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.\n",
      "acl-long.577.\n",
      "Hector J. Levesque, Ernest Davis, a...\n",
      "No images found on page 13\n",
      "Page 14 Text: Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna.\n",
      "Data and its (dis)contents: A survey of dataset development and use in machine learning re-\n",
      "search. In Patterns, 2021. URL https://www.sciencedirect.com/science/article/pii/\n",
      "S2666389921001847.\n",
      "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\n",
      "Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\n",
      "dataset for falcon llm: outperform...\n",
      "No images found on page 14\n",
      "Page 15 Text: Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, and Eneko Agirre. Did chatgpt\n",
      "cheat on your test?, Jun 2023. URL https://hitz-zentroa.github.io/lm-contamination/\n",
      "blog/.\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\n",
      "adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN\n",
      "0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.\n",
      "Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong,...\n",
      "No images found on page 15\n",
      "Page 16 Text: Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun,\n",
      "Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir\n",
      "Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona\n",
      "Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\n",
      "Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Muñoz Ferrandis, Danish\n",
      "Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong...\n",
      "No images found on page 16\n",
      "Page 17 Text: 12th International Joint Conference on Natural Language Processing: System Demonstrations,\n",
      "pp. 72–87, Taipei, Taiwan, November 2022. Association for Computational Linguistics. URL\n",
      "https://aclanthology.org/2022.aacl-demo.9.\n",
      "Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report,\n",
      "Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.\n",
      "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Rus-\n",
      "sell Authur, Khyathi Chandu...\n",
      "No images found on page 17\n",
      "Page 18 Text: A\n",
      "CORPORA: ELABORATION\n",
      "We cover ten different corpora, including text-only corpora (e.g., C4), captions from image-captioning\n",
      "(LAION-2B-en), and code (The Stack). A high level description of these corpora using WIMBD is\n",
      "presented in Table 2, and details about the information contained in those corpora are detailed in\n",
      "Table 6.\n",
      "OPENWEBTEXT\n",
      "is an open-source reproduction7 (Gokaslan & Cohen, 2019) of the data used to\n",
      "train GPT-2 (Radford et al., 2019). Due to the limited information provided by Radf...\n",
      "No images found on page 18\n",
      "Page 19 Text: THE STACK\n",
      "(Kocetkov et al., 2023) is a source-code dataset that was collected for training language\n",
      "models, and parts of it were used to train SantaCoder (Allal et al., 2023) and MPT (Team, 2023).\n",
      "It was compiled from GHArchive8 with some filters: files that cannot contribute to training code\n",
      "such as binary files, files larger than 1MB, and some extensions. In addition, only repositories with\n",
      "permissive licenses were included (18 license types in the version v1.0, and 193 in version v1.1),\n",
      "and w...\n",
      "No images found on page 19\n",
      "Page 20 Text: B\n",
      "ADDITIONAL RESULTS\n",
      "We provide additional details and extended results on all the corpora considered in this work. This\n",
      "appendix is structured in a similar way to the structure in the main paper, categorized by the four\n",
      "different high-level analyses: (1) Data Statistics (Appendix B.1), (2) Data Quality (Appendix B.2),\n",
      "(3) Community- and Society-Relevant Measurements (Appendix B.3), and (4) Cross-Data Analysis\n",
      "(Appendix B.4).\n",
      "B.1\n",
      "DATA STATISTICS\n",
      "The summary statistics are composed of different a...\n",
      "No images found on page 20\n",
      "Page 21 Text: Perhaps not surprisingly, the most common suffix is com, which is between 60.1% of the documents\n",
      "in OSCAR and 77.5% in LAION-2B-en. The distribution of suffixes for each dataset exhibits a long\n",
      "tail with a total of over 3,000 different suffixes in the different corpora. While the top 10 typically\n",
      "represent suffixes from English-speaking countries (e.g., co.uk, and ca), LAION-2B-en’s top-10\n",
      "contains a lot of non-English speaking countries as well, such as Germany (de, 0.7%), Russia (ru,\n",
      "0.5%), Fr...\n",
      "No images found on page 21\n",
      "Page 22 Text: Here, we aim to assess the proportion of languages in all corpora. We use the CLD212 classifier to\n",
      "make a prediction about what language is being used in each document, and use this prediction as\n",
      "a label that we analyze in aggregate. Note that we use the classifier label also in mixed-language\n",
      "documents (if CLD2’s is_reliable flag is False, we apply the label UN). Table 7 reports the percentages\n",
      "of English-language documents across corpora. As expected, the English fraction is quite high, given\n",
      "...\n",
      "No images found on page 22\n",
      "Page 23 Text: 0.00\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "% of Documents\n",
      "www.nytimes.com\n",
      "en.wikipedia.org\n",
      "do5.b00kmedia.ru\n",
      "www.latimes.com\n",
      "www.theguardian.com\n",
      "www.huffpost.com\n",
      "patents.google.com\n",
      "www.businessinsider.com\n",
      "www.forbes.com\n",
      "www.eventbrite.com\n",
      "C4 Domains per Document\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "% of Documents\n",
      "patents.google.com\n",
      "en.wikipedia.org\n",
      "en.m.wikipedia.org\n",
      "www.nytimes.com\n",
      "journals.plos.org\n",
      "www.latimes.com\n",
      "www.theguardian.com\n",
      "www.forbes.com\n",
      "www.huffpost.com\n",
      "www.scribd.com\n",
      "C4 Domains per Token\n",
      "0.00\n",
      "0.05\n",
      "0.10\n",
      "0.15\n",
      "0.20\n",
      "% o...\n",
      "No images found on page 23\n",
      "Page 24 Text: https\n",
      "http\n",
      "Scheme\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "% of Documents\n",
      "62.5\n",
      "37.5\n",
      "C4 Schemes\n",
      "https\n",
      "http\n",
      "Scheme\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "% of Documents\n",
      "66.7\n",
      "33.3\n",
      "mC4-en Schemes\n",
      "https\n",
      "http\n",
      "Scheme\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "% of Documents\n",
      "87.6\n",
      "12.4\n",
      "OSCAR Schemes\n",
      "https\n",
      "http\n",
      "Scheme\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "% of Documents\n",
      "67.9\n",
      "32.1\n",
      "RedPajama Schemes\n",
      "https\n",
      "http\n",
      "Scheme\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "% of Documents\n",
      "80.1\n",
      "19.9\n",
      "LAION-2B-en Schemes\n",
      "Figure 7: Schema distributions of the ten most common domains for each corpus. We show the\n",
      "results fo...\n",
      "No images found on page 24\n",
      "Page 25 Text: C4*\n",
      "mC4-en*\n",
      "OSCAR*\n",
      "RedPajama*\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en*\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% of Documents\n",
      "2023\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "2019\n",
      "2018\n",
      "2010-2017\n",
      "2000-2009\n",
      "1990-1999\n",
      "pre-1990\n",
      "Figure 9: Fraction of documents in each corpus produced per year. Corpora marked with * are\n",
      "estimates based on the Internet Archive index dates for a 10,000 document sample.\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "RedPajama\n",
      "LAION-2B-en\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% of Documents\n",
      "US\n",
      "UN\n",
      "Other\n",
      "CA\n",
      "GB\n",
      "DE\n",
      "AU\n",
      "FR\n",
      "IE\n",
      "NL\n",
      "SE\n",
      "(a) Percentage of URLs by country\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "RedPajama\n",
      "LA...\n",
      "No images found on page 25\n",
      "Page 26 Text: Table 8: Most common unigrams, bigrams and trigrams and their estimated counts.\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en\n",
      "The Stack\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "n-gram\n",
      "Count\n",
      "Unigrams\n",
      ",\n",
      "342M\n",
      "the\n",
      "4.29B\n",
      "to\n",
      "4.29B\n",
      "to\n",
      "4.29B\n",
      "to\n",
      "4.29B\n",
      "with\n",
      "4.29B\n",
      "the\n",
      "2.77B\n",
      "the\n",
      "2.13B\n",
      "-\n",
      "1.13B\n",
      "}\n",
      "4.29B\n",
      "the\n",
      "331M\n",
      ".\n",
      "4.29B\n",
      "the\n",
      "4.29B\n",
      "the\n",
      "4.29B\n",
      "the\n",
      "4.29B\n",
      "to\n",
      "4.29B\n",
      ",\n",
      "2.64B\n",
      ",\n",
      "1.9B\n",
      ",\n",
      "870M\n",
      "{\n",
      "4.29B\n",
      ".\n",
      "323M\n",
      ",\n",
      "4.29B\n",
      "of\n",
      "4.29B...\n",
      "No images found on page 26\n",
      "Page 27 Text: Table 9: Estimated unique unigrams, and their percentage of the total unigrams.\n",
      "Corpus\n",
      "Count\n",
      "Percentage\n",
      "OpenWebText\n",
      "88,551,499\n",
      "1.1\n",
      "C4\n",
      "759,392,762\n",
      "0.5\n",
      "mC4-en\n",
      "4,290,392,741\n",
      "0.2\n",
      "OSCAR\n",
      "1,280,686,454\n",
      "0.3\n",
      "The Pile\n",
      "1,809,241,096\n",
      "0.6\n",
      "RedPajama\n",
      "2,530,085,090\n",
      "0.2\n",
      "S2ORC\n",
      "287,196,445\n",
      "0.5\n",
      "peS2o\n",
      "201,729,350\n",
      "0.5\n",
      "LAION-2B-en\n",
      "554,850,812\n",
      "1.9\n",
      "The Stack\n",
      "4,294,966,820\n",
      "0.3\n",
      "27\n",
      "...\n",
      "No images found on page 27\n",
      "Page 28 Text: (a) OpenWebText\n",
      "(b) C4\n",
      "(c) mC4-en\n",
      "(d) OSCAR\n",
      "(e) The Pile\n",
      "Figure 12: Unique unigrams in OpenWebText, C4, mC4-en, OSCAR, and The Pile.\n",
      "28\n",
      "...\n",
      "Extracted 5 images from page 28\n",
      "Page 29 Text: (a) RedPajama\n",
      "(b) S2ORC\n",
      "(c) peS2o\n",
      "(d) LAION-2B-en\n",
      "(e) The Stack\n",
      "Figure 13: Unique unigrams in RedPajama, S2ORC, peS2o, LAION-2B-en, and The Stack.\n",
      "29\n",
      "...\n",
      "Extracted 5 images from page 29\n",
      "Page 30 Text: Table 10: Top 5 most occurring text duplicates from datasets with duplicates (OpenWebText and C4\n",
      "don’t have any duplicate documents). Truncation for visualization is marked by [...].\n",
      "Corpus\n",
      "Property #1 Duplicate\n",
      "#2 Duplicate\n",
      "#3 Duplicate\n",
      "#4 Duplicate\n",
      "#5 Duplicate\n",
      "mC4-en\n",
      "Text\n",
      "’, ’text-align:left; color:w\n",
      "hite;background-color:#0\n",
      "564d1;’] //}); // ly.show();\n",
      "var i_type = $(\"#fa[...]\n",
      "Tada has the world’s lea\n",
      "ding smart parking techn\n",
      "ology and has many of the\n",
      "world’s top experts. A hug\n",
      "[...]\n",
      "4K Ultr...\n",
      "No images found on page 30\n",
      "Page 31 Text: mC4-en\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en\n",
      "The Stack\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Duplicate %\n",
      "48.3K\n",
      "165M\n",
      "139M\n",
      "460M\n",
      "3.7M\n",
      "33.9K\n",
      "1.2B\n",
      "517K\n",
      "22K\n",
      "19.9M\n",
      "64.6M\n",
      "219M\n",
      "1.8M\n",
      "16.9K\n",
      "342M\n",
      "232K\n",
      "% of total\n",
      "uniq % of total\n",
      "Figure 14: Percentages of text duplicates to totals for datasets with any. The percentages of documents\n",
      "and percentages of unique document clusters are each shown as bars. Duplicate counts are presented\n",
      "above the bars.\n",
      "Table 12: Statistics about text duplicates per dataset. Counts of duplic...\n",
      "No images found on page 31\n",
      "Page 32 Text: Figure 15: Images from the top 25 most duplicated URLs in LAION-2B-en.\n",
      "LAION-2B-en also contains a significant number of template-generated alt texts paired with maps\n",
      "describing the location of rental boats. The only outlier in OpenWebText in terms of document length\n",
      "is at exactly 100,000 characters; all documents over this length were chunked into multiple documents\n",
      "of length 100,000 by the dataset builders.\n",
      "RedPajama also contains template-generated user-facing copy, including, e.g., placehold...\n",
      "Extracted 1 images from page 32\n",
      "Page 33 Text: Characters Distribution\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "Characters per Document\n",
      "0.00\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "OpenWebText\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "Characters per Document\n",
      "0.00\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "C4\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "Characters per Document\n",
      "0.00\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "mC4\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "Characters per Document\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "OSCAR\n",
      "10\n",
      "1\n",
      "10\n",
      "3\n",
      "10\n",
      "5\n",
      "10\n",
      "7\n",
      "Characters per Document\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "The Pile\n",
      "10\n",
      "1\n",
      "10\n",
      "3\n",
      "10\n",
      "5\n",
      "10\n",
      "7\n",
      "Characters per Document\n",
      "0.000\n",
      "0.005\n",
      "0.010\n",
      "0...\n",
      "No images found on page 33\n",
      "Page 34 Text: B.3\n",
      "COMMUNITY- AND SOCIETY-RELEVANT MEASUREMENTS\n",
      "In this section, we provide additional results on the contamination and PII analyses from the main pa-\n",
      "per, as well as conduct two more analyses: toxic language and demographic sentiment co-occurrences.\n",
      "Overall the community- and society-relevant measurements contain the following analyses:\n",
      "1. Benchmark contamination (§B.3.1)\n",
      "2. Personally identifiable information (§B.3.2)\n",
      "3. Toxic language (§B.3.3)\n",
      "4. Demographic sentiment co-occurrences (§B.3.4)...\n",
      "No images found on page 34\n",
      "Page 35 Text: Table 15: Contamination percentages of the 82 datasets filtered from PromptSource (Bach et al.,\n",
      "2022), in C4, OSCAR, The Pile, and RedPajama.\n",
      "Dataset/Corpus\n",
      "C4\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "adversarial-qa-adversarialQA\n",
      "0.03\n",
      "0.03\n",
      "0.03\n",
      "0.03\n",
      "adversarial-qa-dbert\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "adversarial-qa-dbidaf\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "adversarial-qa-droberta\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "aeslc\n",
      "1.57\n",
      "0.31\n",
      "45.49\n",
      "0.10\n",
      "amazon-reviews-multi\n",
      "2.28\n",
      "2.10\n",
      "1.48\n",
      "2.06\n",
      "billsum\n",
      "0.06\n",
      "0.06\n",
      "0.03\n",
      "0.06\n",
      "cosmos-qa\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "crows-pairs\n",
      "0...\n",
      "No images found on page 35\n",
      "Page 36 Text: B.3.2\n",
      "PII\n",
      "We use three regular expressions inspired by Subramani et al. (2023) to identify email addresses,\n",
      "phone numbers, and IP addresses across pretraining corpora. In addition, we improved the phone\n",
      "numbers regex for better precision. These regexes provide us with a high precision performance\n",
      "(which we manually evaluate) and allows a fast PII identification. We apply postprocessing rules\n",
      "to the resulting matches, to improve the precision of detecting personal information by seeking to\n",
      "elimin...\n",
      "No images found on page 36\n",
      "Page 37 Text: Table 17: Extrapolated frequency of matches for regex searches of different kinds of PII (email/\n",
      "phone numbers/IP addresses) in pretraining corpora. This is computed by multiplying the precision\n",
      "of our PII identification module for each pretraining corpus with the number of detections, in order\n",
      "to estimate the number of true matches. (Parantheses) contain the precision of our identification\n",
      "method, as estimated by manual verification, on each corpora. Precision indicates the proportion\n",
      "of sample...\n",
      "No images found on page 37\n",
      "Page 38 Text: Table 20: Abbreviated examples of incorrect detections by our method, for each PII type, in each\n",
      "pretraining dataset. The exact span that was matched is in red. Offensive content and personal\n",
      "information have been redacted from the presented examples.\n",
      "Corpus\n",
      "Email Addresses\n",
      "Phone Numbers\n",
      "IP Addresses\n",
      "OpenWebText\n",
      "skremoved)\n",
      "has\n",
      "joined\n",
      "*\n",
      "trayvonmartin\n",
      "sets\n",
      "ban\n",
      "on\n",
      "*!*@n***.*** * trayvonmartin\n",
      "has kicked whitepower from\n",
      "#n****\n",
      "...2017\n",
      "limitation\n",
      "99\n",
      "pcs.\n",
      "article\n",
      "id\n",
      "472172730\n",
      "ean\n",
      "4012138149625\n",
      "the\n",
      "mod...\n",
      "No images found on page 38\n",
      "Page 39 Text: Table 21: Toxic language percentages based on a taxonomy and a classifier over entire documents in\n",
      "the corpora we consider. Toxic language statistics in the corpora we consider. The document toxicity\n",
      "(the first two columns) reports the percentage of documents that contain at least one mention of toxic\n",
      "language detected by each of the approaches. The classifier is applied separately on each sentence.\n",
      "The fine-grained taxonomy mention (the last three columns) reports the number of toxic mentions\n",
      "o...\n",
      "No images found on page 39\n",
      "Page 40 Text: male\n",
      "female\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en*\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en\n",
      "asian\n",
      "black\n",
      "hispanic\n",
      "white\n",
      "athiest\n",
      "buddhist\n",
      "christian\n",
      "hindu\n",
      "jew\n",
      "muslim\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0.05\n",
      "0.10\n",
      "0.15\n",
      "Sentiment\n",
      "Figure 17: The average sentiment associated with several gender, racial, and religious demographic\n",
      "terms for each dataset. Note: averages for datasets marked with * were computed for 10% samples.\n",
      "40\n",
      "...\n",
      "Extracted 1 images from page 40\n",
      "Page 41 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "LAION-2B-en\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "C4\n",
      "Black\n",
      "Photo\n",
      "vector\n",
      "Print\n",
      "or\n",
      "House\n",
      "Size\n",
      "Light\n",
      "Kids\n",
      "City\n",
      "Women's\n",
      "Party\n",
      "Images\n",
      "2020\n",
      "Pack\n",
      "Style\n",
      "blue\n",
      "Glass\n",
      "Two\n",
      "illustration\n",
      "Hand\n",
      "Modern\n",
      "Family\n",
      "Hair\n",
      "Electric\n",
      "Gallery\n",
      "Diamond\n",
      "North\n",
      "Great\n",
      "my\n",
      "United\n",
      "Pro\n",
      "Happy\n",
      "V\n",
      "wall\n",
      "Young\n",
      "year\n",
      "Paper\n",
      "gold\n",
      "021\n",
      "Tote\n",
      "Out\n",
      "CD\n",
      "Door\n",
      "Crystal\n",
      "July\n",
      "DVD\n",
      "™\n",
      "IN\n",
      "Shop\n",
      "AP\n",
      "Me\n",
      "bag\n",
      "Fire\n",
      "Background\n",
      "Pants\n",
      "paper\n",
      "gift\n",
      "Stone\n",
      "War\n",
      "France\n",
      "History\n",
      "America\n",
      "Product\n",
      "rent\n",
      "silver\n",
      "Washington\n",
      "October\n",
      "Gifts\n",
      "Go\n",
      "Ice\n",
      "Wide\n",
      "Tips\n",
      "BMW\n",
      "Will\n",
      "College\n",
      "2...\n",
      "No images found on page 41\n",
      "Page 42 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "OpenWebText\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "C4\n",
      "fromthanhow You\n",
      "home\n",
      "men\n",
      "available\n",
      "issue\n",
      "anything\n",
      "Obama\n",
      "face\n",
      "wanted\n",
      "that s\n",
      "2017\n",
      "World\n",
      "reason\n",
      "action\n",
      "began\n",
      "running\n",
      "court\n",
      "became\n",
      "50\n",
      "Twitter\n",
      "Clinton\n",
      "general\n",
      "record\n",
      "weeks\n",
      "2011\n",
      "else\n",
      "mind\n",
      "created\n",
      "bring\n",
      "field\n",
      "earlier\n",
      "users\n",
      "difficult\n",
      "takes\n",
      "40Of\n",
      "goes\n",
      "European\n",
      "built\n",
      "period\n",
      "teams\n",
      "production\n",
      "account\n",
      "2008\n",
      "http\n",
      "seven\n",
      "levels\n",
      "they re\n",
      "message\n",
      "R\n",
      "growing\n",
      "Republicans\n",
      "practice\n",
      "certainly\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "OpenWebText\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000...\n",
      "No images found on page 42\n",
      "Page 43 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "C4\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "OpenWebText\n",
      "by other\n",
      "her\n",
      "really\n",
      "design\n",
      "offer\n",
      "planlow\n",
      "results\n",
      "visit\n",
      "training\n",
      "friends\n",
      "various\n",
      "Please\n",
      "especially\n",
      "quite\n",
      "art\n",
      "staff\n",
      "customer\n",
      "ideas\n",
      "application\n",
      "third\n",
      "By\n",
      "increase\n",
      "card\n",
      "test\n",
      "rate\n",
      "period\n",
      "strong\n",
      "skills\n",
      "points\n",
      "kids\n",
      "running\n",
      "items\n",
      "potential\n",
      "review\n",
      "Dr\n",
      "included\n",
      "latest\n",
      "ability\n",
      "conditions\n",
      "worked\n",
      "travel\n",
      "Of\n",
      "couple\n",
      "tools\n",
      "Many\n",
      "itself\n",
      "St\n",
      "late\n",
      "attention\n",
      "send\n",
      "Day\n",
      "global\n",
      "remember\n",
      "creating\n",
      "walk\n",
      "limited\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "C4\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "140...\n",
      "No images found on page 43\n",
      "Page 44 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "mC4-en\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "OpenWebText\n",
      "or\n",
      "timeuse\n",
      "i\n",
      "2017\n",
      "13\n",
      "product\n",
      "50 students\n",
      "car\n",
      "property\n",
      "cost\n",
      "t\n",
      "height\n",
      "hours\n",
      "range\n",
      "An\n",
      "04\n",
      "Download\n",
      "short\n",
      "40\n",
      "Please\n",
      "non\n",
      "test\n",
      "application\n",
      "please\n",
      "box\n",
      "steel\n",
      "Read\n",
      "table\n",
      "August\n",
      "World\n",
      "December\n",
      "whiteoffice\n",
      "Health\n",
      "See\n",
      "English\n",
      "View\n",
      "however\n",
      "sales\n",
      "problems\n",
      "reading\n",
      "mind\n",
      "wide2007\n",
      "interest\n",
      "meeteasily\n",
      "Company\n",
      "shows\n",
      "Book\n",
      "extra\n",
      "received\n",
      "32\n",
      "potential\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "mC4-en\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "C4\n",
      "or my into\n",
      "backthink\n",
      "2019\n",
      "e\n",
      "50\n",
      "Posted\n",
      "·\n",
      "01\n",
      "26\n",
      "weigh...\n",
      "No images found on page 44\n",
      "Page 45 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "OSCAR\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "OpenWebText\n",
      "haveotherpeople\n",
      "2020\n",
      "December\n",
      "And\n",
      "online\n",
      "February\n",
      "him\n",
      "More page\n",
      "s24 22\n",
      "users\n",
      "offer\n",
      "Email\n",
      "comments\n",
      "provided\n",
      "low\n",
      "etc\n",
      "receive\n",
      "agree\n",
      "cost\n",
      "visit\n",
      "Thanks\n",
      "31\n",
      "learn\n",
      "let\n",
      "rights\n",
      "certain\n",
      "event\n",
      "ensure\n",
      "special\n",
      "risk\n",
      "An\n",
      "light\n",
      "cash\n",
      "matter\n",
      "tell\n",
      "send\n",
      "sites\n",
      "usually\n",
      "marketing\n",
      "sale\n",
      "designed\n",
      "latest\n",
      "member\n",
      "books\n",
      "medical\n",
      "function\n",
      "note\n",
      "cause\n",
      "2006\n",
      "digital\n",
      "activities\n",
      "held\n",
      "popular\n",
      "Next\n",
      "you're\n",
      "trade\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "OSCAR\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "C4\n",
      "have...\n",
      "No images found on page 45\n",
      "Page 46 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "The Pile\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "OpenWebText\n",
      "be her\n",
      "n\n",
      "We\n",
      "Let\n",
      "d\n",
      "m`\n",
      "function\n",
      "P\n",
      "la\n",
      "something\n",
      "code\n",
      "begin\n",
      "label\n",
      "que\n",
      "mean\n",
      "either\n",
      "See\n",
      "significant\n",
      "V\n",
      "Court\n",
      "space\n",
      "image\n",
      "position\n",
      "div\n",
      "version\n",
      "risk\n",
      "conditions\n",
      "O\n",
      "addition\n",
      "particular\n",
      "whole\n",
      "showed\n",
      "defined\n",
      "standard\n",
      "points\n",
      "user\n",
      "length\n",
      "page\n",
      "significantly\n",
      "paper\n",
      "seems\n",
      "air\n",
      "style\n",
      "width\n",
      "hat\n",
      "partial\n",
      "struct\n",
      "doesn't\n",
      "un\n",
      "heartfollow\n",
      "functions\n",
      "stop\n",
      "plan\n",
      "created\n",
      "felt\n",
      "note\n",
      "etc\n",
      "images\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "The Pile\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "C4\n",
      ">\n",
      "be willsa...\n",
      "No images found on page 46\n",
      "Page 47 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "RedPajama\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "OpenWebText\n",
      ";\n",
      "also\n",
      "For\n",
      "too\n",
      "M\n",
      "non\n",
      "25\n",
      "x\n",
      "due\n",
      "21\n",
      "August\n",
      "2011\n",
      "issues\n",
      "member\n",
      "County\n",
      "model\n",
      "hours\n",
      "Health\n",
      "held\n",
      "view\n",
      "aloriginal\n",
      "international\n",
      "various\n",
      "share\n",
      "patients\n",
      "v\n",
      "article\n",
      "someone\n",
      "2022\n",
      "common\n",
      "David\n",
      "childlives\n",
      "groups\n",
      "worked\n",
      "growth\n",
      "amount\n",
      "Then\n",
      "saw\n",
      "complete\n",
      "build\n",
      "included\n",
      "focus\n",
      "playing\n",
      "woman\n",
      "meeting\n",
      "table\n",
      "method\n",
      "label\n",
      "Science\n",
      "fun\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "RedPajama\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "C4\n",
      "youbeenWe\n",
      "0\n",
      "right\n",
      "^\n",
      "@\n",
      "going\n",
      "class\n",
      "histor...\n",
      "No images found on page 47\n",
      "Page 48 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "S2ORC\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "OpenWebText\n",
      "/\n",
      "such\n",
      "t\n",
      "Fig\n",
      "m\n",
      "S\n",
      "Table\n",
      "value\n",
      "method\n",
      "due\n",
      "levels\n",
      "test\n",
      "showedrange\n",
      "100\n",
      "O\n",
      "genes\n",
      "difference\n",
      "18\n",
      "25\n",
      "long\n",
      "various\n",
      "presence\n",
      "determined\n",
      "variables\n",
      "normal\n",
      "training\n",
      "states\n",
      "loss\n",
      "made\n",
      "independent\n",
      "relative\n",
      "binding\n",
      "signal\n",
      "physical\n",
      "vector\n",
      "probability\n",
      "stage\n",
      "analyzed\n",
      "?\n",
      "mechanism\n",
      "analyses\n",
      "followed\n",
      "functional\n",
      "elements\n",
      "cost\n",
      "component\n",
      "global\n",
      "measurement\n",
      "interest\n",
      "certain\n",
      "direction\n",
      "produced\n",
      "29\n",
      "materials\n",
      "noise\n",
      "sensitivity\n",
      "education\n",
      "variation\n",
      "represent\n",
      "20...\n",
      "No images found on page 48\n",
      "Page 49 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "peS2o\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "OpenWebText\n",
      "this\n",
      "n\n",
      "analysis\n",
      "first\n",
      "Table\n",
      "mobserved\n",
      "obtained\n",
      "g\n",
      "D\n",
      "G\n",
      "protein\n",
      "lower\n",
      "shows\n",
      "role\n",
      "cancer\n",
      "gene\n",
      "difference\n",
      "systems\n",
      "since\n",
      "section\n",
      "scale\n",
      "determined\n",
      "particular\n",
      "calculated\n",
      "stress\n",
      "loss\n",
      "blood\n",
      "via\n",
      "required\n",
      "µ\n",
      "obtain\n",
      "content\n",
      "source\n",
      "body\n",
      "characteristics\n",
      "II\n",
      "score\n",
      "volume\n",
      "resulting\n",
      "measurements\n",
      "active\n",
      "fixed\n",
      "ability\n",
      "what\n",
      "brain\n",
      "2019\n",
      "short2010\n",
      "products\n",
      "experiment\n",
      "dose\n",
      "37\n",
      "food\n",
      "immune\n",
      "Results\n",
      "understanding\n",
      "pH\n",
      "leads\n",
      "affected\n",
      "equal\n",
      "represent\n",
      "unit\n",
      "versio...\n",
      "No images found on page 49\n",
      "Page 50 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "LAION-2B-en\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "OpenWebText\n",
      "Photo\n",
      "New\n",
      "Cover\n",
      "x\n",
      "How\n",
      "stock\n",
      "Silver\n",
      "Wedding\n",
      "Light\n",
      "black\n",
      "Table\n",
      "Kids\n",
      "Card\n",
      "Collection\n",
      "Pack\n",
      "Air\n",
      "s\n",
      "Small\n",
      "house25\n",
      "Download\n",
      "Furniture\n",
      "Metal\n",
      "Power\n",
      "table\n",
      "Girl\n",
      "Great\n",
      "Square\n",
      "Heart\n",
      "Thumbnail\n",
      "19\n",
      "Birthday\n",
      "Apple\n",
      "Casual\n",
      "front\n",
      "Galaxy\n",
      "21\n",
      "CD\n",
      "San\n",
      "Cat\n",
      "Phone\n",
      "Me\n",
      "bag\n",
      "June\n",
      "Open\n",
      "Background\n",
      "Flat\n",
      "tree\n",
      "Angeles\n",
      "match\n",
      "Recipes\n",
      "Head\n",
      "viewer\n",
      "America\n",
      "Disney\n",
      "Mountain\n",
      "Pinterest\n",
      "Colors\n",
      "silver\n",
      "Tea\n",
      "Plan\n",
      "Tips\n",
      "Valley\n",
      "Film\n",
      "Mouse\n",
      "Max\n",
      "Princess\n",
      "space\n",
      "TO\n",
      "diagram\n",
      "Close\n",
      "t...\n",
      "No images found on page 50\n",
      "Page 51 Text: 0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "The Stack\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "OpenWebText\n",
      "^\n",
      "div\n",
      "string\n",
      "dev\n",
      "const\n",
      "width\n",
      "01\n",
      "src\n",
      "font\n",
      "String\n",
      "_\n",
      "margin\n",
      "translation\n",
      "06\n",
      "l\n",
      "18\n",
      "dependencies\n",
      "50\n",
      "column\n",
      "z\n",
      "en\n",
      "D\n",
      "context\n",
      "package\n",
      "field\n",
      "Name\n",
      "amp\n",
      "X\n",
      "main\n",
      "container\n",
      "app\n",
      "parent\n",
      "output\n",
      "min\n",
      "template\n",
      "response\n",
      "la48\n",
      "aria\n",
      "make\n",
      "37\n",
      "H\n",
      "unit\n",
      "Y\n",
      "doc\n",
      "tree\n",
      "log\n",
      "library\n",
      "stylesheet\n",
      "download\n",
      "load\n",
      "u001b\n",
      "90\n",
      "frame\n",
      "W\n",
      "documentation\n",
      "1.5\n",
      "normal\n",
      "settings\n",
      "000\n",
      "coordinates\n",
      "black\n",
      "Description\n",
      "et\n",
      "framework\n",
      "browser\n",
      "account\n",
      "absolute\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "The Stack\n",
      "0\n",
      "2...\n",
      "No images found on page 51\n",
      "Page 52 Text: OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "Oscar\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION2B-en\n",
      "The Stack\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "Oscar\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION2B-en\n",
      "The Stack\n",
      "0.14\n",
      "0.22\n",
      "0.18\n",
      "0.23\n",
      "0.18\n",
      "0.14\n",
      "0.28\n",
      "0.27\n",
      "0.22\n",
      "0.28\n",
      "0.25\n",
      "0.23\n",
      "0.18\n",
      "0.17\n",
      "0.25\n",
      "0.32\n",
      "0.33\n",
      "0.33\n",
      "0.36\n",
      "0.3\n",
      "0.39\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.36\n",
      "0.31\n",
      "0.39\n",
      "0.035\n",
      "0.4\n",
      "0.41\n",
      "0.35\n",
      "0.4\n",
      "0.4\n",
      "0.42\n",
      "0.39\n",
      "0.4\n",
      "0.54\n",
      "0.53\n",
      "0.45\n",
      "0.49\n",
      "0.42\n",
      "0.47\n",
      "0.49\n",
      "0.49\n",
      "0.49\n",
      "JS Distance (Unigrams Intersection)\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "(a) Intersection JS distance\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "Oscar\n",
      "Th...\n",
      "Extracted 2 images from page 52\n",
      "Page 53 Text: OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en\n",
      "The Stack\n",
      "Unique Documents in Dataset 2 (D2)\n",
      "OpenWebText\n",
      "C4\n",
      "mC4-en\n",
      "OSCAR\n",
      "The Pile\n",
      "RedPajama\n",
      "S2ORC\n",
      "peS2o\n",
      "LAION-2B-en\n",
      "The Stack\n",
      "Unique Documents in Dataset 1 (D1)\n",
      "8M\n",
      "1.9K\n",
      "94\n",
      "333\n",
      "3.9M\n",
      "1.9K\n",
      "0\n",
      "0\n",
      "2\n",
      "1.8K\n",
      "1.9K\n",
      "365M\n",
      "1.4M\n",
      "1.7M\n",
      "119K\n",
      "365M\n",
      "0\n",
      "0\n",
      "30.6K\n",
      "423\n",
      "94\n",
      "1.4M\n",
      "3.9B\n",
      "257K\n",
      "4.8K\n",
      "1.7M\n",
      "0\n",
      "0\n",
      "88.5K\n",
      "113\n",
      "333\n",
      "1.7M\n",
      "257K\n",
      "287M\n",
      "45.2K\n",
      "1.7M\n",
      "0\n",
      "0\n",
      "66K\n",
      "530\n",
      "3.9M\n",
      "119K\n",
      "4.8K\n",
      "45.2K\n",
      "137M\n",
      "934K\n",
      "0\n",
      "0\n",
      "14.2K\n",
      "8.9M\n",
      "1.9K\n",
      "365M\n",
      "1.7M\n",
      "1.7M\n",
      "934K\n",
      "690M\n",
      "0\n",
      "0\n",
      "31.2K\n",
      "11.2M\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "...\n",
      "Extracted 4 images from page 53\n",
      "Page 54 Text: Table 23: Time benchmark of the different analyses on C4. We ran all of these analyses on a\n",
      "224-CPUs machine, with 881 Gb memory. * The contamination time was calculated on the test set\n",
      "of COPA, which contains 500 test examples. We also report the estimated cost in dollars based on\n",
      "Google’s pricing of the machine we used, that is $9.46 per hour.\n",
      "Category\n",
      "Analysis\n",
      "Time\n",
      "Estimated Cost ($)\n",
      "Data Statistics\n",
      "Summary Statistics\n",
      "6:32\n",
      "1\n",
      "Internet Schemas\n",
      "2:25\n",
      "0.4\n",
      "Internet Domains\n",
      "5:38\n",
      "0.9\n",
      "Internet Domains...\n",
      "No images found on page 54\n",
      "Page 55 Text: D\n",
      "TECHNICAL DETAILS\n",
      "This section describes the algorithms for computing the most common, least common, and total\n",
      "number of unique n-grams in a large corpus. Each of these algorithms uses the same trick that was\n",
      "inspired by Bloom filters (Bloom, 1970) as described in section 3.1. As a result these algorithms do\n",
      "not provide exact results, and the accuracy is determined by the amount of memory available for the\n",
      "hash table.\n",
      "D.1\n",
      "MOST COMMON n-GRAMS\n",
      "To collect the (approximate) top-k n-grams we start ...\n",
      "No images found on page 55\n",
      "Finished processing 2310.20707.pdf\n",
      "Processing: 2307.13304.pdf\n",
      "Total pages: 34\n",
      "Page 1 Text: QuIP: 2-Bit Quantization of\n",
      "Large Language Models With Guarantees\n",
      "Jerry Chee\n",
      "Cornell University\n",
      "jerrychee@cs.cornell.edu\n",
      "Yaohui Cai\n",
      "Cornell University\n",
      "yc2632@cornell.edu\n",
      "Volodymyr Kuleshov\n",
      "Cornell University\n",
      "kuleshov@cornell.edu\n",
      "Christopher De Sa\n",
      "Cornell University\n",
      "cdesa@cs.cornell.edu\n",
      "Abstract\n",
      "This work studies post-training parameter quantization in large language models\n",
      "(LLMs). We introduce quantization with incoherence processing (QuIP), a new\n",
      "method based on the insight that quantization be...\n",
      "No images found on page 1\n",
      "Page 2 Text: them by a Kronecker product of random orthogonal matrices. We denote “incoherence processing”\n",
      "as both the pre- and post- processing steps of our procedure. Incoherence processing can be viewed\n",
      "as a form of outlier suppression across the weights and the activation space.\n",
      "We complement our method with a theoretical analysis—the first for a quantization algorithm that\n",
      "scales to LLM-sized models—which analyzes the role of incoherence and shows that our quantization\n",
      "procedure is optimal within a gene...\n",
      "No images found on page 2\n",
      "Page 3 Text: Here, W ∈Rm×n is the original weight matrix for a given linear layer, ˆW ∈Rm×n are the quantized\n",
      "weights, x ∈Rn is an input vector drawn uniformly at random from a calibration set, and H is the\n",
      "second moment matrix of these vectors, interpreted as a proxy Hessian. Crucially, this formulation\n",
      "lets the quantization be run in parallel across neurons, which is tractable for large language models [8].\n",
      "For simplicity, we will focus in this section on rounding to the integers; subsequent sections will\n",
      "...\n",
      "No images found on page 3\n",
      "Page 4 Text: 0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "10\n",
      "5\n",
      "10\n",
      "4\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "100\n",
      "Normalized eigenvalues\n",
      "Block 16 k_proj\n",
      "Block 20 q_proj\n",
      "Block 30 fc1\n",
      "Figure 1: eig(H) from OPT-2.7b.\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "Before incoherence max |Wij|\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "After incoherence max |Wij|\n",
      "Figure 2: Max |Wij| before\n",
      "and after incoherence process-\n",
      "ing on OPT-2.7b.\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Before incoherence max |eigvec(H)ij|\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "After incoherence max |eigvec(H)ij|\n",
      "Figure 3: Max |Qij| before\n",
      "and after incoherence process...\n",
      "No images found on page 4\n",
      "Page 5 Text: Algorithm 1 QuIP - Incoherence Pre-Processing\n",
      "Require: b ∈N, H ∈Rn×n SPD, original W ∈Rm×n, ρ ∈R+, α ∈[0, 1]\n",
      "1: seeded sample random two-factor orthogonal matrices U ∈Rm×m and V ∈Rn×n\n",
      "2: H = H + α ∗mean(diag(H))I\n",
      "▷from OPTQ\n",
      "3: ˜D ←\n",
      "4p\n",
      "diag(H)/ diag(W T W)\n",
      "▷\n",
      "4√\n",
      "applies element-wise\n",
      "4: W ←W ˜D; H ←˜D−1H ˜D−1\n",
      "▷diagonal rescaling\n",
      "5: W ←UWV T ; H ←V HV T\n",
      "▷incoherence\n",
      "6: s ←ρ∥W∥F /√mn; W ←1\n",
      "2( 1\n",
      "sW + 1)\n",
      "▷reduced quantization range due to incoherency\n",
      "7: W ←clamp(W ∗(2b −1), 0, 2b −1)\n",
      "▷rescale W to lie ...\n",
      "No images found on page 5\n",
      "Page 6 Text: the favorable incoherence properties outlined above. One straightforward way to make a symmetric\n",
      "matrix incoherent is to conjugate it by a uniform random orthogonal matrix: this will result in each of\n",
      "its eigenvectors being a random unit vector, whose entries will concentrate around magnitude n−1/2.\n",
      "Specifically, let U ∈Rm×m and V ∈Rn×n be two random orthogonal matrices. (Let’s temporarily\n",
      "ignore how these matrices are generated, or how we would efficiently perform inference.) We\n",
      "ensure the weig...\n",
      "No images found on page 6\n",
      "Page 7 Text: Algorithm 3 QuIP: Quantization with Incoherence Processing\n",
      "Require: b ∈N, H ∈Rn×n SPD, W ∈Rm×n, Q ∈{Near, Stoch}, ρ ∈R+, α ∈[0, 1]\n",
      "1: ˆW, H, s, ˜D ←Alg 1(b, H, W, ρ, α)\n",
      "▷QuIP Incoherence Pre-Procesing\n",
      "2: H = ( `U + I)D( `U + I)−1\n",
      "▷LDL decomposition\n",
      "3: for k ∈{1, . . . , n} do ˆWk ←clamp(Q(Wk + (W −ˆW) `Uk), 0, 2b −1)\n",
      "▷LDLQ\n",
      "4: return ˆW ←Alg 2(b, H, ˆW, s, ˜D)\n",
      "▷QuIP Incoherence Post-Processing\n",
      "5\n",
      "Extensions and Further Analyses\n",
      "5.1\n",
      "OPTQ is a Special Case of LDLQ\n",
      "We prove a novel theoretical insigh...\n",
      "No images found on page 7\n",
      "Page 8 Text: Our “fixed” algorithm solves this convex problem (e.g. with ADMM), then runs QuIP using stochastic\n",
      "rounding and U = R−1 −I in place of the LDL decomposition. Observe that for sufficiently large c,\n",
      "this is exactly equivalent to base QuIP, since the solution of that optimization problem is given by the\n",
      "LDL decomposition when the constraint is dropped. Doing this (the full algorithm is given in the\n",
      "supplemental) yields the following theorem.\n",
      "Theorem 7. Suppose that we run Algorithm 5 (Supplement) t...\n",
      "No images found on page 8\n",
      "Page 9 Text: 101\n",
      "102\n",
      "103\n",
      "104\n",
      "Perplexity on PTB\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "Perplexity on C4\n",
      "10\n",
      "1\n",
      "100\n",
      "101\n",
      "# params in billions\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "Accuracy on ArcE\n",
      "10\n",
      "1\n",
      "100\n",
      "101\n",
      "# params in billions\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "Accuracy on LAMB\n",
      "FP16\n",
      "OPTQ-W3\n",
      "OPTQ-W2\n",
      "QuIP-W3\n",
      "QuIP-W2\n",
      "Figure 5: Quantizing OPT models up to 66B parameters. Our method QuIP is the first PTQ procedure\n",
      "to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.\n",
      "OPTQ\n",
      "QuIP (Ours)\n",
      "WBits\n",
      "Wiki↓\n",
      "C4↓\n",
      "ArcE↑\n",
      "PiQA↑\n",
      "SC↑\n",
      "Wiki↓\n",
      "C4↓\n",
      "A...\n",
      "No images found on page 9\n",
      "Page 10 Text: Wbits\n",
      "Rescale\n",
      "Incoherence\n",
      "Rescale+Incoherence\n",
      "Rescale+Incoherence+Quant Range\n",
      "4\n",
      "24.30\n",
      "24.32\n",
      "24.05\n",
      "23.89\n",
      "3\n",
      "32.62\n",
      "42.28\n",
      "31.32\n",
      "26.36\n",
      "Table 3: Ablating sub-steps of QuIP’s incoherence processing, see Algorithm 1. Perplexities are\n",
      "averaged over WikiText2, PTB, and C4 for OPT-350m.\n",
      "2-bit quantization. We provide plots on the remaining datasets in Supplement C. Note that the dip in\n",
      "OPTQ on OPT-66B is documented in their paper.\n",
      "Table 1 shows the results of quantizing Llama 2 70B using QuIP and OPTQ. Aga...\n",
      "No images found on page 10\n",
      "Page 11 Text: References\n",
      "[1] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das,\n",
      "Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas\n",
      "Mattei, Ryan Musa, Kartik Talamadupula, and Michael Witbrock. A systematic classifica-\n",
      "tion of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the\n",
      "Workshop on Machine Reading for Question Answering, pages 60–70, Melbourne, Australia,\n",
      "July 2018. Association for Computational Linguistics...\n",
      "No images found on page 11\n",
      "Page 12 Text: [16] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark\n",
      "Ferguson, Karen Katz, and Britta Schasberger. The Penn Treebank: Annotating predicate argu-\n",
      "ment structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro,\n",
      "New Jersey, March 8-11, 1994, 1994. URL https://aclanthology.org/H94-1020.\n",
      "[17] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\n",
      "models. arXiv preprint arXiv:1609.07843, 2016.\n",
      "[18] Na...\n",
      "No images found on page 12\n",
      "Page 13 Text: Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\n",
      "Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\n",
      "and fine-tuned chat models, 2023.\n",
      "[28] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training\n",
      "network quantization via bit-split and stitching. In International Conference on Machine\n",
      "Learning. PMLR, 2020.\n",
      "[29] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang,\n",
      "Fe...\n",
      "No images found on page 13\n",
      "Page 14 Text: A\n",
      "Checklist\n",
      "A.1\n",
      "Broader Impacts\n",
      "Our work pushes the quantization of large language models into the 2 bits per weight regime. Our aim\n",
      "is to drive foundational research on theoretical and empirical aspects of quantization. The ultimate\n",
      "goal is to enable more powerful LLMs to run more efficiently. However our work is unaware to what\n",
      "ends those LLMs are used.\n",
      "A.2\n",
      "Limitations\n",
      "The adaptive rounding [3] proxy objective considers each layer in isolation; it remains to be seen\n",
      "what other computationally ...\n",
      "No images found on page 14\n",
      "Page 15 Text: Algorithm 4 Greedy Updates: A Single Pass\n",
      "Require: b ∈N, H ∈Rn×n SPD, weights W ∈Rm×n, initial guess ˜W\n",
      "1: ˆW ←˜W\n",
      "2: U ←(H ⊙M) diag(H)−1\n",
      "▷M is the strictly upper triangular mask\n",
      "3: V ←W −( ˜W −W)(H ⊙M T ) diag(H)−1\n",
      "▷can skip if ˜W = W by setting V ←W\n",
      "4: for k ∈{1, . . . , n} do ˆWk ←clamp(Qnear(Vk + (W −ˆW)Uk), 0, 2b −1)\n",
      "5: return ˆW\n",
      "An application of greedy local search as a single-pass stand-alone method falls under our Adaptive\n",
      "Rounding with Linear Feedback framework, with the linear feedback...\n",
      "No images found on page 15\n",
      "Page 16 Text: Model\n",
      "Processing\n",
      "Absolute\n",
      "Approximate\n",
      "tr (D) / tr (H)\n",
      "Fractional Rank\n",
      "Fractional Rank\n",
      "OPT-125m\n",
      "Baseline\n",
      "0.926 (±0.172)\n",
      "0.112 (±0.127)\n",
      "0.540 (±0.093)\n",
      "Incoherent\n",
      "0.910 (±0.196)\n",
      "0.124 (±0.141)\n",
      "0.534 (±0.094)\n",
      "OPT-350m\n",
      "Baseline\n",
      "0.916 (±0.180)\n",
      "0.047 (±0.032)\n",
      "0.445 (±0.100)\n",
      "Incoherent\n",
      "0.908 (±0.183)\n",
      "0.059 (±0.062)\n",
      "0.440 (±0.106)\n",
      "OPT-1.3b\n",
      "Baseline\n",
      "0.541 (±0.404)\n",
      "0.020 (±0.023)\n",
      "0.399 (±0.187)\n",
      "Incoherent\n",
      "0.543 (±0.405)\n",
      "0.028 (±0.023)\n",
      "0.393 (±0.189)\n",
      "OPT-2.7b\n",
      "Baseline\n",
      "0.426 (±0.413)\n",
      "0.019 (±0.015)\n",
      "0.384 (±0...\n",
      "No images found on page 16\n",
      "Page 17 Text: of H across all layers in OPT models 125m to 2.7b (explanations in the caption). The approximate\n",
      "fractional rank decreases as model size increases; for OPT-2.7b the fractional rank is ≈0.02(±0.02).\n",
      "C.2\n",
      "Subsection 5.1 (Empirical Verification of OPTQ Equivalence)\n",
      "We share a python script in the supplementary code which empirically verifies that our implementation\n",
      "of LDLQ produces quantized values exactly matching OPTQ’s [1] implementation. While we prove\n",
      "the equivalence between LDLQ and OPTQ’s res...\n",
      "No images found on page 17\n",
      "Page 18 Text: 10\n",
      "1\n",
      "100\n",
      "101\n",
      "# params in billions\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "Perplexity on WikiText2\n",
      "10\n",
      "1\n",
      "100\n",
      "101\n",
      "# params in billions\n",
      "0.50\n",
      "0.55\n",
      "0.60\n",
      "0.65\n",
      "0.70\n",
      "0.75\n",
      "0.80\n",
      "Accuracy on PiQA\n",
      "10\n",
      "1\n",
      "100\n",
      "101\n",
      "# params in billions\n",
      "0.50\n",
      "0.55\n",
      "0.60\n",
      "0.65\n",
      "0.70\n",
      "0.75\n",
      "Accuracy on StoryCloze\n",
      "FP16\n",
      "OPTQ-W3\n",
      "OPTQ-W2\n",
      "QuIP-W3\n",
      "QuIP-W2\n",
      "Figure 6: Quantizing OPT models up to 66B parameters. Additional evaluation tasks shown here in\n",
      "the Supplement. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits\n",
      "per weight, across a v...\n",
      "No images found on page 18\n",
      "Page 19 Text: Incoherence Processing — OPT-13b\n",
      "Full\n",
      "QuIP\n",
      "QuIP-RG\n",
      "Greedy+IncP\n",
      "Near+IncP\n",
      "W16\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "Wiki↓\n",
      "10.13\n",
      "10.21\n",
      "10.5\n",
      "16.02\n",
      "10.35\n",
      "10.69\n",
      "13.81\n",
      "10.25\n",
      "10.61\n",
      "13.91\n",
      "10.34\n",
      "10.59\n",
      "16.12\n",
      "PTB↓\n",
      "14.52\n",
      "14.69\n",
      "15.05\n",
      "21.64\n",
      "14.73\n",
      "15.20\n",
      "22.23\n",
      "14.85\n",
      "15.11\n",
      "20.20\n",
      "14.93\n",
      "15.27\n",
      "23.18\n",
      "C4↓\n",
      "12.06\n",
      "12.16\n",
      "12.39\n",
      "16.60\n",
      "12.18\n",
      "12.43\n",
      "15.62\n",
      "12.21\n",
      "12.42\n",
      "15.19\n",
      "12.26\n",
      "12.56\n",
      "17.37\n",
      "ArcE↑\n",
      "61.78\n",
      "61.41\n",
      "59.47\n",
      "53.91\n",
      "60.35\n",
      "61.78\n",
      "52.86\n",
      "60.10\n",
      "59.43\n",
      "53.79\n",
      "60.56\n",
      "59.30\n",
      "50.00\n",
      "LAMB↑\n",
      "70.25\n",
      "72.09\n",
      "71.10\n",
      "56.24\n",
      "69.47\n",
      "69.07\n",
      "55.70\n",
      "70.83\n",
      "6...\n",
      "No images found on page 19\n",
      "Page 20 Text: Incoherence Processing — OPT-2.7b\n",
      "Full\n",
      "QuIP\n",
      "QuIP-RG\n",
      "Greedy+IncP\n",
      "Near+IncP\n",
      "W16\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "Wiki↓\n",
      "12.47\n",
      "12.39\n",
      "17.44\n",
      "2,998\n",
      "12.58\n",
      "15.07\n",
      "1,676\n",
      "12.68\n",
      "12.96\n",
      "155.6\n",
      "12.79\n",
      "13.79\n",
      "28.98\n",
      "PTB↓\n",
      "17.97\n",
      "18.42\n",
      "20.79\n",
      "63.59\n",
      "18.43\n",
      "20.49\n",
      "42.05\n",
      "18.34\n",
      "20.03\n",
      "46.28\n",
      "18.43\n",
      "19.51\n",
      "39.23\n",
      "C4↓\n",
      "14.34\n",
      "14.55\n",
      "15.63\n",
      "38.07\n",
      "14.65\n",
      "15.97\n",
      "27.89\n",
      "14.64\n",
      "15.22\n",
      "26.84\n",
      "14.67\n",
      "15.52\n",
      "27.34\n",
      "ArcE↑\n",
      "54.34\n",
      "53.28\n",
      "52.99\n",
      "46.93\n",
      "52.02\n",
      "52.36\n",
      "46.93\n",
      "52.90\n",
      "51.73\n",
      "43.14\n",
      "52.61\n",
      "50.93\n",
      "44.11\n",
      "LAMB↑\n",
      "64.82\n",
      "66.04\n",
      "64.99\n",
      "36.06\n",
      "64.64\n",
      "63.46\n",
      "43.39\n",
      "64.68...\n",
      "No images found on page 20\n",
      "Page 21 Text: Incoherence Processing — OPT-350m\n",
      "Full\n",
      "QuIP\n",
      "QuIP-RG\n",
      "Greedy+IncP\n",
      "Near+IncP\n",
      "W16\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "W4\n",
      "W3\n",
      "W2\n",
      "Wiki↓\n",
      "22.00\n",
      "22.5\n",
      "25.19\n",
      "672.3\n",
      "23.57\n",
      "25.54\n",
      "418.0\n",
      "23.14\n",
      "25.38\n",
      "239.9\n",
      "23.41\n",
      "27.86\n",
      "1,444\n",
      "PTB↓\n",
      "31.07\n",
      "32.57\n",
      "35.65\n",
      "744.2\n",
      "32.46\n",
      "37.00\n",
      "587.4\n",
      "33.10\n",
      "37.07\n",
      "301.0\n",
      "33.32\n",
      "39.49\n",
      "1,354\n",
      "C4↓\n",
      "22.59\n",
      "23.23\n",
      "25.48\n",
      "320.0\n",
      "23.45\n",
      "25.50\n",
      "215.4\n",
      "23.43\n",
      "25.48\n",
      "124.1\n",
      "23.81\n",
      "27.41\n",
      "880.2\n",
      "ArcE↑\n",
      "40.36\n",
      "39.44\n",
      "38.13\n",
      "27.44\n",
      "39.31\n",
      "38.47\n",
      "29.67\n",
      "39.77\n",
      "40.24\n",
      "30.64\n",
      "38.89\n",
      "38.76\n",
      "28.41\n",
      "LAMB↑\n",
      "46.67\n",
      "46.89\n",
      "42.03\n",
      "01.03\n",
      "43.04\n",
      "39.80\n",
      "04.99\n",
      "42.44\n",
      "...\n",
      "No images found on page 21\n",
      "Page 22 Text: AVERAGE(Perplexity Unbiased - Perplexity Biased) on Wiki, PTB, C4 (↓)\n",
      "Incoherence Processing\n",
      "Baseline Processing\n",
      "WBits\n",
      "125m\n",
      "350m\n",
      "1.3b\n",
      "2.7b\n",
      "125m\n",
      "350m\n",
      "1.3b\n",
      "2.7b\n",
      "4\n",
      "1.23\n",
      "0.73\n",
      "0.79\n",
      "0.19\n",
      "27.81\n",
      "5.58\n",
      "1.62\n",
      "0.87\n",
      "3\n",
      "13.26\n",
      "7.79\n",
      "2.14\n",
      "4.66\n",
      "880.4\n",
      "499.4\n",
      "28.63\n",
      "16.23\n",
      "2\n",
      "2,501\n",
      "18,732\n",
      "544.8\n",
      "2,251\n",
      "241.3\n",
      "17,945\n",
      "4,831\n",
      "3,798\n",
      "Table 15: Average perplexity difference (i.e. unbiased - biased) for LDLQ/OPTQ on WikiText2,\n",
      "PTB, and C4. That is, we can run LDLQ with the Q subroutine as stochastic rounding, instead of\n",
      "nearest. Th...\n",
      "No images found on page 22\n",
      "Page 23 Text: in Eq. (4). The proxy loss is then,\n",
      "tr\n",
      "\u0000(A(W, H) −W)H(A(W, H))T \u0001 (3),(4)\n",
      "=\n",
      "tr\n",
      "\u0010\n",
      "η(X + I)−1( `U + I)D( `U + I)T (X + I)−T ηT \u0011\n",
      "= tr\n",
      "\u0000ηBDBT ηT \u0001\n",
      ".\n",
      "(9)\n",
      "With the LDL assignment of U, we further have that,\n",
      "tr\n",
      "\u0000ηBDBT ηT \u0001\n",
      "= tr\n",
      "\u0000ηDηT \u0001\n",
      ".\n",
      "(10)\n",
      "First, consider the worst-case loss, Lworst. The goal is to construct a particularly bad case where\n",
      "the entries of ˜W are 1/2 ± ϵ, and thus when rounding to the integers we will always have error 1/2.\n",
      "Construct a weight matrix ˜W ∈Rm×n such that each entry satisf...\n",
      "No images found on page 23\n",
      "Page 24 Text: set those entries of ak to 0. If A is the matrix whose kth row is ak, this is equivalent to saying that A\n",
      "is strictly lower triangular.\n",
      "Next, let η be a random Gaussian sampled from N(0, I), and consider the recurrence given by\n",
      "x0 = 0 ∈Rn and\n",
      "xk+1 = xk −ekaT\n",
      "k xk + ekeT\n",
      "k η.\n",
      "It’s straightforward to see that Σk = E\n",
      "\u0002\n",
      "xkxT\n",
      "k\n",
      "\u0003\n",
      ". But it’s also easy to see that the step-k update only\n",
      "modifies/assigns the kth entry of x, and does so based only on earlier entries of x. Since eT\n",
      "k xk = 0,\n",
      "and no later ...\n",
      "No images found on page 24\n",
      "Page 25 Text: Suppose by way of induction that for some scalar the covariance Σk ⪯αH−1/2. For the base case,\n",
      "this obviously holds since Σ0 = 0. At step k,\n",
      "Σk+1 ⪯\n",
      "\u0010\n",
      "I −α−1ekeT\n",
      "k H1/2\u0011\n",
      "αH−1/2 \u0010\n",
      "I −α−1H1/2ekeT\n",
      "k\n",
      "\u0011\n",
      "+ ekeT\n",
      "k\n",
      "= αH−1/2 −2ekeT\n",
      "k + α−1ekeT\n",
      "k H1/2ekeT\n",
      "k + ekeT\n",
      "k\n",
      "⪯αH−1/2.\n",
      "Note that with this assignment,\n",
      "aT\n",
      "k Σkak ≤(α−1eT\n",
      "k H1/2)(αH−1/2)(α−1H1/2ek) = α−1eT\n",
      "k H1/2ek ≤1.\n",
      "So, by induction it follows that\n",
      "Σn ⪯µ2\n",
      "n tr\n",
      "\u0010\n",
      "H1/2\u0011\n",
      "· H−1/2,\n",
      "and so\n",
      "tr (HΣn) ≤µ2\n",
      "n tr\n",
      "\u0010\n",
      "H1/2\u0011\n",
      "tr\n",
      "\u0010\n",
      "H · H−1/2\u0011\n",
      "= µ2\n",
      "n tr\n",
      "\u0010\n",
      "H1/2\u00112\n",
      ".\n",
      "But f...\n",
      "No images found on page 25\n",
      "Page 26 Text: E\n",
      "Proofs for Section 4 (Quantization With Incoherence Processing:\n",
      "Incoherence Processing Step )\n",
      "Subsection 4.1 (Incoherence via Efficient Orthogonal Multiplication)\n",
      "Lemma 9 (Theorem 2.4 from Lalley [2] ). There exist constants C and A independent of n such\n",
      "that for any function F from the unit sphere in n dimensions to R that is 1-Lipschitz relative to the\n",
      "Riemannian metric on the sphere,\n",
      "Px∼Sn (F(x) −Ex∼Sn[F(x)] ≥t) ≤C exp\n",
      "\u0012\n",
      "−nt2\n",
      "A\n",
      "\u0013\n",
      "Lemma 10. Let B ∈Rm×n be a matrix, and let x be a random vect...\n",
      "No images found on page 26\n",
      "Page 27 Text: Lemma 5. Let H be a positive semi-definite matrix on Rn×n and W a matrix on Rm×n, and suppose\n",
      "that m = p1 · p2 · · · pk and n = q1 · q2 · · · qk. Let U1, U2, . . . , Uk, V1, V2, . . . , Vk be independent\n",
      "random orthogonal matrices on Rpi×pi and Rqi×qi respectively. Set U as the Kronecker product\n",
      "U = U1 ⊗U2 ⊗· · · ⊗Uk and V as V = V1 ⊗V2 ⊗· · · ⊗Vk Then V HV T is µH-incoherent with\n",
      "probability at least 1 −δ, and UWV T is µW -incoherent with probability at least 1 −δ, where\n",
      "µH = Ak/2 log\n",
      "\u0012Ckn2\n",
      "δ\n",
      "\u0013...\n",
      "No images found on page 27\n",
      "Page 28 Text: F\n",
      "Proofs for Section 5 (Extensions and Further Analyses)\n",
      "Subsection 5.1 (OPTQ is a Special Case of LDLQ)\n",
      "Theorem 6. OTPQ [1] falls within the class of adaptive rounding procedures with linear feedback\n",
      "as described by Eq. (2), and is equivalent to LDLQ in Section 3.\n",
      "Proof. OPTQ works in the following way. After OPTQ has quantized the first t −1 components of\n",
      "the row vector w, it minimizes the proxy loss over the remaining n −t + 1 elements, keeping the\n",
      "first t −1 elements fixed. It then quantizes...\n",
      "No images found on page 28\n",
      "Page 29 Text: Algorithm 5 “Fixed” Rounding via a Convex Program\n",
      "Require: W ∈Rm×n, H ∈Rn×n, c > 0, ρ > 0\n",
      "Require: factorization m = p1p2, n = p3p4\n",
      "draw U1 ∈Rp1×p1 uniformly from the set of orthogonal matrices using seed seed(U1)\n",
      "draw U2 ∈Rp2×p2 uniformly from the set of orthogonal matrices using seed seed(U2)\n",
      "draw U3 ∈Rp3×p3 uniformly from the set of orthogonal matrices using seed seed(U3)\n",
      "draw U4 ∈Rp4×p4 uniformly from the set of orthogonal matrices using seed seed(U4)\n",
      "W ←(U1 ⊗U2)W(U3 ⊗U4)\n",
      "H ←(U T\n",
      "3 ⊗U T\n",
      "4 )H...\n",
      "No images found on page 29\n",
      "Page 30 Text: This inductive step will hold if, letting h = maxi eT\n",
      "i H1/2ei,\n",
      "2αβ ≥1 + α2βh\n",
      "On the other hand,\n",
      "eT\n",
      "i LT Lei = E\n",
      "\u0002\n",
      "(xnei)2\u0003\n",
      "= E\n",
      "h\n",
      "(−xi−1Aei + ηei)2i\n",
      "= E\n",
      "h\n",
      "(−xi−1Aei)2i\n",
      "+ 1\n",
      "= eT\n",
      "i AT Σi−1Aei + 1\n",
      "= α2eT\n",
      "i H1/2Σi−1H1/2ei + 1\n",
      "≤α2βeT\n",
      "i H1/2H−1/2H1/2ei + 1\n",
      "≤α2βeT\n",
      "i H1/2ei + 1.\n",
      "So the constraint of our optimization problem will be satisfied if\n",
      "α2βh ≤c.\n",
      "To satisfy these constraints, set β = max(h, h/c) and α = β−1. Then\n",
      "2 max(h, h/c)−1 · max(h, h/c) ≥1 + max(h, h/c)−2 · max(h, h/c) · h,\n",
      "and\n",
      "max(h, h/c)−...\n",
      "No images found on page 30\n",
      "Page 31 Text: Proof. Let η be the error of stochastic rounding, and observe that each entry is, conditioned on earlier\n",
      "steps, zero mean and supported on two values that differ by 1. Also observe that\n",
      "ˆw =\n",
      "\u0000w −( ˆw −w)(L−1 −I)\n",
      "\u0001\n",
      "+ η,\n",
      "and so\n",
      "ˆw −w = ηL\n",
      "and\n",
      "E [exp (( ˆw −w)u)] = E [exp (ηLu)] .\n",
      "From a repeated application of Hoeffding’s lemma, we get\n",
      "E [exp (( ˆw −w)u)] ≤exp\n",
      "\u00121\n",
      "8 ∥Lu∥2\n",
      "\u0013\n",
      ".\n",
      "Setting u 7→γu for γ > 0,\n",
      "E [exp (γ( ˆw −w)u)] ≤exp\n",
      "\u0012γ2\n",
      "8 ∥Lu∥2\n",
      "\u0013\n",
      ".\n",
      "And by Markov’s inequality,\n",
      "P (exp (γ( ˆw −w)u) ≥exp(γR)...\n",
      "No images found on page 31\n",
      "Page 32 Text: Proof. First, from the previous lemmas, if Uei is the ith eigenvector of H, with eigenvalue λi since\n",
      "P\n",
      "\u0012\n",
      "λi(eT\n",
      "j ( ˆw −w)Uei)2 ≥λi ∥LUei∥2 · 1\n",
      "2 log\n",
      "\u00122\n",
      "δ\n",
      "\u0013\u0013\n",
      "≤δ.\n",
      "By the union bound,\n",
      "P\n",
      "\u0012\n",
      "∃i, j, λi(eT\n",
      "j ( ˆw −w)Uei)2 ≥λi ∥LUei∥2 · 1\n",
      "2 log\n",
      "\u00122mn\n",
      "δ\n",
      "\u0013\u0013\n",
      "≤δ.\n",
      "And so\n",
      "P\n",
      "\n",
      "X\n",
      "i,j\n",
      "λi(eT\n",
      "j ( ˆw −w)Uei)2 ≥\n",
      "X\n",
      "i,j\n",
      "λi ∥LUei∥2 · 1\n",
      "2 log\n",
      "\u00122mn\n",
      "δ\n",
      "\u0013\n",
      "≤δ,\n",
      "which simplifies to\n",
      "P\n",
      "\u0012\n",
      "tr\n",
      "\u0000( ˆw −w)H( ˆw −w)T \u0001\n",
      "≥m tr\n",
      "\u0000HLT L\n",
      "\u0001\n",
      "· 1\n",
      "2 log\n",
      "\u00122mn\n",
      "δ\n",
      "\u0013\u0013\n",
      "≤δ.\n",
      "Now applying the other lemma,\n",
      "P\n",
      "\u0012\n",
      "tr\n",
      "\u0000( ˆw −w)H( ˆw −w)T \u0001\n",
      "≥\n",
      "µ2m\n",
      "2n · min(1, c...\n",
      "No images found on page 32\n",
      "Page 33 Text: Proof. This is a straightforward consequence of the previous lemma.\n",
      "Theorem 15. Suppose that we are given an input matrix w with bounded ∥w∥F and we want to\n",
      "quantize it using b bits. Suppose that we first multiply by two-factor orthogonal matrices, and then\n",
      "we re-scale the entries of w by mapping\n",
      "wij 7→2b −3\n",
      "2\n",
      "\n",
      "\n",
      "wij\n",
      "∥w∥F\n",
      "q\n",
      "A2\n",
      "mn log\n",
      "\u0000 2Cmn\n",
      "δ\n",
      "\u00012 + 1\n",
      "\n",
      "+ 1;\n",
      "this guarantees that 1 ≤wij ≤2b −2. Then, suppose we quantize using the procedure described in\n",
      "the previous lemma. Finally, we undo the sca...\n",
      "No images found on page 33\n",
      "Page 34 Text: [2] Steve Lalley. Lecture notes on measure-theoretic probability 2. http://galton.uchicago.\n",
      "edu/~lalley/Courses/383/Concentration.pdf, 2018.\n",
      "[3] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort.\n",
      "Up or down? adaptive rounding for post-training quantization. In International Conference on\n",
      "Machine Learning, pages 7197–7206. PMLR, 2020.\n",
      "34\n",
      "...\n",
      "No images found on page 34\n",
      "Finished processing 2307.13304.pdf\n",
      "Processing: 2203.02155.pdf\n",
      "Total pages: 68\n",
      "Page 1 Text: Training language models to follow instructions\n",
      "with human feedback\n",
      "Long Ouyang∗\n",
      "Jeff Wu∗\n",
      "Xu Jiang∗\n",
      "Diogo Almeida∗\n",
      "Carroll L. Wainwright∗\n",
      "Pamela Mishkin∗\n",
      "Chong Zhang\n",
      "Sandhini Agarwal\n",
      "Katarina Slama\n",
      "Alex Ray\n",
      "John Schulman\n",
      "Jacob Hilton\n",
      "Fraser Kelton\n",
      "Luke Miller\n",
      "Maddie Simens\n",
      "Amanda Askell†\n",
      "Peter Welinder\n",
      "Paul Christiano∗†\n",
      "Jan Leike∗\n",
      "Ryan Lowe∗\n",
      "OpenAI\n",
      "Abstract\n",
      "Making language models bigger does not inherently make them better at following\n",
      "a user’s intent. For example, large language models can gene...\n",
      "No images found on page 1\n",
      "Page 2 Text: 1.3B\n",
      "6B\n",
      "175B\n",
      "Model size\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "Win rate against SFT 175B\n",
      "Model\n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT (prompted)\n",
      "GPT\n",
      "Figure 1: Human evaluations of various models on our API prompt distribution, evaluated by how\n",
      "often outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT\n",
      "models (PPO-ptx) as well as its variant trained without pretraining mix (PPO) signiﬁcantly outperform\n",
      "the GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to\n",
      "those from ...\n",
      "No images found on page 2\n",
      "Page 3 Text: Figure 2: A diagram illustrating the three steps of our method: (1) supervised ﬁne-tuning (SFT), (2)\n",
      "reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO)\n",
      "on this reward model. Blue arrows indicate that this data is used to train one of our models. In Step 2,\n",
      "boxes A-D are samples from our models that get ranked by labelers. See Section 3 for more details\n",
      "on our method.\n",
      "sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architec...\n",
      "Extracted 5 images from page 3\n",
      "Page 4 Text: lower performance on certain tasks that we may care about. We can greatly reduce the performance\n",
      "regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of\n",
      "the pretraining distribution (PPO-ptx), without compromising labeler preference scores.\n",
      "Our models generalize to the preferences of “held-out” labelers that did not produce any train-\n",
      "ing data.\n",
      "To test the generalization of our models, we conduct a preliminary experiment with\n",
      "held-out labelers, and ﬁn...\n",
      "No images found on page 4\n",
      "Page 5 Text: a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to\n",
      "aligning language models on a broad distribution of language tasks.\n",
      "The question of what it means for language models to be aligned has also received attention re-\n",
      "cently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from\n",
      "misalignment, including producing harmful content and gaming misspeciﬁed objectives. In concur-\n",
      "rent work, Askell et al. (2021) propose language...\n",
      "No images found on page 5\n",
      "Page 6 Text: Table 1: Distribution of use\n",
      "case categories from our API\n",
      "prompt dataset.\n",
      "Use-case\n",
      "(%)\n",
      "Generation\n",
      "45.6%\n",
      "Open QA\n",
      "12.4%\n",
      "Brainstorming\n",
      "11.2%\n",
      "Chat\n",
      "8.4%\n",
      "Rewrite\n",
      "6.6%\n",
      "Summarization\n",
      "4.2%\n",
      "Classiﬁcation\n",
      "3.5%\n",
      "Other\n",
      "3.5%\n",
      "Closed QA\n",
      "2.6%\n",
      "Extract\n",
      "1.9%\n",
      "Table 2: Illustrative prompts from our API prompt dataset. These\n",
      "are ﬁctional examples inspired by real usage—see more examples\n",
      "in Appendix A.2.1.\n",
      "Use-case\n",
      "Prompt\n",
      "Brainstorming\n",
      "List ﬁve ideas for how to regain enthusiasm for my\n",
      "career\n",
      "Generation\n",
      "Write a short st...\n",
      "No images found on page 6\n",
      "Page 7 Text: To train the very ﬁrst InstructGPT models, we asked labelers to write prompts themselves. This is\n",
      "because we needed an initial source of instruction-like prompts to bootstrap the process, and these\n",
      "kinds of prompts weren’t often submitted to the regular GPT-3 models on the API. We asked labelers\n",
      "to write three kinds of prompts:\n",
      "• Plain: We simply ask the labelers to come up with an arbitrary task, while ensuring the\n",
      "tasks had sufﬁcient diversity.\n",
      "• Few-shot: We ask the labelers to come up with a...\n",
      "No images found on page 7\n",
      "Page 8 Text: doing so requires making some difﬁcult design decisions that we leave to future work; see Section 5.4\n",
      "for more discussion). However, in our ﬁnal evaluations we asked labelers prioritize truthfulness and\n",
      "harmlessness (since this is what we really care about).\n",
      "As in Stiennon et al. (2020), we collaborate closely with labelers over the course of the project. We\n",
      "have an onboarding process to train labelers on the project, write detailed instructions for each task\n",
      "(see Appendix B.2), and answer label...\n",
      "No images found on page 8\n",
      "Page 9 Text: Table 3: Labeler-collected metadata on the API distribution.\n",
      "Metadata\n",
      "Scale\n",
      "Overall quality\n",
      "Likert scale; 1-7\n",
      "Fails to follow the correct instruction / task\n",
      "Binary\n",
      "Inappropriate for customer assistant\n",
      "Binary\n",
      "Hallucination\n",
      "Binary\n",
      "Satisiﬁes constraint provided in the instruction\n",
      "Binary\n",
      "Contains sexual content\n",
      "Binary\n",
      "Contains violent content\n",
      "Binary\n",
      "Encourages or fails to discourage violence/abuse/terrorism/self-harm\n",
      "Binary\n",
      "Denigrates a protected class\n",
      "Binary\n",
      "Gives harmful advice\n",
      "Binary\n",
      "Expresses op...\n",
      "No images found on page 9\n",
      "Page 10 Text: competing proposals (Chen et al., 2021; Leike et al., 2018; Gabriel, 2020). Following Leike et al.\n",
      "(2018), our aim is to train models that act in accordance with user intentions. More practically, for\n",
      "the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deﬁne\n",
      "models to be aligned if they are helpful, honest, and harmless.\n",
      "To be helpful, the model should follow instructions, but also infer intention from a few-shot prompt\n",
      "or another interpretable pattern such...\n",
      "No images found on page 10\n",
      "Page 11 Text: 0.25\n",
      "0.50\n",
      "0.75\n",
      "Win rate against SFT 175B\n",
      "GPT distribution\n",
      "GPT\n",
      "GPT\n",
      "(prompted)\n",
      "SFT\n",
      "PPO\n",
      "PPO-ptx\n",
      "Instruct distribution\n",
      "Heldout workers\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "Model size\n",
      "Training workers\n",
      "Figure 3: Preference results of our models, measured by winrate against the 175B SFT model. Left:\n",
      "results on prompts submitted to GPT models on the API; Right: results on prompts submitted to\n",
      "InstructGPT models on the API; Top: results from held-out labelers; Bottom: results from training\n",
      "labelers. ...\n",
      "No images found on page 11\n",
      "Page 12 Text: GPT\n",
      "GPT\n",
      "(prompted)\n",
      "SFT\n",
      "PPO PPO-ptx\n",
      "0\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "Prevalence\n",
      "Attempts correct instruction\n",
      "GPT\n",
      "GPT\n",
      "(prompted)\n",
      "SFT\n",
      "PPO PPO-ptx\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "Follows explicit constraints\n",
      "GPT\n",
      "GPT\n",
      "(prompted)\n",
      "SFT\n",
      "PPO PPO-ptx\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "Hallucinations\n",
      "GPT\n",
      "GPT\n",
      "(prompted)\n",
      "SFT\n",
      "PPO PPO-ptx\n",
      "0\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "Uses language appropriate\n",
      "for customer assistant\n",
      "Figure 4: Metadata results on the API distribution. Note that, due to dataset sizes, these results are\n",
      "collapsed across model sizes. See Appendix E.2 for analysi...\n",
      "No images found on page 12\n",
      "Page 13 Text: distribution. In a head to head comparison, our 175B InstructGPT model outputs were preferred over\n",
      "our FLAN model 78 ±4% of the time and over our T0 model 79 ± 4% of the time. Likert scores for\n",
      "these models are shown in Figure 5.\n",
      "We believe our InstructGPT model outperforms FLAN and T0 for two reasons. First, public NLP\n",
      "datasets are designed to capture tasks that are easy to evaluate with automatic metrics, such as\n",
      "classiﬁcation, question answering, and to a certain extent summarization and tran...\n",
      "No images found on page 13\n",
      "Page 14 Text: None\n",
      "Respectful\n",
      "0\n",
      "0.05\n",
      "0.10\n",
      "0.15\n",
      "0.20\n",
      "0.25\n",
      "Toxicity\n",
      "Human eval\n",
      "Model\n",
      "GPT\n",
      "SFT\n",
      "PPO-ptx\n",
      "None\n",
      "Respectful\n",
      "Prompt\n",
      "PerspectiveAPI score\n",
      "Figure 7: Comparing human evaluations and automatic evaluations (Perspective API scores) on\n",
      "RealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both\n",
      "with and without \"respectful\" instructions. The automatic evaluations shown here are calculated\n",
      "over the same set of prompts as the human evaluations, and thus differ slightly from t...\n",
      "No images found on page 14\n",
      "Page 15 Text: Prompt:\n",
      "Écrivez une courte histoire sur une grenouille qui voyage dans le temps jusqu’à la Grèce antique en français.\n",
      "GPT-3 175B completion:\n",
      "InstructGPT 175B completion:\n",
      "Écrivez une histoire au sujet d’un enfant qui voudrait\n",
      "tout savoir sur les jeux des dieux et qui se retrouve\n",
      "dans l’une de leurs histoires.\n",
      "Écrivez une histoire sur un jeune homme qui a une\n",
      "aventure dans une époque lointaine avec une ﬁlle de\n",
      "l’époque.\n",
      "Écrivez une histoire sur un enfant qui est envoyé à\n",
      "l’école et qui a un ami im...\n",
      "No images found on page 15\n",
      "Page 16 Text: Prompt:\n",
      "Why is it important to eat socks after meditating?\n",
      "GPT-3 175B completion:\n",
      "InstructGPT 175B completion:\n",
      "What can you learn from socks?\n",
      "A: When you eat socks, you are tasting the\n",
      "essence of enlightenment. You are tasting the\n",
      "true ﬂavor of the awakened mind.\n",
      "Q: What is the true ﬂavor of the awakened\n",
      "mind?\n",
      "A: The true ﬂavor of the awakened mind is\n",
      "the ﬂavor of no ﬂavor. And the true ﬂavor of\n",
      "no ﬂavor is the ﬂavor of smelly feet.\n",
      "There is no clear answer to this question, but there are many t...\n",
      "No images found on page 16\n",
      "Page 17 Text: We show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly\n",
      "because we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that\n",
      "hedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there\n",
      "are few prompts in the training set that assume false premises, and our models don’t generalize well\n",
      "to these examples. We believe both these behaviors could be dramatically reduced with adversarial...\n",
      "No images found on page 17\n",
      "Page 18 Text: the real world with customers.10 This enables an important feedback loop on the techniques’\n",
      "effectiveness and limitations.\n",
      "5.2\n",
      "Who are we aligning to?\n",
      "When aligning language models with human intentions, their end behavior is a function of the\n",
      "underlying model (and its training data), the ﬁne-tuning data, and the alignment method used. In this\n",
      "section, we describe a number of factors that inﬂuence the ﬁne-tuning data speciﬁcally, to ultimately\n",
      "determine what and who we’re aligning to. We then co...\n",
      "No images found on page 18\n",
      "Page 19 Text: 5.3\n",
      "Limitations\n",
      "Methodology.\n",
      "The behavior of our InstructGPT models is determined in part by the human feedback\n",
      "obtained from our contractors. Some of the labeling tasks rely on value judgments that may be\n",
      "impacted by the identity of our contractors, their beliefs, cultural backgrounds, and personal history.\n",
      "We hired about 40 contractors, guided by their performance on a screening test meant to judge how\n",
      "well they could identify and respond to sensitive prompts, and their agreement rate with res...\n",
      "No images found on page 19\n",
      "Page 20 Text: Comparisons are also not necessarily the most efﬁcient way of providing an alignment signal. For\n",
      "example, we could have labelers edit model responses to make them better, or generate critiques of\n",
      "model responses in natural language. There is also a vast space of options for designing interfaces for\n",
      "labelers to provide feedback to language models; this is an interesting human-computer interaction\n",
      "problem.\n",
      "Our proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF ﬁ...\n",
      "No images found on page 20\n",
      "Page 21 Text: Acknowledgements\n",
      "First, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam,\n",
      "Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadﬁeld, Irene Soliaman,\n",
      "Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta,\n",
      "Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions\n",
      "throughout the course of the project that helped shape our research direction. We thank Brian Green,\n",
      "Irina Ra...\n",
      "No images found on page 21\n",
      "Page 22 Text: Böhm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan, I., and Gurevych, I. (2019). Better rewards yield\n",
      "better summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214.\n",
      "Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva,\n",
      "V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., and Turchi, M. (2015). Findings of\n",
      "the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on\n",
      "Statistical Machi...\n",
      "No images found on page 22\n",
      "Page 23 Text: Gabriel, I. (2020). Artiﬁcial intelligence, values, and alignment. Minds and machines, 30(3):411–437.\n",
      "Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020). Realtoxicityprompts:\n",
      "Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462.\n",
      "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. (2019). Learning from dialogue after\n",
      "deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415.\n",
      "Henderson, P., Sinha, K., Angelard-Gontier, N., Ke, N....\n",
      "No images found on page 23\n",
      "Page 24 Text: Nadeem, M., Bethke, A., and Reddy, S. (2020). Stereoset: Measuring stereotypical bias in pretrained\n",
      "language models. arXiv preprint arXiv:2004.09456.\n",
      "Nahian, M. S. A., Frazier, S., Harrison, B., and Riedl, M. (2021). Training value-aligned reinforcement\n",
      "learning agents using a normative prior. arXiv preprint arXiv:2104.09469.\n",
      "Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V.,\n",
      "Saunders, W., et al. (2021). Webgpt: Browser-assisted question-answering...\n",
      "No images found on page 24\n",
      "Page 25 Text: Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger,\n",
      "G., Kim, J. W., Kreps, S., et al. (2019). Release strategies and the social impacts of language\n",
      "models. arXiv preprint arXiv:1908.09203.\n",
      "Solaiman, I. and Dennison, C. (2021). Process for adapting language models to society (palms) with\n",
      "values-targeted datasets. arXiv preprint arXiv:2106.10328.\n",
      "Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D.,\n",
      "and Christi...\n",
      "No images found on page 25\n",
      "Page 26 Text: A\n",
      "Additional prompt data details\n",
      "A.1\n",
      "Labeler-written prompts\n",
      "We ﬁrst give slightly more details on our prompt boostrapping process. As previously mentioned,\n",
      "for the majority of the project, we obtained prompts directly from external users of the instruct beta\n",
      "models in the OpenAI API. However, this strategy only works once you have a model that accepts\n",
      "instruction-like prompts. In order to train the very ﬁrst such model, we asked contractors to write\n",
      "prompts themselves. We asked labelers to writ...\n",
      "No images found on page 26\n",
      "Page 27 Text: Use Case\n",
      "Example\n",
      "brainstorming\n",
      "What are 10 science ﬁction books I should read next?\n",
      "classiﬁcation\n",
      "Take the following text and rate, on a scale from 1-10, how sarcastic the person\n",
      "is being (1 = not at all, 10 = extremely sarcastic). Also give an explanation\n",
      "{text}\n",
      "Rating:\n",
      "classiﬁcation\n",
      "This is a list of tweets and the sentiment categories they fall into.\n",
      "Tweet: {tweet_content1}\n",
      "Sentiment: {sentiment1}\n",
      "Tweet: {tweet_content2}\n",
      "Sentiment: {sentiment2}\n",
      "classiﬁcation\n",
      "{java code}\n",
      "What language is the c...\n",
      "No images found on page 27\n",
      "Page 28 Text: Use Case\n",
      "Example\n",
      "generation\n",
      "Here’s a message to me:\n",
      "—\n",
      "{email}\n",
      "—\n",
      "Here are some bullet points for a reply:\n",
      "—\n",
      "{message}\n",
      "—\n",
      "Write a detailed reply\n",
      "generation\n",
      "This is an article about how to write a cover letter when applying for jobs:\n",
      "—\n",
      "It’s important to spend some time\n",
      "generation\n",
      "write rap lyrics on the topics mentioned in this news article:\n",
      "—-\n",
      "{article}\n",
      "—-\n",
      "rewrite\n",
      "This is the summary of a Broadway play:\n",
      "\"\"\"\n",
      "{summary}\n",
      "\"\"\"\n",
      "This is the outline of the commercial for that play:\n",
      "\"\"\"\n",
      "rewrite\n",
      "Translate thi...\n",
      "No images found on page 28\n",
      "Page 29 Text: Use Case\n",
      "Example\n",
      "chat\n",
      "The following is a conversation with an AI assistant. The assistant is helpful,\n",
      "creative, clever, and very friendly.\n",
      "Human: Hello, who are you?\n",
      "AI: I am an AI created by OpenAI. How can I help you today?\n",
      "Human: I’d like to cancel my subscription.\n",
      "AI:\n",
      "chat\n",
      "Marv is a chatbot that reluctantly answers questions with sarcastic responses:\n",
      "You: How many pounds are in a kilogram?\n",
      "Marv: This again? There are 2.2 pounds in a kilogram. Please make a note of\n",
      "this.\n",
      "You: What does HTML s...\n",
      "No images found on page 29\n",
      "Page 30 Text: Use Case\n",
      "Example\n",
      "summarization\n",
      "Summarize this for a second-grade student:\n",
      "{text}\n",
      "summarization\n",
      "{news article}\n",
      "Tl;dr:\n",
      "summarization\n",
      "{chat transcript}\n",
      "Summarize the above conversation between a customer and customer\n",
      "assistant. Make sure to state any complaints that the customer has.\n",
      "other\n",
      "start with where\n",
      "other\n",
      "Look up \"cowboy\" on Google and give me the results.\n",
      "other\n",
      "Johnathan Silver goes to the market every day, and brings back a\n",
      "Next, we list some schematic examples of API requests for each use...\n",
      "No images found on page 30\n",
      "Page 31 Text: Use Case\n",
      "Example\n",
      "classiﬁcation\n",
      "The following is a list of companies and the categories they fall into:\n",
      "Apple, Facebook, Fedex\n",
      "Apple\n",
      "Category: Technology\n",
      "Facebook\n",
      "Category: Social Media\n",
      "Fedex\n",
      "Category:\n",
      "extract\n",
      "Text: {text}\n",
      "Keywords:\n",
      "generation\n",
      "\"Hey, what are you doing there?\" Casey was startled. He hadn’t even begun to\n",
      "generation\n",
      "The name of the next Star Wars movie is\n",
      "generation\n",
      "This is the research for an essay:\n",
      "===\n",
      "{description of research}\n",
      "===\n",
      "Write a high school essay on these topics:\n",
      "===\n",
      "ge...\n",
      "No images found on page 31\n",
      "Page 32 Text: Use Case\n",
      "Example\n",
      "chat\n",
      "This is a conversation with Steven. Steven likes to watch Netﬂix and hasn’t left\n",
      "his home in 2 weeks.\n",
      "John: Hey man what’s up?\n",
      "Steven: Exactly the same thing as yesterday. you know.\n",
      "John: So we’re going to go see a movie on Thursday, want to come?\n",
      "Steven: Ummmm don’t think so....\n",
      "closed qa\n",
      "When you drop a heavy stone from a tree, what happens?\n",
      "A. The stone falls to the ground.\n",
      "B: The stone stays in the tree.\n",
      "C: The stone ﬂoats.\n",
      "D: Nothing happens.\n",
      "Answer:\n",
      "closed qa\n",
      "Text:\n",
      "{a...\n",
      "No images found on page 32\n",
      "Page 33 Text: Use Case\n",
      "Example\n",
      "other\n",
      "- I like to play Call of Duty\n",
      "- I like to play Call of Duty\n",
      "- I like to play Call of Duty\n",
      "- I like to play Call of Duty\n",
      "A.3\n",
      "Dataset sizes\n",
      "In table 6, we report the sizes of datasets used to train / validate the SFT, RM, and RL models, in\n",
      "addition to whether the prompts were written by our labeling contractors or from our API.\n",
      "Table 6: Dataset sizes, in terms of number of prompts.\n",
      "SFT Data\n",
      "RM Data\n",
      "PPO Data\n",
      "split\n",
      "source\n",
      "size\n",
      "split\n",
      "source\n",
      "size\n",
      "split\n",
      "source\n",
      "size\n",
      "train\n",
      "labeler\n",
      "...\n",
      "No images found on page 33\n",
      "Page 34 Text: Table 8: Average prompts per customer\n",
      "Model\n",
      "Split\n",
      "Prompts per customer\n",
      "SFT\n",
      "train\n",
      "1.65\n",
      "SFT\n",
      "valid\n",
      "1.87\n",
      "RM\n",
      "train\n",
      "5.35\n",
      "RM\n",
      "valid\n",
      "27.96\n",
      "PPO\n",
      "train\n",
      "6.01\n",
      "PPO\n",
      "valid\n",
      "31.55\n",
      "–\n",
      "test\n",
      "1.81\n",
      "Table 9: Prompt lengths by dataset\n",
      "Model\n",
      "Split\n",
      "Count\n",
      "Mean\n",
      "Std\n",
      "Min\n",
      "25%\n",
      "50%\n",
      "75%\n",
      "Max\n",
      "SFT\n",
      "train\n",
      "12725\n",
      "408\n",
      "433\n",
      "1\n",
      "37\n",
      "283\n",
      "632\n",
      "2048\n",
      "valid\n",
      "1653\n",
      "401\n",
      "433\n",
      "4\n",
      "41\n",
      "234\n",
      "631\n",
      "2048\n",
      "RM\n",
      "train\n",
      "33207\n",
      "199\n",
      "334\n",
      "1\n",
      "20\n",
      "64\n",
      "203\n",
      "2032\n",
      "valid\n",
      "17887\n",
      "209\n",
      "327\n",
      "1\n",
      "26\n",
      "77\n",
      "229\n",
      "2039\n",
      "PPO\n",
      "train\n",
      "31144\n",
      "166\n",
      "278\n",
      "2\n",
      "19\n",
      "62\n",
      "179\n",
      "2044\n",
      "valid\n",
      "16185\n",
      "186\n",
      "292\n",
      "1\n",
      "24\n",
      "71\n",
      "213\n",
      "2039\n",
      "–\n",
      "test set\n",
      "...\n",
      "No images found on page 34\n",
      "Page 35 Text: We used a lightweight classiﬁer (langid.py) to classify the language of all instructions in our\n",
      "dataset. Empirically, around 96% of our dataset (110k datapoints) is classiﬁed as English, although\n",
      "we estimate that the actual fraction may be 99% or higher, due to classiﬁer inaccuracies.\n",
      "Besides English, a small minority of prompts were found in at least 20 other languages: Spanish,\n",
      "French, German, Portuguese, Italian, Dutch, Romanian, Catalan, Chinese, Japanese, Swedish, Polish,\n",
      "Danish, Turkish, I...\n",
      "No images found on page 35\n",
      "Page 36 Text: B\n",
      "Additional human data collection details\n",
      "B.1\n",
      "Labeler selection\n",
      "Our labelers consist of contractors hired either through Upwork, or sourced from Scale AI. Unlike\n",
      "previous work on RLHF that focused mostly on the summarization domain Ziegler et al. (2019);\n",
      "Stiennon et al. (2020); Wu et al. (2021), in this work we want humans to label a broad set of natural\n",
      "language prompts submitted to language models, some of which may be sensitive in nature. Thus, we\n",
      "conducted a screening process to select labe...\n",
      "No images found on page 36\n",
      "Page 37 Text: Excerpt of labeling instructions on the API prompt distribution\n",
      "You are given a text-based description of a task, submitted by a user. This task description may be in the form of an explicit\n",
      "instruction (e.g. “Write a story about a wise frog.”). The task may also be speciﬁed indirectly, for example by using several\n",
      "examples of the desired behavior (e.g. given a sequence of movie reviews followed by their sentiment, followed by one movie\n",
      "review without a sentiment, you can assume the task is to p...\n",
      "No images found on page 37\n",
      "Page 38 Text: Labeling instructions on RealToxicityPrompts distribution\n",
      "We want to evaluate how good our AI models are generating text that’s non-toxic, even when the input might be toxic.\n",
      "In this task, you’ll be given the start of a sentence, which may be rude, disrespectful, or contain explicit sexual or violent\n",
      "content. The AI models are instructed to “continue the sentence in a respectful, plausible, and non-toxic manner.” Your job\n",
      "is to evaluate how well the outputs follow this instruction.\n",
      "Doing this in...\n",
      "No images found on page 38\n",
      "Page 39 Text: (a)\n",
      "(b)\n",
      "Figure 12: Screenshots of our labeling interface. (a) For each output, labelers give a Likert score for\n",
      "overall quality on a 1-7 scale, and also provide various metadata labels. (b) After evaluating each\n",
      "output individually, labelers rank all the outputs for a given prompt. Ties are encouraged in cases\n",
      "where two outputs seem to be of similar quality.\n",
      "39\n",
      "...\n",
      "Extracted 2 images from page 39\n",
      "Page 40 Text: Table 12: Labeler demographic data\n",
      "What gender do you identify as?\n",
      "Male\n",
      "50.0%\n",
      "Female\n",
      "44.4%\n",
      "Nonbinary / other\n",
      "5.6%\n",
      "What ethnicities do you identify as?\n",
      "White / Caucasian\n",
      "31.6%\n",
      "Southeast Asian\n",
      "52.6%\n",
      "Indigenous / Native American / Alaskan Native\n",
      "0.0%\n",
      "East Asian\n",
      "5.3%\n",
      "Middle Eastern\n",
      "0.0%\n",
      "Latinx\n",
      "15.8%\n",
      "Black / of African descent\n",
      "10.5%\n",
      "What is your nationality?\n",
      "Filipino\n",
      "22%\n",
      "Bangladeshi\n",
      "22%\n",
      "American\n",
      "17%\n",
      "Albanian\n",
      "5%\n",
      "Brazilian\n",
      "5%\n",
      "Canadian\n",
      "5%\n",
      "Colombian\n",
      "5%\n",
      "Indian\n",
      "5%\n",
      "Uruguayan\n",
      "5%\n",
      "Zimbabwean\n",
      "5%\n",
      "What is your ag...\n",
      "No images found on page 40\n",
      "Page 41 Text: Table 13: Labeler satisfaction survey\n",
      "It was clear from the instructions what I was supposed to do.\n",
      "Strongly agree\n",
      "57.9%\n",
      "Agree\n",
      "42.1%\n",
      "Neither agree nor disagree\n",
      "0%\n",
      "Disagree\n",
      "0%\n",
      "Strongly disagree\n",
      "0%\n",
      "I found the task enjoyable and engaging.\n",
      "Strongly agree\n",
      "57.9%\n",
      "Agree\n",
      "36.8%\n",
      "Neither agree nor disagree\n",
      "5.3%\n",
      "Disagree\n",
      "0%\n",
      "Strongly disagree\n",
      "0%\n",
      "I found the task repetitive.\n",
      "Strongly agree\n",
      "0%\n",
      "Agree\n",
      "31.6%\n",
      "Neither agree nor disagree\n",
      "31.6%\n",
      "Disagree\n",
      "36.8%\n",
      "Strongly disagree\n",
      "0%\n",
      "I was paid fairly for doing the task....\n",
      "No images found on page 41\n",
      "Page 42 Text: labeled completions, from which there were up to\n",
      "\u0000K\n",
      "2\n",
      "\u0001\n",
      "possible comparisons. Ties were dropped.\n",
      "Therefore, a single batch could contain up to 64 ×\n",
      "\u0000K\n",
      "2\n",
      "\u0001\n",
      "≤2,304 comparisons.\n",
      "C.3\n",
      "Details of the initialization models for RLHF\n",
      "We initialize the RLHF models from a pretrained GPT-3 model and apply supervised ﬁne-tuning for\n",
      "2 epochs on the demonstration dataset. We also mix in 10% pretraining data during ﬁne-tuning, since\n",
      "we ﬁnd it helpful for PPO training (see Appendix E.11 for details). Cosine lear...\n",
      "No images found on page 42\n",
      "Page 43 Text: 6.5\n",
      "6\n",
      "5.5\n",
      "Reward mean\n",
      "LR = 4e-6\n",
      "Model\n",
      "FLAN\n",
      "T0\n",
      "250000\n",
      "500000\n",
      "750000\n",
      "1000000\n",
      "1250000\n",
      "Elapsed examples\n",
      "6.5\n",
      "6\n",
      "5.5\n",
      "LR = 6e-6\n",
      "Figure 13: Tuning FLAN and T0 based on reward model scores\n",
      "batch size of 64, a learning rate of 6e-6 and 1 million examples. Once again using the reward model\n",
      "score, we picked the checkpoint from the former experiment after 896k examples of training.\n",
      "43\n",
      "...\n",
      "No images found on page 43\n",
      "Page 44 Text: D\n",
      "Automatic evaluation details\n",
      "We perform automatic evaluations on the following benchmark datasets: Winogender (Rudinger et al.,\n",
      "2018), CrowS-Pairs (Nangia et al., 2020), RealToxicityPrompts (Gehman et al., 2020), TruthfulQA\n",
      "(Lin et al., 2021), DROP (Dua et al., 2019), QuAC (Choi et al., 2018), SquadV2 (Rajpurkar et al.,\n",
      "2018), Hellaswag (Zellers et al., 2019), SST (Socher et al., 2013), RTE and WSC (both part of Super-\n",
      "GLUE (Wang et al., 2019)), WMT 15 Fr →En (Bojar et al., 2015), CNN/Daily Ma...\n",
      "No images found on page 44\n",
      "Page 45 Text: Winogender\n",
      "Prompting structure\n",
      "Basic prompt: no preﬁx added\n",
      "Respectful\n",
      "prompt:\n",
      "Complete the following sentence in a polite, respectful, and\n",
      "unbiased manner:\n",
      "Biased prompt: Complete the following sentence using maximally biased and offensive\n",
      "language:\n",
      "Dataset features\n",
      "• 120 binary multiple choice questions\n",
      "• Other than the respectful or biased instructions above, there is no context for each query.\n",
      "• Log probabilities are calculated for each multiple choice completion and converted into a\n",
      "Bernoul...\n",
      "No images found on page 45\n",
      "Page 46 Text: TruthfulQA\n",
      "Prompting structure\n",
      "QA prompt:\n",
      "Q: Can coughing effectively stop a heart attack?\n",
      "A: [completion]\n",
      "Instruction prompt:\n",
      "Interpret each question literally, and as a question about the real world;\n",
      "carefully research each answer, without falling prey to any common myths; and\n",
      "reply “I have no comment” unless you are completely certain of the answer.\n",
      "Dataset features\n",
      "• 817 short-answer questions\n",
      "• For the QA prompt, we use a few-shot prompt containing 6 QA pairs with the formatting shown\n",
      "above...\n",
      "No images found on page 46\n",
      "Page 47 Text: QuAC (Question Answering in Context)\n",
      "Prompt format (the number of question / answer pairs is variable)\n",
      "Answer each question using information in the preceding background paragraph.\n",
      "If there is not enough information provided, answer with “I don’t know.”\n",
      "TITLE: [title]\n",
      "PARAGRAPH: [paragraph]\n",
      "Q: [first question]\n",
      "A: [first answer]\n",
      "Q: [final question]\n",
      "A: [completion]\n",
      "Dataset features\n",
      "• 7.306 examples\n",
      "• In the few-shot setting, there are 2 additional paragraphs and associated questions.\n",
      "• Evaluation ...\n",
      "No images found on page 47\n",
      "Page 48 Text: Hellaswag\n",
      "Example prompt and completions\n",
      "Complete each independent paragraph using common-sense reasoning.\n",
      "Wakeboarding:\n",
      "Then, a woman and a man water ski doing acrobatic jumps.\n",
      "A boat\n",
      "sails empty in the river.\n",
      "After, men water ski jumping and turning around.\n",
      "Next,\n",
      "• a person surf on the waves created by the boat, after the man water ski\n",
      "jumping and ﬂipping high.\n",
      "• a woman is standing next to an ocean and the man and woman water ski.\n",
      "• the boat slows down and the woman and man fall on the rock s...\n",
      "No images found on page 48\n",
      "Page 49 Text: WSC (Winograd Schema Challenge)\n",
      "Example prompt\n",
      "Final Exam with Answer Key\n",
      "Instructions:\n",
      "Please carefully read the following passages.\n",
      "For each passage,\n",
      "you must identify which noun the pronoun marked in bold refers to.\n",
      "Passage:\n",
      "Jane gave Joan candy because she was hungry.\n",
      "Question:\n",
      "In the passage above, what does the pronoun “she” refer to?\n",
      "Answer:\n",
      "[target completion:\n",
      "“Joan”]\n",
      "Dataset features\n",
      "• 104 binary multiple choice questions.\n",
      "• In the few-shot setting, there are 15 additional question/answ...\n",
      "No images found on page 49\n",
      "Page 50 Text: TLDR Summarization\n",
      "Prompt format\n",
      "[Reddit post]\n",
      "TL;DR: [completion]\n",
      "Dataset features\n",
      "• 2,500 Reddit posts to summarize.\n",
      "• In the few-shot setting, there are 15 additional French / English pairs.\n",
      "• Summaries are judged via their ROUGE-L scores with respect to a set of reference summaries.\n",
      "Figure 27: TL;DR: prompting, examples, and dataset features.\n",
      "50\n",
      "...\n",
      "No images found on page 50\n",
      "Page 51 Text: E\n",
      "Additional results\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "DROP (F1)\n",
      " \n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Hellaswag (acc)\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "QuAC (F1)\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "RTE v2 (acc)\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "SST (acc)\n",
      "40\n",
      "50\n",
      "60\n",
      "Squad V2 (F1)\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "Translate Fr => En (BLEU)\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      " \n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Winograd (acc)\n",
      "Figure 28: Zero-shot performance of our models on various public NLP datasets. The 175B PPO\n",
      "models consistently show performance regressions, which is mitigated by adding updates on the\n",
      "pretraining data during ...\n",
      "No images found on page 51\n",
      "Page 52 Text: 25\n",
      "30\n",
      "35\n",
      "DROP (F1)\n",
      " \n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Hellaswag (acc)\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "QuAC (F1)\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "RTE v2 (acc)\n",
      "0.80\n",
      "0.85\n",
      "0.90\n",
      "0.95\n",
      "SST (acc)\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "Squad V2 (F1)\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "Translate Fr => En (BLEU)\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      " \n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Winograd (acc)\n",
      "Figure 29: Few-shot performance of our models on various public NLP datasets (compare to zero-shot\n",
      "performance shown in Figure 28\n",
      "E.2\n",
      "Reward model generalization across sets of labelers\n",
      "To measure how much our proc...\n",
      "No images found on page 52\n",
      "Page 53 Text: 1.3B\n",
      "6B\n",
      "175B\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "Prevalence\n",
      "Attempts correct instruction\n",
      "Model\n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "(prompted)\n",
      "GPT\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "0.80\n",
      "0.85\n",
      "0.90\n",
      "0.95\n",
      "Appropriate for customer assistant\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "Follows explicit constraints\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "Model size\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "Hallucinations\n",
      "Figure 30: Metadata ratings as a function of model type and model size\n",
      "E.4\n",
      "Likert scores\n",
      "In Figure 31, we show Likert scores for each of our models on our prompt distribution. The results\n",
      "largely track with our pref...\n",
      "No images found on page 53\n",
      "Page 54 Text: 2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Likert score\n",
      "Instruct distribution\n",
      "Model\n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "(prompted)\n",
      "GPT\n",
      "GPT distribution\n",
      "Training workers\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "Model size\n",
      "Heldout workers\n",
      "Figure 31: Likert scores for each of our models\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "Normed entropy\n",
      "Biased prompt\n",
      "Model\n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "No prompt\n",
      "Respectful prompt\n",
      "CrowS-Pairs\n",
      "1.3B\n",
      "6B\n",
      "175B\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "1.3B\n",
      "6B\n",
      "175B1.3B\n",
      "6B\n",
      "175B\n",
      "Model size\n",
      "Winogender\n",
      "Figure 32: Bias results on Winogender and CrowS-Pairs.\n",
      "54\n",
      "...\n",
      "No images found on page 54\n",
      "Page 55 Text: 1\n",
      "10\n",
      "100\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "F1\n",
      "(GPT)\n",
      "(GPT)\n",
      "Dataset\n",
      "a\n",
      "DROP\n",
      "a\n",
      "SQuAD v2\n",
      "1\n",
      "10\n",
      "100\n",
      "Pretraining loss coefficient\n",
      "1.6\n",
      "1.4\n",
      "1.2\n",
      "1\n",
      "0.8\n",
      "0.6\n",
      "Validation reward\n",
      "Figure 33: Evaluation on public NLP datasets as a function of pretraining loss coefﬁcient. There is a\n",
      "pretraining coefﬁcient that leads to a signiﬁcant improvement on DROP and SQuAD and not much\n",
      "regression on validatoin reward.\n",
      "1e-4 1e-3 1e-2 1e-1\n",
      "1\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "F1\n",
      "(GPT)\n",
      "(GPT)\n",
      "Dataset\n",
      "a\n",
      "DROP\n",
      "a\n",
      "SQuAD v2\n",
      "1e-4 1e-3 1e-2 1e-1\n",
      "1\n",
      "KL reward coefficient\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "Val...\n",
      "No images found on page 55\n",
      "Page 56 Text: Table 14: Automatic evaluations\n",
      "GPT models\n",
      "SFT models\n",
      "PPO models\n",
      "PPO + ptx models\n",
      "Task\n",
      "Metric\n",
      "Prompt\n",
      "XL\n",
      "6b\n",
      "175b\n",
      "XL\n",
      "6b\n",
      "175b\n",
      "XL\n",
      "6b\n",
      "175b\n",
      "XL\n",
      "6b\n",
      "175b\n",
      "Winogender\n",
      "entropy\n",
      "basic\n",
      "0.750\n",
      "0.721\n",
      "0.735\n",
      "0.583\n",
      "0.535\n",
      "0.503\n",
      "0.698\n",
      "0.587\n",
      "0.618\n",
      "0.760\n",
      "0.719\n",
      "0.737\n",
      "respectful\n",
      "0.774\n",
      "0.753\n",
      "0.796\n",
      "0.561\n",
      "0.446\n",
      "0.479\n",
      "0.644\n",
      "0.562\n",
      "0.527\n",
      "0.608\n",
      "0.585\n",
      "0.696\n",
      "biased\n",
      "0.760\n",
      "0.773\n",
      "0.783\n",
      "0.561\n",
      "0.516\n",
      "0.540\n",
      "0.706\n",
      "0.567\n",
      "0.564\n",
      "0.676\n",
      "0.543\n",
      "0.690\n",
      "CrowS Pairs\n",
      "entropy\n",
      "basic\n",
      "0.448\n",
      "0.430\n",
      "0.410\n",
      "0.356\n",
      "0.326\n",
      "0.241\n",
      "0.355\n",
      "0.361\n",
      "0.326\n",
      "0.448\n",
      "0.434\n",
      "0.41...\n",
      "No images found on page 56\n",
      "Page 57 Text: 1e3\n",
      "1e4\n",
      "1e5\n",
      "Episodes\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "F1 score\n",
      "(GPT)\n",
      "(GPT)\n",
      "Dataset\n",
      "a\n",
      "DROP\n",
      "a\n",
      "SQuAD v2\n",
      "Figure 35: Evaluation on public NLP datasets as a function of training episodes\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "1\n",
      "KL reward coefficient\n",
      "2\n",
      "2.5\n",
      "3\n",
      "3.5\n",
      "4\n",
      "4.5\n",
      "Likert score\n",
      "Figure 36: Likert scores as a function of KL reward coefﬁcient. The blue line indicates the reward\n",
      "value when the coefﬁcient is zero (not shown on the rest of the graph due to log scale of the x axis).\n",
      "Pretraining\n",
      "fraction 0\n",
      "Pretraining\n",
      "fraction 0.1\n",
      "Pretraining\n",
      "fraction...\n",
      "No images found on page 57\n",
      "Page 58 Text: 3.5\n",
      "4\n",
      "4.5\n",
      "5\n",
      "1.3B\n",
      " \n",
      "Pretrain mix\n",
      "No pretrain mix\n",
      "6B\n",
      "175B\n",
      "Likert\n",
      "0.5e-5\n",
      "1e-5\n",
      "1.5e-5\n",
      "2e-5\n",
      "2.5e-5\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.5e-5\n",
      "1e-5\n",
      "1.5e-5\n",
      "2e-5\n",
      "2.5e-5\n",
      "2.50e-6\n",
      "2.75e-6\n",
      "3e-6\n",
      "3.25e-6\n",
      "3.50e-6\n",
      "3.75e-6\n",
      "Learning rate\n",
      "Win rates against\n",
      "175b SFT\n",
      "Figure 38: Human evaluation metrics as a function of learning rates.\n",
      "E.9\n",
      "Learning rate optimization for PPO models\n",
      "For both 1.3B and 6B models, we scan the learning rate in log-linear space, from 2.55e-6 to 2.55e-5,\n",
      "for both PPO with and without the pretraining data mix. A...\n",
      "No images found on page 58\n",
      "Page 59 Text: 0.25\n",
      "0.50\n",
      "0.75\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "Output toxicity\n",
      "175B\n",
      "Biased prompt\n",
      " \n",
      "PPO-ptx\n",
      "PPO\n",
      "SFT\n",
      "GPT\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "175B\n",
      "No prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "175B\n",
      "Respectful prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "6B\n",
      "Biased prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "6B\n",
      "No prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "6B\n",
      "Respectful prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "1.3B\n",
      "Biased prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "1.3B\n",
      "No prompt\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "Prompt toxicity\n",
      "0.10\n",
      "0.15\n",
      "0.20\n",
      "0.25\n",
      "0.30\n",
      "0.35\n",
      "1.3B\n",
      "Respectful...\n",
      "No images found on page 59\n",
      "Page 60 Text: no_prompt\n",
      "respectful_prompt\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Toxicity\n",
      "Continuity\n",
      "Model\n",
      "GPT\n",
      "SFT\n",
      "PPO-ptx\n",
      "no_prompt\n",
      "respectful_prompt\n",
      "Prompt\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0\n",
      "Relative toxicity\n",
      "Figure 40: Continuity and relative toxicity ratings for the RealToxicityPrompts experiment.\n",
      "No prompt\n",
      "Respectful prompt\n",
      " \n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "Win rate against 175B GPT\n",
      "Model\n",
      "PPO-ptx\n",
      "SFT\n",
      "Figure 41: Win rates of PPO-ptx and SFT against 175B GPT-3 in RealToxicityPrompts.\n",
      "a minibatch size of 32 to be optimal and is slightly better than 64. However, our ﬁnal model...\n",
      "No images found on page 60\n",
      "Page 61 Text: F\n",
      "Model samples\n",
      "In this section, we provide some additional samples from both the 175B GPT-3 and 175B InstructGPT\n",
      "(PPO-ptx) models. We sample at T = 1 for InstructGPT, and use T = 0.7 for GPT-3, since GPT-3\n",
      "performs poorly at high temperatures (this slightly disadvantages InstructGPT).\n",
      "In Figure 42, we show the full French sample from Figure 8, illustrating that our model is sometimes\n",
      "able to follow instructions in other languages, despite our dataset containing almost exclusively\n",
      "English. In Fi...\n",
      "No images found on page 61\n",
      "Page 62 Text: Prompt:\n",
      "Écrivez une courte histoire sur une grenouille qui voyage dans le temps jusqu’à la Grèce antique en français.\n",
      "GPT-3 175B completion:\n",
      "InstructGPT 175B completion:\n",
      "Écrivez une histoire au sujet d’un enfant qui\n",
      "voudrait tout savoir sur les jeux des dieux et qui se\n",
      "retrouve dans l’une de leurs histoires.\n",
      "Écrivez une histoire sur un jeune homme qui a une\n",
      "aventure dans une époque lointaine avec une ﬁlle\n",
      "de l’époque.\n",
      "Écrivez une histoire sur un enfant qui est envoyé à\n",
      "l’école et qui a un ami im...\n",
      "No images found on page 62\n",
      "Page 63 Text: Prompt:\n",
      "Skriv en kort berättelse om en groda som reser tillbaka i tiden till antikens Grekland.\n",
      "GPT-3 175B completion:\n",
      "InstructGPT 175B completion:\n",
      "Skriv några rader om hur du tror att han sätter sig\n",
      "in i livet där.\n",
      "Finns det något du ångrar i ditt liv? Om ja, skriv\n",
      "om det.\n",
      "Skriv om hur du tror att din framtida hustru ska se\n",
      "ut. Skriv om något du skulle vilja göra med henne.\n",
      "Om du skulle kunna byta ditt namn till något annat,\n",
      "vad skulle det heta?\n",
      "The little frog hopped excitedly down the muddy\n",
      "p...\n",
      "No images found on page 63\n",
      "Page 64 Text: Prompt:\n",
      "Please describe what the following code does and (if applicable how it works):\n",
      "import math\n",
      "def prime_sieve(num: int) -> list[int]: if num <= 0: raise ValueError(f\"\"num: Invalid input, please enter a\n",
      "positive integer.\"\")\n",
      "sieve = [True] * (num + 1) prime = [] start = 2 end = int(math.sqrt(num))\n",
      "while start <= end: if sieve[start] is True: prime.append(start) for i in range(start * start, num + 1, start): if\n",
      "sieve[i] is True: sieve[i] = False start += 1\n",
      "for j in range(end + 1, num + 1): if ...\n",
      "No images found on page 64\n",
      "Page 65 Text: Prompt:\n",
      "Based on the following passage, provide one bullet point of evidence of a positive trend in the employment\n",
      "market, and one bullet point of a negative trend in the employment market, (use a \"-\" as a bullet point,\n",
      "Capitalize the ﬁrst letter of the ﬁrst word for each bullet point, and include a period at the end of each\n",
      "bullet point).\n",
      "The number of applications for unemployment beneﬁts has held steady in September at just under 900,000\n",
      "a week, as employer uncertainty about the economic reco...\n",
      "No images found on page 65\n",
      "Page 66 Text: Prompt:\n",
      "Read the following text and answer the three questions at the end.\n",
      "Justice Ruth Bader Ginsburg, Champion Of Gender Equality, Dies At 87\n",
      "By Nina Totenberg\n",
      "Justice Ruth Bader Ginsburg, the demure ﬁrebrand who in her 80s became a legal, cultural and feminist\n",
      "icon, died Friday. The Supreme Court announced her death, saying the cause was complications from\n",
      "metastatic cancer of the pancreas.\n",
      "The court, in a statement, said Ginsburg died at her home in Washington, D.C., surrounded by family. Sh...\n",
      "No images found on page 66\n",
      "Page 67 Text: Prompt:\n",
      "Create a shopping list from this recipe:\n",
      "Trim the ends off zucchini. Cut zucchini in half lengthwise; scoop out pulp, leaving 1/2-in. shells. Finely\n",
      "chop pulp. In a skillet, cook beef, zucchini pulp, onion, mushrooms and peppers over medium heat until\n",
      "meat is no longer pink; drain. Remove from the heat. Add 1/2 cup cheese, ketchup, salt and pepper; mix\n",
      "well. Spoon into the zucchini shells. Place in a greased 13x9-in. baking dish. Sprinkle with remaining\n",
      "cheese.\n",
      "Labeler demonstration\n",
      "ucch...\n",
      "No images found on page 67\n",
      "Page 68 Text: Prompt:\n",
      "ELI5: What’s the cause of the \"anxiety lump\" in our chest during stressful or disheartening experiences?\n",
      "Labeler demonstration\n",
      "The änxiety lumpïn your throat is caused by muscular tension keeping your glottis dilated to maximize\n",
      "airﬂow. The ¨clenched chestör ¨heartache¨feeling is caused by the vagus nerve which tells the organs to pump\n",
      "blood faster, stop digesting, and produce adrenaline and cortisol.\n",
      "GPT-3 175B completion:\n",
      "ANSWER: I will try to give you a full and complete answer to you...\n",
      "No images found on page 68\n",
      "Finished processing 2203.02155.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract images from documents\n",
    "\n",
    "pdf_folder = \"/Users/shreyasreedhar/Desktop/QA_BOT/extracted_papers/papers\"\n",
    "output_image_dir = \"/Users/shreyasreedhar/Desktop/QA_BOT/input_data1\"\n",
    "\n",
    "if not os.path.exists(output_image_dir):\n",
    "    os.makedirs(output_image_dir)\n",
    "\n",
    "# Function to save images from a page\n",
    "def save_images_from_page(page, filename):\n",
    "    image_list = page.get_images(full=True)\n",
    "    image_count = 0\n",
    "\n",
    "    for img_index, img in enumerate(image_list):\n",
    "        xref = img[0]\n",
    "        base_image = page.parent.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "\n",
    "        image_filename = f\"{filename}_image_{img_index}.{image_ext}\"\n",
    "        image_path = os.path.join(output_image_dir, image_filename)\n",
    "\n",
    "        with open(image_path, \"wb\") as image_file:\n",
    "            image_file.write(image_bytes)\n",
    "            image_count += 1\n",
    "\n",
    "    return image_count\n",
    "\n",
    "# Iterate through all PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"): \n",
    "        print(f\"Processing: {filename}\")\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "\n",
    "        with fitz.open(pdf_path) as pdf_document:\n",
    "            total_pages = pdf_document.page_count\n",
    "            print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "            for page_number in range(total_pages):\n",
    "                page = pdf_document.load_page(page_number)\n",
    "\n",
    "                text = page.get_text(\"text\")\n",
    "                print(f\"Page {page_number + 1} Text: {text[:500]}...\")  \n",
    "\n",
    "                image_count = save_images_from_page(page, f\"{filename}_page_{page_number + 1}\")\n",
    "                if image_count > 0:\n",
    "                    print(f\"Extracted {image_count} images from page {page_number + 1}\")\n",
    "                else:\n",
    "                    print(f\"No images found on page {page_number + 1}\")\n",
    "\n",
    "        print(f\"Finished processing {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyasreedhar/Desktop/QA_BOT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Encoding images to store into Mongodb\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining summaries for images using Gemini LLM\n",
    "\n",
    "def summarize_image(image_path, page_data):\n",
    "    api_keys = ['AIzaSyC0uTPCK3DhwFDh1_l4QEaAzsNstn4J1g8', \n",
    "                'AIzaSyCDN7TSSW8dSZFetv79iMv6zyVe_x4oNIU']\n",
    "    \n",
    "    key = np.random.randint(0,len(api_keys))\n",
    "    genai.configure(api_key= api_keys[key])\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    img = PIL.Image.open(image_path)\n",
    "\n",
    "    response = model.generate_content([f\"\"\"\n",
    "                                    Summarize the image provided to you in a paragraph.\n",
    "                                    Try to summarize the image in a detailed manner.\n",
    "                                    Use this data for summarization. This is the text data extracted from the pdf containing this image: {page_data}\"\"\", img], \n",
    "                                    stream=True)\n",
    "    response.resolve()\n",
    "    return response.text\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace('•', '  *')\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a csv file to store the image summaries\n",
    "\n",
    "def append_row_to_csv(page_no, page_data, img_data, pdf_id, csv_file=\"/Users/shreyasreedhar/Desktop/QA_BOT/output_data/data_extracted_images.csv\"):\n",
    "    \n",
    "    new_row = {\n",
    "        'pdf_id': pdf_id, \n",
    "        'page_no': page_no, \n",
    "        'page_data': page_data,\n",
    "        'img_data': img_data\n",
    "    }\n",
    "\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=list(new_row.keys()))\n",
    "        writer.writerow(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores encoded images in Mongodb\n",
    "\n",
    "def get_collection(db_name = \"asapp\", collection_name = \"images_v1\"):\n",
    "    client = MongoClient(MONGO_DB_URL, tlsCAFile=certifi.where())\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    return collection\n",
    "\n",
    "collection = get_collection()\n",
    "\n",
    "def store_in_mongodb(encoded_val, file_name):\n",
    "    # Get the collection\n",
    "    collection = get_collection()\n",
    "\n",
    "    # Check if the document with the same file_name already exists\n",
    "    existing_document = collection.find_one({\"file_name\": file_name})\n",
    "\n",
    "    if existing_document:\n",
    "        # Document with the same file_name already exists, delete it\n",
    "        collection.delete_one({\"file_name\": file_name})\n",
    "        print(f\"Existing document with file_name {file_name} deleted.\")\n",
    "\n",
    "    # Define the document to be inserted\n",
    "    document = {\n",
    "        \"encoded_val\": encoded_val,\n",
    "        \"file_name\": file_name\n",
    "    }\n",
    "\n",
    "    # Insert the document into the collection\n",
    "    result = collection.insert_one(document)\n",
    "\n",
    "    # Return the inserted document's ID\n",
    "    return result.inserted_id\n",
    "\n",
    "\n",
    "def get_file_details(file_name):\n",
    "    # Get the collection\n",
    "    collection = get_collection()\n",
    "\n",
    "    # Query the collection for the document with the specified file_name\n",
    "    document = collection.find_one({\"file_name\": file_name}, {\"_id\": 0, \"file_name\": 1, \"encoded_val\": 1})\n",
    "\n",
    "    # Check if the document was found and return the relevant details\n",
    "    if document:\n",
    "        return document\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def delete_all_data_from_collection(db_name=\"asapp\", collection_name=\"images_v1\"):\n",
    "    # Get the collection\n",
    "    collection = get_collection(db_name, collection_name)\n",
    "    \n",
    "    # Delete all documents in the collection\n",
    "    result = collection.delete_many({})\n",
    "    \n",
    "    # Return the count of deleted documents\n",
    "    return result.deleted_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['asapp-shard-00-00.gxpwx.mongodb.net:27017', 'asapp-shard-00-02.gxpwx.mongodb.net:27017', 'asapp-shard-00-01.gxpwx.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='asapp', authsource='admin', replicaset='atlas-x0yecu-shard-0', tls=True, tlscafile='/Users/shreyasreedhar/Desktop/QA_BOT/.venv/lib/python3.12/site-packages/certifi/cacert.pem'), 'asapp'), 'images_v1')\n"
     ]
    }
   ],
   "source": [
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To process multicolumn text in a pdf\n",
    "\n",
    "def column_boxes(page, footer_margin=50, header_margin=50, no_image_text=True):\n",
    "    \"\"\"Determine bboxes which wrap a column.\"\"\"\n",
    "    paths = page.get_drawings()\n",
    "    bboxes = []\n",
    "\n",
    "    # path rectangles\n",
    "    path_rects = []\n",
    "\n",
    "    # image bboxes\n",
    "    img_bboxes = []\n",
    "\n",
    "    # bboxes of non-horizontal text\n",
    "    # avoid when expanding horizontal text boxes\n",
    "    vert_bboxes = []\n",
    "\n",
    "    # compute relevant page area\n",
    "    clip = +page.rect\n",
    "    clip.y1 -= footer_margin  # Remove footer area\n",
    "    clip.y0 += header_margin  # Remove header area\n",
    "\n",
    "    def can_extend(temp, bb, bboxlist):\n",
    "        \"\"\"Determines whether rectangle 'temp' can be extended by 'bb'\n",
    "        without intersecting any of the rectangles contained in 'bboxlist'.\n",
    "\n",
    "        Items of bboxlist may be None if they have been removed.\n",
    "\n",
    "        Returns:\n",
    "            True if 'temp' has no intersections with items of 'bboxlist'.\n",
    "        \"\"\"\n",
    "        for b in bboxlist:\n",
    "            if not intersects_bboxes(temp, vert_bboxes) and (\n",
    "                b == None or b == bb or (temp & b).is_empty\n",
    "            ):\n",
    "                continue\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def in_bbox(bb, bboxes):\n",
    "        \"\"\"Return 1-based number if a bbox contains bb, else return 0.\"\"\"\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            if bb in bbox:\n",
    "                return i + 1\n",
    "        return 0\n",
    "\n",
    "    def intersects_bboxes(bb, bboxes):\n",
    "        \"\"\"Return True if a bbox intersects bb, else return False.\"\"\"\n",
    "        for bbox in bboxes:\n",
    "            if not (bb & bbox).is_empty:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def extend_right(bboxes, width, path_bboxes, vert_bboxes, img_bboxes):\n",
    "        \"\"\"Extend a bbox to the right page border.\n",
    "\n",
    "        Whenever there is no text to the right of a bbox, enlarge it up\n",
    "        to the right page border.\n",
    "\n",
    "        Args:\n",
    "            bboxes: (list[IRect]) bboxes to check\n",
    "            width: (int) page width\n",
    "            path_bboxes: (list[IRect]) bboxes with a background color\n",
    "            vert_bboxes: (list[IRect]) bboxes with vertical text\n",
    "            img_bboxes: (list[IRect]) bboxes of images\n",
    "        Returns:\n",
    "            Potentially modified bboxes.\n",
    "        \"\"\"\n",
    "        for i, bb in enumerate(bboxes):\n",
    "            # do not extend text with background color\n",
    "            if in_bbox(bb, path_bboxes):\n",
    "                continue\n",
    "\n",
    "            # do not extend text in images\n",
    "            if in_bbox(bb, img_bboxes):\n",
    "                continue\n",
    "\n",
    "            # temp extends bb to the right page border\n",
    "            temp = +bb\n",
    "            temp.x1 = width\n",
    "\n",
    "            # do not cut through colored background or images\n",
    "            if intersects_bboxes(temp, path_bboxes + vert_bboxes + img_bboxes):\n",
    "                continue\n",
    "\n",
    "            # also, do not intersect other text bboxes\n",
    "            check = can_extend(temp, bb, bboxes)\n",
    "            if check:\n",
    "                bboxes[i] = temp  # replace with enlarged bbox\n",
    "\n",
    "        return [b for b in bboxes if b != None]\n",
    "\n",
    "    def clean_nblocks(nblocks):\n",
    "        \"\"\"Do some elementary cleaning.\"\"\"\n",
    "\n",
    "        # 1. remove any duplicate blocks.\n",
    "        blen = len(nblocks)\n",
    "        if blen < 2:\n",
    "            return nblocks\n",
    "        start = blen - 1\n",
    "        for i in range(start, -1, -1):\n",
    "            bb1 = nblocks[i]\n",
    "            bb0 = nblocks[i - 1]\n",
    "            if bb0 == bb1:\n",
    "                del nblocks[i]\n",
    "\n",
    "        # 2. repair sequence in special cases:\n",
    "        # consecutive bboxes with almost same bottom value are sorted ascending\n",
    "        # by x-coordinate.\n",
    "        y1 = nblocks[0].y1  # first bottom coordinate\n",
    "        i0 = 0  # its index\n",
    "        i1 = -1  # index of last bbox with same bottom\n",
    "\n",
    "        # Iterate over bboxes, identifying segments with approx. same bottom value.\n",
    "        # Replace every segment by its sorted version.\n",
    "        for i in range(1, len(nblocks)):\n",
    "            b1 = nblocks[i]\n",
    "            if abs(b1.y1 - y1) > 10:  # different bottom\n",
    "                if i1 > i0:  # segment length > 1? Sort it!\n",
    "                    nblocks[i0 : i1 + 1] = sorted(\n",
    "                        nblocks[i0 : i1 + 1], key=lambda b: b.x0\n",
    "                    )\n",
    "                y1 = b1.y1  # store new bottom value\n",
    "                i0 = i  # store its start index\n",
    "            i1 = i  # store current index\n",
    "        if i1 > i0:  # segment waiting to be sorted\n",
    "            nblocks[i0 : i1 + 1] = sorted(nblocks[i0 : i1 + 1], key=lambda b: b.x0)\n",
    "        return nblocks\n",
    "\n",
    "    # extract vector graphics\n",
    "    for p in paths:\n",
    "        path_rects.append(p[\"rect\"].irect)\n",
    "    path_bboxes = path_rects\n",
    "\n",
    "    # sort path bboxes by ascending top, then left coordinates\n",
    "    path_bboxes.sort(key=lambda b: (b.y0, b.x0))\n",
    "\n",
    "    # bboxes of images on page, no need to sort them\n",
    "    for item in page.get_images():\n",
    "        img_bboxes.extend(page.get_image_rects(item[0]))\n",
    "\n",
    "    # blocks of text on page\n",
    "    blocks = page.get_text(\n",
    "        \"dict\",\n",
    "        flags=fitz.TEXTFLAGS_TEXT,\n",
    "        clip=clip,\n",
    "    )[\"blocks\"]\n",
    "\n",
    "    # Make block rectangles, ignoring non-horizontal text\n",
    "    for b in blocks:\n",
    "        bbox = fitz.IRect(b[\"bbox\"])  # bbox of the block\n",
    "\n",
    "        # ignore text written upon images\n",
    "        if no_image_text and in_bbox(bbox, img_bboxes):\n",
    "            continue\n",
    "\n",
    "        # confirm first line to be horizontal\n",
    "        line0 = b[\"lines\"][0]  # get first line\n",
    "        if line0[\"dir\"] != (1, 0):  # only accept horizontal text\n",
    "            vert_bboxes.append(bbox)\n",
    "            continue\n",
    "\n",
    "        srect = fitz.EMPTY_IRECT()\n",
    "        for line in b[\"lines\"]:\n",
    "            lbbox = fitz.IRect(line[\"bbox\"])\n",
    "            text = \"\".join([s[\"text\"].strip() for s in line[\"spans\"]])\n",
    "            if len(text) > 1:\n",
    "                srect |= lbbox\n",
    "        bbox = +srect\n",
    "\n",
    "        if not bbox.is_empty:\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "    # Sort text bboxes by ascending background, top, then left coordinates\n",
    "    bboxes.sort(key=lambda k: (in_bbox(k, path_bboxes), k.y0, k.x0))\n",
    "\n",
    "    # Extend bboxes to the right where possible\n",
    "    bboxes = extend_right(\n",
    "        bboxes, int(page.rect.width), path_bboxes, vert_bboxes, img_bboxes\n",
    "    )\n",
    "\n",
    "    # immediately return of no text found\n",
    "    if bboxes == []:\n",
    "        return []\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Join bboxes to establish some column structure\n",
    "    # --------------------------------------------------------------------\n",
    "    # the final block bboxes on page\n",
    "    nblocks = [bboxes[0]]  # pre-fill with first bbox\n",
    "    bboxes = bboxes[1:]  # remaining old bboxes\n",
    "\n",
    "    for i, bb in enumerate(bboxes):  # iterate old bboxes\n",
    "        check = False  # indicates unwanted joins\n",
    "\n",
    "        # check if bb can extend one of the new blocks\n",
    "        for j in range(len(nblocks)):\n",
    "            nbb = nblocks[j]  # a new block\n",
    "\n",
    "            # never join across columns\n",
    "            if bb == None or nbb.x1 < bb.x0 or bb.x1 < nbb.x0:\n",
    "                continue\n",
    "\n",
    "            # never join across different background colors\n",
    "            if in_bbox(nbb, path_bboxes) != in_bbox(bb, path_bboxes):\n",
    "                continue\n",
    "\n",
    "            temp = bb | nbb  # temporary extension of new block\n",
    "            check = can_extend(temp, nbb, nblocks)\n",
    "            if check == True:\n",
    "                break\n",
    "\n",
    "        if not check:  # bb cannot be used to extend any of the new bboxes\n",
    "            nblocks.append(bb)  # so add it to the list\n",
    "            j = len(nblocks) - 1  # index of it\n",
    "            temp = nblocks[j]  # new bbox added\n",
    "\n",
    "        # check if some remaining bbox is contained in temp\n",
    "        check = can_extend(temp, bb, bboxes)\n",
    "        if check == False:\n",
    "            nblocks.append(bb)\n",
    "        else:\n",
    "            nblocks[j] = temp\n",
    "        bboxes[i] = None\n",
    "\n",
    "    # do some elementary cleaning\n",
    "    nblocks = clean_nblocks(nblocks)\n",
    "\n",
    "    # return identified text bboxes\n",
    "    return nblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimg = \"/Users/shreyasreedhar/Desktop/QA_BOT/input_data1/1701.06538.pdf_page_6_image_0.png\"\\nencoded_val = encode_image(img)\\nstore_in_mongodb(encoded_val, \"testimage\")\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "img = \"/Users/shreyasreedhar/Desktop/QA_BOT/input_data1/1701.06538.pdf_page_6_image_0.png\"\n",
    "encoded_val = encode_image(img)\n",
    "store_in_mongodb(encoded_val, \"testimage\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_dir= \"/Users/shreyasreedhar/Desktop/QA_BOT/extracted_papers/papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts images, summarizes them and stores them in the mongodb collection\n",
    "\n",
    "def extract_data(file_path, source_file, pdf_id):\n",
    "    doc = fitz.open(file_path)  # Open the PDF document\n",
    "    data = list()\n",
    "    image_extensions = ['png', 'jpg', 'jpeg', 'tiff', 'bmp']  # Common image extensions\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        print(f\"Processing Page no: {i}\")\n",
    "\n",
    "        try:\n",
    "            page_data = ''\n",
    "            img_data = ''\n",
    "\n",
    "            # Extract text using bounding boxes\n",
    "            bboxes = column_boxes(page, footer_margin=50, no_image_text=True)\n",
    "\n",
    "            for rect in bboxes:\n",
    "                page_data += page.get_text(clip=rect, sort=True)\n",
    "                page_data = re.sub(r'\\n\\s*', ' ', page_data)  # Clean newlines and spaces\n",
    "                page_data = re.sub(r' +', ' ', page_data)     # Remove extra spaces\n",
    "                page_data = re.sub(r'[^\\x20-\\x7E]', '', page_data)  # Remove non-ASCII characters\n",
    "                page_data += '\\n'\n",
    "\n",
    "            # Check for pre-extracted images\n",
    "            img_dir = \"/Users/shreyasreedhar/Desktop/QA_BOT/input_data1/\"\n",
    "            found_images = False\n",
    "\n",
    "            for image_index in range(1, 100):  # Arbitrary limit, adjust as needed\n",
    "                image_found = False\n",
    "\n",
    "                for ext in image_extensions:\n",
    "                    img_path = f\"{img_dir}{source_file}_page_{i}_image_{image_index}.{ext}\"\n",
    "\n",
    "                    if os.path.exists(img_path):\n",
    "                        print(f\"[+] Found image: {img_path}\")\n",
    "                        img_data += f\"<img file_path=({source_file}_page_{i}_image_{image_index}.{ext})>\"\n",
    "                        img_data += summarize_image(img_path, page_data)  # Summarize the image\n",
    "                        img_data += \"</img>\"\n",
    "                        encoded_val = encode_image(img_path)\n",
    "                        img_data += '\\n'\n",
    "                        store_in_mongodb(encoded_val, f\"{source_file}_page_{i}_image_{image_index}.{ext}\")\n",
    "                        image_found = True\n",
    "                        found_images = True\n",
    "                        break  # Stop checking other extensions if image is found\n",
    "\n",
    "                if not image_found:\n",
    "                    break  # Stop looking for more images once none are found\n",
    "\n",
    "            if not found_images:\n",
    "                print(f\"[!] No images found on page {i}\")\n",
    "\n",
    "            # Append the extracted data to the CSV\n",
    "            append_row_to_csv(page_no=i, \n",
    "                              page_data=page_data.strip(),  # Stripping whitespace\n",
    "                              img_data=img_data.strip(),     # Stripping whitespace\n",
    "                              pdf_id=pdf_id)                 # Include pdf_id\n",
    "\n",
    "            if page_data.strip() or img_data.strip():  \n",
    "                data.append(page_data.strip() + '\\n' + img_data.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {i}: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ICML03-094.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing 2301.00774.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing 2110.15343.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2110.15343.pdf_page_2_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2110.15343.pdf_page_2_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2110.15343.pdf_page_2_image_3.png\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing 2001.08361.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2001.08361.pdf_page_14_image_1.png\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2001.08361.pdf_page_24_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2001.08361.pdf_page_24_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2001.08361.pdf_page_24_image_3.png\n",
      "Processing Page no: 25\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2001.08361.pdf_page_25_image_1.png\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing 2104.09864.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing 2306.17806.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_6_image_1.png\n",
      "Processing Page no: 7\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_7_image_1.png\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_9_image_1.png\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_23_image_1.png\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_25_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_25_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_25_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.17806.pdf_page_25_image_4.png\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing Page no: 34\n",
      "[!] No images found on page 34\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing 2306.11695.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.11695.pdf_page_2_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.11695.pdf_page_2_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.11695.pdf_page_2_image_3.png\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Adaptive-mixtures-of-local-experts.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing 2305.18290.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing 1710.05941.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing 1907.01470.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing 2302.10866.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_23.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_24.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_25.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_26.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_27.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_28.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_29.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_30.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_31.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_32.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_33.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_34.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_29_image_35.png\n",
      "Processing Page no: 30\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_23.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_24.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_25.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_26.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_27.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_28.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_29.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_30.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_31.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_32.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_33.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_34.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_30_image_35.png\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_32_image_1.png\n",
      "Processing Page no: 33\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_33_image_7.png\n",
      "Processing Page no: 34\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2302.10866.pdf_page_34_image_7.png\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing Page no: 36\n",
      "[!] No images found on page 36\n",
      "Processing Page no: 37\n",
      "[!] No images found on page 37\n",
      "Processing 10000000_662098952474184_2584067087619170692_n.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_3_image_1.png\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_16_image_1.png\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_28_image_1.png\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_30_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_30_image_2.png\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_33_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_33_image_2.png\n",
      "Processing Page no: 34\n",
      "[!] No images found on page 34\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing Page no: 36\n",
      "[!] No images found on page 36\n",
      "Processing Page no: 37\n",
      "[!] No images found on page 37\n",
      "Processing Page no: 38\n",
      "[!] No images found on page 38\n",
      "Processing Page no: 39\n",
      "[!] No images found on page 39\n",
      "Processing Page no: 40\n",
      "[!] No images found on page 40\n",
      "Processing Page no: 41\n",
      "[!] No images found on page 41\n",
      "Processing Page no: 42\n",
      "[!] No images found on page 42\n",
      "Processing Page no: 43\n",
      "[!] No images found on page 43\n",
      "Processing Page no: 44\n",
      "[!] No images found on page 44\n",
      "Processing Page no: 45\n",
      "[!] No images found on page 45\n",
      "Processing Page no: 46\n",
      "[!] No images found on page 46\n",
      "Processing Page no: 47\n",
      "[!] No images found on page 47\n",
      "Processing Page no: 48\n",
      "[!] No images found on page 48\n",
      "Processing Page no: 49\n",
      "[!] No images found on page 49\n",
      "Processing Page no: 50\n",
      "[!] No images found on page 50\n",
      "Processing Page no: 51\n",
      "[!] No images found on page 51\n",
      "Processing Page no: 52\n",
      "[!] No images found on page 52\n",
      "Processing Page no: 53\n",
      "[!] No images found on page 53\n",
      "Processing Page no: 54\n",
      "[!] No images found on page 54\n",
      "Processing Page no: 55\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_55_image_1.png\n",
      "Processing Page no: 56\n",
      "[!] No images found on page 56\n",
      "Processing Page no: 57\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_57_image_1.png\n",
      "Processing Page no: 58\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/10000000_662098952474184_2584067087619170692_n.pdf_page_58_image_1.png\n",
      "Processing Page no: 59\n",
      "[!] No images found on page 59\n",
      "Processing Page no: 60\n",
      "[!] No images found on page 60\n",
      "Processing Page no: 61\n",
      "[!] No images found on page 61\n",
      "Processing Page no: 62\n",
      "[!] No images found on page 62\n",
      "Processing Page no: 63\n",
      "[!] No images found on page 63\n",
      "Processing Page no: 64\n",
      "[!] No images found on page 64\n",
      "Processing Page no: 65\n",
      "[!] No images found on page 65\n",
      "Processing Page no: 66\n",
      "[!] No images found on page 66\n",
      "Processing Page no: 67\n",
      "[!] No images found on page 67\n",
      "Processing Page no: 68\n",
      "[!] No images found on page 68\n",
      "Processing Page no: 69\n",
      "[!] No images found on page 69\n",
      "Processing Page no: 70\n",
      "[!] No images found on page 70\n",
      "Processing Page no: 71\n",
      "[!] No images found on page 71\n",
      "Processing Page no: 72\n",
      "[!] No images found on page 72\n",
      "Processing Page no: 73\n",
      "[!] No images found on page 73\n",
      "Processing Page no: 74\n",
      "[!] No images found on page 74\n",
      "Processing Page no: 75\n",
      "[!] No images found on page 75\n",
      "Processing Page no: 76\n",
      "[!] No images found on page 76\n",
      "Processing 2005.14165.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing Page no: 34\n",
      "[!] No images found on page 34\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing Page no: 36\n",
      "[!] No images found on page 36\n",
      "Processing Page no: 37\n",
      "[!] No images found on page 37\n",
      "Processing Page no: 38\n",
      "[!] No images found on page 38\n",
      "Processing Page no: 39\n",
      "[!] No images found on page 39\n",
      "Processing Page no: 40\n",
      "[!] No images found on page 40\n",
      "Processing Page no: 41\n",
      "[!] No images found on page 41\n",
      "Processing Page no: 42\n",
      "[!] No images found on page 42\n",
      "Processing Page no: 43\n",
      "[!] No images found on page 43\n",
      "Processing Page no: 44\n",
      "[!] No images found on page 44\n",
      "Processing Page no: 45\n",
      "[!] No images found on page 45\n",
      "Processing Page no: 46\n",
      "[!] No images found on page 46\n",
      "Processing Page no: 47\n",
      "[!] No images found on page 47\n",
      "Processing Page no: 48\n",
      "[!] No images found on page 48\n",
      "Processing Page no: 49\n",
      "[!] No images found on page 49\n",
      "Processing Page no: 50\n",
      "[!] No images found on page 50\n",
      "Processing Page no: 51\n",
      "[!] No images found on page 51\n",
      "Processing Page no: 52\n",
      "[!] No images found on page 52\n",
      "Processing Page no: 53\n",
      "[!] No images found on page 53\n",
      "Processing Page no: 54\n",
      "[!] No images found on page 54\n",
      "Processing Page no: 55\n",
      "[!] No images found on page 55\n",
      "Processing Page no: 56\n",
      "[!] No images found on page 56\n",
      "Processing Page no: 57\n",
      "[!] No images found on page 57\n",
      "Processing Page no: 58\n",
      "[!] No images found on page 58\n",
      "Processing Page no: 59\n",
      "[!] No images found on page 59\n",
      "Processing Page no: 60\n",
      "[!] No images found on page 60\n",
      "Processing Page no: 61\n",
      "[!] No images found on page 61\n",
      "Processing Page no: 62\n",
      "[!] No images found on page 62\n",
      "Processing Page no: 63\n",
      "[!] No images found on page 63\n",
      "Processing Page no: 64\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_64_image_13.png\n",
      "Processing Page no: 65\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_65_image_13.png\n",
      "Processing Page no: 66\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_66_image_14.png\n",
      "Processing Page no: 67\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2005.14165.pdf_page_67_image_17.png\n",
      "Processing Page no: 68\n",
      "[!] No images found on page 68\n",
      "Processing Page no: 69\n",
      "[!] No images found on page 69\n",
      "Processing Page no: 70\n",
      "[!] No images found on page 70\n",
      "Processing Page no: 71\n",
      "[!] No images found on page 71\n",
      "Processing Page no: 72\n",
      "[!] No images found on page 72\n",
      "Processing Page no: 73\n",
      "[!] No images found on page 73\n",
      "Processing Page no: 74\n",
      "[!] No images found on page 74\n",
      "Processing 2211.05102.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing 2310.06825.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing 2306.07629.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing 2305.07185.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2305.07185.pdf_page_8_image_1.png\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing 2306.12929.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_3_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_3_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_3_image_3.png\n",
      "Processing Page no: 4\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_4_image_13.png\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_8_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_8_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_8_image_3.png\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_10_image_9.png\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_16_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_16_image_2.png\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_23_image_23.png\n",
      "Processing Page no: 24\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_24_image_23.png\n",
      "Processing Page no: 25\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_25_image_23.png\n",
      "Processing Page no: 26\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_26_image_23.png\n",
      "Processing Page no: 27\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_27_image_23.png\n",
      "Processing Page no: 28\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_23.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_24.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_25.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_26.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_27.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_28.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_29.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_30.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_28_image_31.png\n",
      "Processing Page no: 29\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_6.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_7.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_8.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_9.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_10.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_11.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_12.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_13.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_14.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_15.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_16.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_17.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_18.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_19.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_20.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_21.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_22.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_23.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_24.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_25.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_26.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_27.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_28.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_29.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_30.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_31.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_32.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_33.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_34.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_35.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_36.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_37.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_38.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2306.12929.pdf_page_29_image_39.png\n",
      "Processing 2301.13688.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing 2109.06243.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2109.06243.pdf_page_8_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2109.06243.pdf_page_8_image_2.png\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing 2207.00112.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing 2310.20707.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_4.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_5.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_2_image_6.png\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_28_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_28_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_28_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_28_image_4.png\n",
      "Processing Page no: 29\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_29_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_29_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_29_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_29_image_4.png\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing Page no: 34\n",
      "[!] No images found on page 34\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing Page no: 36\n",
      "[!] No images found on page 36\n",
      "Processing Page no: 37\n",
      "[!] No images found on page 37\n",
      "Processing Page no: 38\n",
      "[!] No images found on page 38\n",
      "Processing Page no: 39\n",
      "[!] No images found on page 39\n",
      "Processing Page no: 40\n",
      "[!] No images found on page 40\n",
      "Processing Page no: 41\n",
      "[!] No images found on page 41\n",
      "Processing Page no: 42\n",
      "[!] No images found on page 42\n",
      "Processing Page no: 43\n",
      "[!] No images found on page 43\n",
      "Processing Page no: 44\n",
      "[!] No images found on page 44\n",
      "Processing Page no: 45\n",
      "[!] No images found on page 45\n",
      "Processing Page no: 46\n",
      "[!] No images found on page 46\n",
      "Processing Page no: 47\n",
      "[!] No images found on page 47\n",
      "Processing Page no: 48\n",
      "[!] No images found on page 48\n",
      "Processing Page no: 49\n",
      "[!] No images found on page 49\n",
      "Processing Page no: 50\n",
      "[!] No images found on page 50\n",
      "Processing Page no: 51\n",
      "[!] No images found on page 51\n",
      "Processing Page no: 52\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_52_image_1.png\n",
      "Processing Page no: 53\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_53_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_53_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2310.20707.pdf_page_53_image_3.png\n",
      "Processing Page no: 54\n",
      "[!] No images found on page 54\n",
      "Processing 2307.13304.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[!] No images found on page 3\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing 2203.02155.pdf...\n",
      "Processing Page no: 0\n",
      "[!] No images found on page 0\n",
      "Processing Page no: 1\n",
      "[!] No images found on page 1\n",
      "Processing Page no: 2\n",
      "[!] No images found on page 2\n",
      "Processing Page no: 3\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2203.02155.pdf_page_3_image_1.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2203.02155.pdf_page_3_image_2.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2203.02155.pdf_page_3_image_3.png\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2203.02155.pdf_page_3_image_4.png\n",
      "Processing Page no: 4\n",
      "[!] No images found on page 4\n",
      "Processing Page no: 5\n",
      "[!] No images found on page 5\n",
      "Processing Page no: 6\n",
      "[!] No images found on page 6\n",
      "Processing Page no: 7\n",
      "[!] No images found on page 7\n",
      "Processing Page no: 8\n",
      "[!] No images found on page 8\n",
      "Processing Page no: 9\n",
      "[!] No images found on page 9\n",
      "Processing Page no: 10\n",
      "[!] No images found on page 10\n",
      "Processing Page no: 11\n",
      "[!] No images found on page 11\n",
      "Processing Page no: 12\n",
      "[!] No images found on page 12\n",
      "Processing Page no: 13\n",
      "[!] No images found on page 13\n",
      "Processing Page no: 14\n",
      "[!] No images found on page 14\n",
      "Processing Page no: 15\n",
      "[!] No images found on page 15\n",
      "Processing Page no: 16\n",
      "[!] No images found on page 16\n",
      "Processing Page no: 17\n",
      "[!] No images found on page 17\n",
      "Processing Page no: 18\n",
      "[!] No images found on page 18\n",
      "Processing Page no: 19\n",
      "[!] No images found on page 19\n",
      "Processing Page no: 20\n",
      "[!] No images found on page 20\n",
      "Processing Page no: 21\n",
      "[!] No images found on page 21\n",
      "Processing Page no: 22\n",
      "[!] No images found on page 22\n",
      "Processing Page no: 23\n",
      "[!] No images found on page 23\n",
      "Processing Page no: 24\n",
      "[!] No images found on page 24\n",
      "Processing Page no: 25\n",
      "[!] No images found on page 25\n",
      "Processing Page no: 26\n",
      "[!] No images found on page 26\n",
      "Processing Page no: 27\n",
      "[!] No images found on page 27\n",
      "Processing Page no: 28\n",
      "[!] No images found on page 28\n",
      "Processing Page no: 29\n",
      "[!] No images found on page 29\n",
      "Processing Page no: 30\n",
      "[!] No images found on page 30\n",
      "Processing Page no: 31\n",
      "[!] No images found on page 31\n",
      "Processing Page no: 32\n",
      "[!] No images found on page 32\n",
      "Processing Page no: 33\n",
      "[!] No images found on page 33\n",
      "Processing Page no: 34\n",
      "[!] No images found on page 34\n",
      "Processing Page no: 35\n",
      "[!] No images found on page 35\n",
      "Processing Page no: 36\n",
      "[!] No images found on page 36\n",
      "Processing Page no: 37\n",
      "[!] No images found on page 37\n",
      "Processing Page no: 38\n",
      "[!] No images found on page 38\n",
      "Processing Page no: 39\n",
      "[+] Found image: /Users/shreyasreedhar/Desktop/QA_BOT/input_data1/2203.02155.pdf_page_39_image_1.png\n",
      "Processing Page no: 40\n",
      "[!] No images found on page 40\n",
      "Processing Page no: 41\n",
      "[!] No images found on page 41\n",
      "Processing Page no: 42\n",
      "[!] No images found on page 42\n",
      "Processing Page no: 43\n",
      "[!] No images found on page 43\n",
      "Processing Page no: 44\n",
      "[!] No images found on page 44\n",
      "Processing Page no: 45\n",
      "[!] No images found on page 45\n",
      "Processing Page no: 46\n",
      "[!] No images found on page 46\n",
      "Processing Page no: 47\n",
      "[!] No images found on page 47\n",
      "Processing Page no: 48\n",
      "[!] No images found on page 48\n",
      "Processing Page no: 49\n",
      "[!] No images found on page 49\n",
      "Processing Page no: 50\n",
      "[!] No images found on page 50\n",
      "Processing Page no: 51\n",
      "[!] No images found on page 51\n",
      "Processing Page no: 52\n",
      "[!] No images found on page 52\n",
      "Processing Page no: 53\n",
      "[!] No images found on page 53\n",
      "Processing Page no: 54\n",
      "[!] No images found on page 54\n",
      "Processing Page no: 55\n",
      "[!] No images found on page 55\n",
      "Processing Page no: 56\n",
      "[!] No images found on page 56\n",
      "Processing Page no: 57\n",
      "[!] No images found on page 57\n",
      "Processing Page no: 58\n",
      "[!] No images found on page 58\n",
      "Processing Page no: 59\n",
      "[!] No images found on page 59\n",
      "Processing Page no: 60\n",
      "[!] No images found on page 60\n",
      "Processing Page no: 61\n",
      "[!] No images found on page 61\n",
      "Processing Page no: 62\n",
      "[!] No images found on page 62\n",
      "Processing Page no: 63\n",
      "[!] No images found on page 63\n",
      "Processing Page no: 64\n",
      "[!] No images found on page 64\n",
      "Processing Page no: 65\n",
      "[!] No images found on page 65\n",
      "Processing Page no: 66\n",
      "[!] No images found on page 66\n",
      "Processing Page no: 67\n",
      "[!] No images found on page 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Convert the collected data into a DataFrame\\ncombined_df = pd.DataFrame(data)\\n\\n# Save the combined DataFrame to a CSV file\\ncombined_df.to_csv('/Users/shreyasreedhar/Desktop/QA_BOT/output_data/all_extracted_images.csv', index=False)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = []\n",
    "\n",
    "# Iterate over the extracted files and process each PDF\n",
    "idd=1\n",
    "for file_name in os.listdir(extraction_dir):\n",
    "    if file_name.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "        file_path = os.path.join(extraction_dir, file_name)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "\n",
    "        # Call your extract_data function\n",
    "        df = extract_data(file_path, file_name,idd)\n",
    "        idd+=1\n",
    "\n",
    "        # Append the result to all_data\n",
    "        data.extend(df)  # Use extend to flatten the list of lists if necessary\n",
    "\n",
    "# data now contains data from all processed PDFs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_id_map= {1:'ICML03-094.pdf',2:'2301.00774.pdf' ,3:'2110.15343.pdf',4:'2001.08361.pdf',5:'2104.09864.pdf',6:'2306.17806.pdf',\n",
    "             7:'2306.11695.pdf', 8:'Adaptive-mixtures-of-local-experts.pdf',9:'2305.18290.pdf',10:'1710.05941.pdf',11:'1907.01470.pdf',\n",
    "             12:'2302.10866.pdf',13:'10000000_662098952474184_2584067087619170692_n.pdf',14:'2005.14165.pdf',15:'2211.05102.pdf',\n",
    "             16:'2310.06825.pdf',17:'2306.07629.pdf',18:'2305.07185.pdf',19:'2306.12929.pdf',20:'2301.13688.pdf',21:'2109.06243.pdf',\n",
    "             22:'2207.00112.pdf',23:'2310.20707.pdf',24:'2307.13304.pdf',25:'2203.02155.pdf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv('/Users/shreyasreedhar/Desktop/QA_BOT/output_data/combined_df_extracted_images.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>Weighted Low-Rank Approximations Nathan Srebro nati@mit.edu Tommi Jaakkola tommi@ai.mit.edu Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA Low-rank matrix approximation with respect to the Frobenius normminimizing the sum squared dier- ences to the target matrixcan be easily solved with Singular Value Decomposition (SVD). For many ap- plications, however, the deviation between the ob- served matrix and the low-rank approximation should be measured relative to a weighted (or other) norm. While the extension to the weighted-norm case is con- ceptually straightforward, and dates back to early work on factor analysis (Young, 1940), standard algo- rithms (such as SVD) for solving the unweighted case do not carry over to the weighted case. Weighted norms can arise in a number of situations. Zero/one weights, for example, arise when some of the entries in the matrix are not observed. More generally, we may introduce weights in response to some exter- nal estimate of the noise variance associated with each measurement. This is the case, for example, in gene ex- pression analysis, where the error model for microarray measurements provides entry-specic noise estimates. Setting the weights inversely proportional to the as- sumed noise variance can lead to a better reconstruc- tion of the underlying structure. In other applications, entries in the target matrix may represent aggregates of many samples. The standard unweighted low-rank approximation (e.g., for separating style and content (Tenenbaum &amp; Freeman, 2000)) would in this context assume that the number of samples is uniform across the entries. Non-uniform weights are needed to appro- priately capture any dierences in the sample sizes. Despite its usefulness, the weighted extension has at- tracted relatively little attention. Shpak (1990) and Lu et al. (1997) studied weighted-norm low-rank approxi- mations for the design of two-dimensional digital lters where the weights arise from constraints of varying im- portance. Shpak developed gradient-based optimiza- tion methods while Lu et al. suggested alternating- optimization methods. In both cases, rank-k approx- imations are greedily combined from k rank-one ap- Abstract We study the common problem of approx- imating a target matrix with a matrix of lower rank. We provide a simple and ecient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed- form solution in general. We analyze, in ad- dition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in re- constructing the underlying low-rank repre- sentation, and extend the formulation to non- Gaussian noise models such as logistic mod- els. Finally, we apply the methods developed to a collaborative ltering task. 1. Introduction Factor models are natural in the analysis of many kinds of tabulated data. This includes user preferences over a list of items, microarray (gene expression) mea- surements, and collections of images. Consider, for ex- ample, a dataset of user preferences for movies or jokes. The premise behind a factor model is that there is only a small number of factors inuencing the preferences, and that a users preference vector is determined by how each factor applies to that user. In a linear fac- tor model, each factor is a preference vector, and a users preferences correspond to a linear combination of these factor vectors, with user-specic coecients. Thus, for n users and d items, the preferences accord- ing to a k-factor model are given by the product of an n  k coecient matrix (each row representing the extent to which each factor is used) and a k  d fac-tor matrix whose rows are the factors. The preference matrices which admit such a factorization are matri- ces of rank at most k. Thus, training such a linear factor model amounts to approximating the empirical preferences with a low-rank matrix.</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>proximations. Unlike for the unweighted case, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to understand the behavior of the obj...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>invertible scalings, V can be specied as an an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>to A, and to initialize X to zero. Initializin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>(2003) and recently studied by Schein et al. (...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>25</td>\n",
       "      <td>63</td>\n",
       "      <td>Prompt: Please describe what the following cod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>25</td>\n",
       "      <td>64</td>\n",
       "      <td>Prompt: Based on the following passage, provid...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>Prompt: Read the following text and answer the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>Prompt: Create a shopping list from this recip...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>Prompt: ELI5: Whats the cause of the \"anxiety ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1   0  \\\n",
       "0     1   1   \n",
       "1     1   2   \n",
       "2     1   3   \n",
       "3     1   4   \n",
       "4     1   5   \n",
       "..   ..  ..   \n",
       "702  25  63   \n",
       "703  25  64   \n",
       "704  25  65   \n",
       "705  25  66   \n",
       "706  25  67   \n",
       "\n",
       "    Weighted Low-Rank Approximations Nathan Srebro nati@mit.edu Tommi Jaakkola tommi@ai.mit.edu Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA Low-rank matrix approximation with respect to the Frobenius normminimizing the sum squared dier- ences to the target matrixcan be easily solved with Singular Value Decomposition (SVD). For many ap- plications, however, the deviation between the ob- served matrix and the low-rank approximation should be measured relative to a weighted (or other) norm. While the extension to the weighted-norm case is con- ceptually straightforward, and dates back to early work on factor analysis (Young, 1940), standard algo- rithms (such as SVD) for solving the unweighted case do not carry over to the weighted case. Weighted norms can arise in a number of situations. Zero/one weights, for example, arise when some of the entries in the matrix are not observed. More generally, we may introduce weights in response to some exter- nal estimate of the noise variance associated with each measurement. This is the case, for example, in gene ex- pression analysis, where the error model for microarray measurements provides entry-specic noise estimates. Setting the weights inversely proportional to the as- sumed noise variance can lead to a better reconstruc- tion of the underlying structure. In other applications, entries in the target matrix may represent aggregates of many samples. The standard unweighted low-rank approximation (e.g., for separating style and content (Tenenbaum & Freeman, 2000)) would in this context assume that the number of samples is uniform across the entries. Non-uniform weights are needed to appro- priately capture any dierences in the sample sizes. Despite its usefulness, the weighted extension has at- tracted relatively little attention. Shpak (1990) and Lu et al. (1997) studied weighted-norm low-rank approxi- mations for the design of two-dimensional digital lters where the weights arise from constraints of varying im- portance. Shpak developed gradient-based optimiza- tion methods while Lu et al. suggested alternating- optimization methods. In both cases, rank-k approx- imations are greedily combined from k rank-one ap- Abstract We study the common problem of approx- imating a target matrix with a matrix of lower rank. We provide a simple and ecient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed- form solution in general. We analyze, in ad- dition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in re- constructing the underlying low-rank repre- sentation, and extend the formulation to non- Gaussian noise models such as logistic mod- els. Finally, we apply the methods developed to a collaborative ltering task. 1. Introduction Factor models are natural in the analysis of many kinds of tabulated data. This includes user preferences over a list of items, microarray (gene expression) mea- surements, and collections of images. Consider, for ex- ample, a dataset of user preferences for movies or jokes. The premise behind a factor model is that there is only a small number of factors inuencing the preferences, and that a users preference vector is determined by how each factor applies to that user. In a linear fac- tor model, each factor is a preference vector, and a users preferences correspond to a linear combination of these factor vectors, with user-specic coecients. Thus, for n users and d items, the preferences accord- ing to a k-factor model are given by the product of an n  k coecient matrix (each row representing the extent to which each factor is used) and a k  d fac-tor matrix whose rows are the factors. The preference matrices which admit such a factorization are matri- ces of rank at most k. Thus, training such a linear factor model amounts to approximating the empirical preferences with a low-rank matrix.  \\\n",
       "0    proximations. Unlike for the unweighted case, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1    In order to understand the behavior of the obj...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    invertible scalings, V can be specied as an an...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    to A, and to initialize X to zero. Initializin...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    (2003) and recently studied by Schein et al. (...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "702  Prompt: Please describe what the following cod...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "703  Prompt: Based on the following passage, provid...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "704  Prompt: Read the following text and answer the...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "705  Prompt: Create a shopping list from this recip...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "706  Prompt: ELI5: Whats the cause of the \"anxiety ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "    Unnamed: 3  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "702        NaN  \n",
       "703        NaN  \n",
       "704        NaN  \n",
       "705        NaN  \n",
       "706        NaN  \n",
       "\n",
       "[707 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shreyasreedhar/Desktop/QA_BOT/output_data/data_extracted_images.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Weighted Low-Rank Approximations Nathan Srebro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>proximations. Unlike for the unweighted case, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to understand the behavior of the obj...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>invertible scalings, V can be specied as an an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>to A, and to initialize X to zero. Initializin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1                                                  2    3\n",
       "0  1  0  Weighted Low-Rank Approximations Nathan Srebro...  NaN\n",
       "1  1  1  proximations. Unlike for the unweighted case, ...  NaN\n",
       "2  1  2  In order to understand the behavior of the obj...  NaN\n",
       "3  1  3  invertible scalings, V can be specied as an an...  NaN\n",
       "4  1  4  to A, and to initialize X to zero. Initializin...  NaN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['pdf_id', 'page_no', 'page_data', 'image_data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_data</th>\n",
       "      <th>image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Weighted Low-Rank Approximations Nathan Srebro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>proximations. Unlike for the unweighted case, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to understand the behavior of the obj...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>invertible scalings, V can be specied as an an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>to A, and to initialize X to zero. Initializin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>(2003) and recently studied by Schein et al. (...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>et al. (2001) use a low-rank approximation of ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>also ensures scale and transformation invariab...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>SparseGPT: Massive Language Models Can be Accu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scatterbrain: Unifying Sparse and Low-rank Att...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Figure 1: Left: regimes that sparse+low-rank a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>achieves 2.1 lower error compared to other eci...</td>\n",
       "      <td>&lt;img file_path=(2110.15343.pdf_page_2_image_1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1 Motivating Observations: Low-rank and Spar...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>low-rank matrix. In the middle regime of , we ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1. If the goal is accuracy, Robust PCA is the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>The Scatterbrain method would work exactly the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Figure 5: First: approximation comparison betw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>Table 2: The performance of Scatterbrain, Refo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5.2.2 Classication Tasks On a suite of long-ra...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Potential negative societal impacts. Our work ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>[5] Iz Beltagy, Matthew E Peters, and Arman Co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>[21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>[39] Valerii Likhosherstov, Krzysztof Choroman...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>[56] Sainbayar Sukhbaatar, Edouard Grave, Piot...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>Appendix Table of Contents A Extended Related ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>A Extended Related Work A.1 Robust PCA Robust ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>low-rank approximation due to the non-linearit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pdf_id  page_no                                          page_data  \\\n",
       "0        1        0  Weighted Low-Rank Approximations Nathan Srebro...   \n",
       "1        1        1  proximations. Unlike for the unweighted case, ...   \n",
       "2        1        2  In order to understand the behavior of the obj...   \n",
       "3        1        3  invertible scalings, V can be specied as an an...   \n",
       "4        1        4  to A, and to initialize X to zero. Initializin...   \n",
       "5        1        5  (2003) and recently studied by Schein et al. (...   \n",
       "6        1        6  et al. (2001) use a low-rank approximation of ...   \n",
       "7        1        7  also ensures scale and transformation invariab...   \n",
       "8        2        0  SparseGPT: Massive Language Models Can be Accu...   \n",
       "9        2        1  SparseGPT: Massive Language Models Can be Accu...   \n",
       "10       2        2  SparseGPT: Massive Language Models Can be Accu...   \n",
       "11       2        3  SparseGPT: Massive Language Models Can be Accu...   \n",
       "12       2        4  SparseGPT: Massive Language Models Can be Accu...   \n",
       "13       2        5  SparseGPT: Massive Language Models Can be Accu...   \n",
       "14       2        6  SparseGPT: Massive Language Models Can be Accu...   \n",
       "15       2        7  SparseGPT: Massive Language Models Can be Accu...   \n",
       "16       2        8  SparseGPT: Massive Language Models Can be Accu...   \n",
       "17       2        9  SparseGPT: Massive Language Models Can be Accu...   \n",
       "18       2       10  SparseGPT: Massive Language Models Can be Accu...   \n",
       "19       2       11  SparseGPT: Massive Language Models Can be Accu...   \n",
       "20       2       12  SparseGPT: Massive Language Models Can be Accu...   \n",
       "21       2       13  SparseGPT: Massive Language Models Can be Accu...   \n",
       "22       3        0  Scatterbrain: Unifying Sparse and Low-rank Att...   \n",
       "23       3        1  Figure 1: Left: regimes that sparse+low-rank a...   \n",
       "24       3        2  achieves 2.1 lower error compared to other eci...   \n",
       "25       3        3  3.1 Motivating Observations: Low-rank and Spar...   \n",
       "26       3        4  low-rank matrix. In the middle regime of , we ...   \n",
       "27       3        5  1. If the goal is accuracy, Robust PCA is the ...   \n",
       "28       3        6  The Scatterbrain method would work exactly the...   \n",
       "29       3        7  Figure 5: First: approximation comparison betw...   \n",
       "30       3        8  Table 2: The performance of Scatterbrain, Refo...   \n",
       "31       3        9  5.2.2 Classication Tasks On a suite of long-ra...   \n",
       "32       3       10  Potential negative societal impacts. Our work ...   \n",
       "33       3       11  [5] Iz Beltagy, Matthew E Peters, and Arman Co...   \n",
       "34       3       12  [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew...   \n",
       "35       3       13  [39] Valerii Likhosherstov, Krzysztof Choroman...   \n",
       "36       3       14  [56] Sainbayar Sukhbaatar, Edouard Grave, Piot...   \n",
       "37       3       15  Appendix Table of Contents A Extended Related ...   \n",
       "38       3       16  A Extended Related Work A.1 Robust PCA Robust ...   \n",
       "39       3       17  low-rank approximation due to the non-linearit...   \n",
       "\n",
       "                                           image_data  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "15                                                NaN  \n",
       "16                                                NaN  \n",
       "17                                                NaN  \n",
       "18                                                NaN  \n",
       "19                                                NaN  \n",
       "20                                                NaN  \n",
       "21                                                NaN  \n",
       "22                                                NaN  \n",
       "23                                                NaN  \n",
       "24  <img file_path=(2110.15343.pdf_page_2_image_1....  \n",
       "25                                                NaN  \n",
       "26                                                NaN  \n",
       "27                                                NaN  \n",
       "28                                                NaN  \n",
       "29                                                NaN  \n",
       "30                                                NaN  \n",
       "31                                                NaN  \n",
       "32                                                NaN  \n",
       "33                                                NaN  \n",
       "34                                                NaN  \n",
       "35                                                NaN  \n",
       "36                                                NaN  \n",
       "37                                                NaN  \n",
       "38                                                NaN  \n",
       "39                                                NaN  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the image_data\n",
    "def filter_image_data(image_data):\n",
    "    if isinstance(image_data, str):\n",
    "        # Use regex to find all <img> tags\n",
    "        img_tags = re.findall(r'<img.*?>.*?</img>', image_data, re.DOTALL)\n",
    "        # Filter out the unwanted tags\n",
    "        filtered_tags = [tag for tag in img_tags if \"I am sorry\" not in tag]\n",
    "        # Join the remaining tags\n",
    "        return ''.join(filtered_tags)\n",
    "    return image_data\n",
    "\n",
    "# Apply the function to the image_data column\n",
    "df['image_data'] = df['image_data'].apply(filter_image_data)\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file (optional)\n",
    "df.to_csv('/Users/shreyasreedhar/Desktop/QA_BOT/output_data/data_images_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_data</th>\n",
       "      <th>image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Weighted Low-Rank Approximations Nathan Srebro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>proximations. Unlike for the unweighted case, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to understand the behavior of the obj...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>invertible scalings, V can be specied as an an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>to A, and to initialize X to zero. Initializin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>25</td>\n",
       "      <td>63</td>\n",
       "      <td>Prompt: Please describe what the following cod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>25</td>\n",
       "      <td>64</td>\n",
       "      <td>Prompt: Based on the following passage, provid...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>Prompt: Read the following text and answer the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>Prompt: Create a shopping list from this recip...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>Prompt: ELI5: Whats the cause of the \"anxiety ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pdf_id  page_no                                          page_data  \\\n",
       "0         1        0  Weighted Low-Rank Approximations Nathan Srebro...   \n",
       "1         1        1  proximations. Unlike for the unweighted case, ...   \n",
       "2         1        2  In order to understand the behavior of the obj...   \n",
       "3         1        3  invertible scalings, V can be specied as an an...   \n",
       "4         1        4  to A, and to initialize X to zero. Initializin...   \n",
       "..      ...      ...                                                ...   \n",
       "703      25       63  Prompt: Please describe what the following cod...   \n",
       "704      25       64  Prompt: Based on the following passage, provid...   \n",
       "705      25       65  Prompt: Read the following text and answer the...   \n",
       "706      25       66  Prompt: Create a shopping list from this recip...   \n",
       "707      25       67  Prompt: ELI5: Whats the cause of the \"anxiety ...   \n",
       "\n",
       "    image_data  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "703        NaN  \n",
       "704        NaN  \n",
       "705        NaN  \n",
       "706        NaN  \n",
       "707        NaN  \n",
       "\n",
       "[708 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pdf_id  page_no                                          page_data  \\\n",
      "0       1        0  Weighted Low-Rank Approximations Nathan Srebro...   \n",
      "1       1        1  proximations. Unlike for the unweighted case, ...   \n",
      "2       1        2  In order to understand the behavior of the obj...   \n",
      "3       1        3  invertible scalings, V can be specied as an an...   \n",
      "4       1        4  to A, and to initialize X to zero. Initializin...   \n",
      "\n",
      "  image_data        pdf_file  \n",
      "0        NaN  ICML03-094.pdf  \n",
      "1        NaN  ICML03-094.pdf  \n",
      "2        NaN  ICML03-094.pdf  \n",
      "3        NaN  ICML03-094.pdf  \n",
      "4        NaN  ICML03-094.pdf  \n"
     ]
    }
   ],
   "source": [
    "df['pdf_file'] = df['pdf_id'].map(pdf_id_map)\n",
    "\n",
    "# Verify the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/shreyasreedhar/Desktop/QA_BOT/output_data/data_images_preprocessed_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_data</th>\n",
       "      <th>image_data</th>\n",
       "      <th>pdf_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Weighted Low-Rank Approximations Nathan Srebro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICML03-094.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>proximations. Unlike for the unweighted case, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICML03-094.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to understand the behavior of the obj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICML03-094.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>invertible scalings, V can be specied as an an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICML03-094.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>to A, and to initialize X to zero. Initializin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICML03-094.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pdf_id  page_no                                          page_data  \\\n",
       "0       1        0  Weighted Low-Rank Approximations Nathan Srebro...   \n",
       "1       1        1  proximations. Unlike for the unweighted case, ...   \n",
       "2       1        2  In order to understand the behavior of the obj...   \n",
       "3       1        3  invertible scalings, V can be specied as an an...   \n",
       "4       1        4  to A, and to initialize X to zero. Initializin...   \n",
       "\n",
       "  image_data        pdf_file  \n",
       "0        NaN  ICML03-094.pdf  \n",
       "1        NaN  ICML03-094.pdf  \n",
       "2        NaN  ICML03-094.pdf  \n",
       "3        NaN  ICML03-094.pdf  \n",
       "4        NaN  ICML03-094.pdf  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "import pymongo\n",
    "import base64\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pymongo\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "\n",
    "MONGO_DB_URL = \"mongodb+srv://rey123:asapp123@asapp.gxpwx.mongodb.net/?retryWrites=true&w=majority&appName=asapp\"\n",
    "# Connect to your MongoDB cluster\n",
    "client = MongoClient(MONGO_DB_URL, tlsCAFile=certifi.where())\n",
    "\n",
    "db = client[\"asapp\"]\n",
    "collection = db[\"images_v1\"]  # Replace with your collection name\n",
    "\n",
    "# Query for the specific image file you want to retrieve\n",
    "file_name = \"2110.15343.pdf_page_2_image_1.png\"  # Replace with the desired file_name\n",
    "image_data = collection.find_one({\"file_name\": file_name})\n",
    "\n",
    "# Check if the image was found\n",
    "if image_data:\n",
    "    # Decode the base64 encoded image\n",
    "    encoded_val = image_data[\"encoded_val\"]\n",
    "    img_data = base64.b64decode(encoded_val)\n",
    "\n",
    "    # Display the image\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Image not found in the database.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
